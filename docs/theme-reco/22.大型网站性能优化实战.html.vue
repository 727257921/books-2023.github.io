<template><div><h1 id="大型网站性能优化实战-从前端、网络、cdn到后端、大促的全链路性能优化详解" tabindex="-1"><a class="header-anchor" href="#大型网站性能优化实战-从前端、网络、cdn到后端、大促的全链路性能优化详解" aria-hidden="true">#</a> 大型网站性能优化实战：从前端、网络、CDN到后端、大促的全链路性能优化详解</h1>
<p>周涛明 张荣华 张新兵</p>
<h2 id="◆-序言" tabindex="-1"><a class="header-anchor" href="#◆-序言" aria-hidden="true">#</a> ◆ 序言</h2>
<blockquote>
<blockquote>
<p>我们处于飞速发展的时代，同时我们也处于浮躁的时代，这个时代造就了很多英雄，但也很容易在其中迷失自己。在迷失的时候，给自己定一个目标，只有不断地学习和努力，同时摆正自己的心态，明确自己想要的东西，才能处在浪潮之巅，在时代的浪潮里留下自己的痕迹。</p>
</blockquote>
</blockquote>
<h3 id="◆-1-1-页面用户体验的要素介绍" tabindex="-1"><a class="header-anchor" href="#◆-1-1-页面用户体验的要素介绍" aria-hidden="true">#</a> ◆ 1.1 页面用户体验的要素介绍</h3>
<blockquote>
<blockquote>
<p>下面几个要素来衡量网站性能方面的用户体验</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 白屏。● 首屏。● 页面整体加载。● 页面可交互。● 功能交互响应。</p>
</blockquote>
</blockquote>
<h3 id="◆-1-2-白屏时间" tabindex="-1"><a class="header-anchor" href="#◆-1-2-白屏时间" aria-hidden="true">#</a> ◆ 1.2 白屏时间</h3>
<blockquote>
<blockquote>
<p>1.2.1 白屏时间的重要性</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>1.DNS Lookup</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对大中型网站来说，通常在页面加载过程中会产生下列域名解析：● www.example.com，页面URL本身。● style.example.com,JS、CSS等静态资源请求。● img.example.com，静态图片请求。● ajax.example.com，各种站内的Ajax请求。● *.google.com，其他比如Google的站外请求。域名解析的过程，可短至几十毫秒，也可能消耗几秒，所以解析过程的快慢直接影响页面整体加载的渲染速度，特别是对于每天访问量达千万级别的页面。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>DNS解析速度的优化策略有很多，通常我们会从以下几个方面思考细节的优化：● DNS缓存优化。● DNS预加载策略。● 页面中资源的域名的合理分配。● 稳定可靠的DNS服务器等。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>2.建立TCP请求连接</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>浏览器针对各个域名进行DNS Lookup得到IP地址后，就开始建立TCP请求连接的过程。TCP通过三次握手建立连接，并提供可靠的数据传输服务。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>TCP优化的思路</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 基于不同的网络环境，优化数据包的大小，以减少数据因传输丢失或者被破坏产生的重传，从而提高传输效率。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>尝试在部署在不同国家的后台服务系统之间，有针对性地建立高速的专属网络通道，或者通过购买第三方内容网络服务来加速浏览器和 Web 服务器之间的网络通道优化</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对于中小型网站来说，重心可能主要放在数据获取过程的优化。其中对缓存、数据库等处理的优化，可为网站页面带来较大的响应速度提升。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对于有着亿级流量的大型网络平台，每一个过程的优化都可能带来质的提升。比如被很多人忽略的模板渲染过程，通过优化模板渲染逻辑，可使模板渲染速度提高一倍；或者通过调整Gzip算法及响应内容的代码结构，来提高Gzip压缩率、减小压缩包大小等。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>4.客户端下载、解析、渲染显示页面</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>服务器返回HTTP Response后，浏览器陆续开始接收数据，进行HTML下载、解析、渲染显示等过程。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>具体步骤如下。（1）如果是Gzip包，则先解压为HTML。（2）解析HTML的头部代码，下载头部代码中引用的样式资源文件或者脚本资源文件。（3）解析HTML代码和样式文件代码，这个过程会构造出两个树结构，即与HTML相关的DOM树，以及与CSS相关的CSSOM树。（4）通过遍历DOM树和CSSOM树，浏览器依次计算每个节点的大小、坐标、颜色等样式，构造出渲染树。（5）根据渲染树完成绘制的过程。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>浏览器下载 HTML 后，首先解析头部代码，进行样式表下载，然后继续向下解析HTML代码，构造DOM树，同时进行样式下载。DOM树构建完成后，立即开始构造CSSOM树。如果样式表的下载速度足够快，DOM树和CSSOM树就进入一个并行的过程，当两棵树准备完毕，即可开始构造渲染树，最后进行绘制。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>（1）HTML代码中的JavaScript代码（简称JS代码）会阻断DOM树的构造，因为浏览器认为这段JS代码可能会修改DOM结构，所以必须等待JS代码执行完毕，再恢复DOM树的构造过程。这是由浏览器的安全解析策略决定的，目前并没有指定某个JS代码不涉及DOM的属性。（2）浏览器必须等待样式表加载完成，才能开始构建CSSOM树。（3）还有一种特殊情况，浏览器在解析HTML时遇到JS代码，而此时CSSOM树还未构建完成，则浏览器会暂停脚本的执行（浏览器同时也会暂停继续向下解析HTML代码，从而导致DOM树的构建过程被暂停阻塞），直到CSS样式文件下载完成，并完成CSSOM树的构建，才会重新恢复原来的解析。这也是由浏览器的安全解析策略决定的。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>HTML中的内联JS代码执行的危害之处，不在于它阻断了 DOM 树的构建过程，除非是一段特别恶劣的 JS 代码运行非常慢。通常内联的 JS代码运行大概消耗几十毫秒，也就是暂停构建几毫秒到几十毫秒。它最大的危害在于上面第三种情况提到的，即DOM树构建被阻塞的时间不是只有JS代码运行的时间，而是会加上样式资源文件下载和 CSSOM 树的构建时间，这时浏览器所进行的串行解析过程，与我们在前面期望的DOM树和CSSOM树[1]的并行解析过程相差甚远</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>关于浏览器下载、解析、渲染显示页面的优化策略，根据渲染步骤，大概可以从以下几个方面着手：● 优化HTML代码和结构，缩短HTML下载时间，加快HTML解析速度。● 优化CSS文件和结构，缩短CSS文件下载时间和解析时间。● 合理放置JS代码，避免前面第三种情况的出现，这也是最重要的。</p>
</blockquote>
</blockquote>
<h3 id="◆-1-3-首屏时间" tabindex="-1"><a class="header-anchor" href="#◆-1-3-首屏时间" aria-hidden="true">#</a> ◆ 1.3 首屏时间</h3>
<blockquote>
<blockquote>
<p>受限于不同设备的分辨率的差异，不同用户看到的首屏也是有区别的，通常都由网站的负责人基于各自用户的访问设备情况，来定义一个普适性的标准尺寸，方便后期进行指标量化。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在未加入首屏指标之前，我们使用 StartRender 和 OnLoad 来衡量这个页面的性能：在1s内开始渲染，最终花了5s左右的时间加载完成整个页面。从整体上看，是一个尚可接受的性能数据。而在加入首屏指标之后，这个页面的性能描述是这样的：在1s内开始渲染，花了5s左右完成加载，用户整整等待了4.2s，才看到首屏的内容。很明显，页面存在一些隐含的因素，导致了如上描述的性能问题。那么我们为什么要加入首屏指标，让原本性能尚可的页面一下子变成一个性能糟糕的页面呢？</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>通过统计发现，70%以上的用户点击路径集中在首屏区域。基于此数据，我们认为，至少在我们的电商网站上，页面首屏内容的展示速度会直接影响用户的性能体验。如果在你的网站上，用户的浏览、操作也相对集中在首屏区域，那么就应该制定方案，有针对性地去做首屏展示速度优化。</p>
</blockquote>
</blockquote>
<h3 id="◆-1-4-页面整体加载完成" tabindex="-1"><a class="header-anchor" href="#◆-1-4-页面整体加载完成" aria-hidden="true">#</a> ◆ 1.4 页面整体加载完成</h3>
<blockquote>
<blockquote>
<p>页面加载完成又称为PageLoad，顾名思义，即页面相关资源（CSS样式文件、JS脚本文件、图片等）全部加载完成的时间。当这个时间点被触发时，意味着当前页面对于用户来说是可以进行安全交互的。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>SpeedIndex由WebPagetest提供，通过视频录制、画面到帧的分析来模拟检测首屏的渲染，可以反映在实验室中配置的各种不同终端及网络下，页面首屏内容展示完成的速度快慢。</p>
</blockquote>
</blockquote>
<h3 id="◆-2-1-延迟渲染" tabindex="-1"><a class="header-anchor" href="#◆-2-1-延迟渲染" aria-hidden="true">#</a> ◆ 2.1 延迟渲染</h3>
<blockquote>
<blockquote>
<p>通常为了加快页面渲染的速度，基础的解决思路是，通过去除页面上除首屏以外的对于用户不可见的信息区块，让页面的DOM节点数更少、DOM树结构更简单，从而达到加快页面下载和渲染速度的目的。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>然后使用懒加载异步化请求、BigPipe等方案，动态加载这些不可见的信息区块。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在服务端数据查询请求的优化中，强调的是对于查询请求资源次数的优化，而不是查询结果的大小。在这样的请求中，查询结果一次返回3条数据还是50条数据，所消耗的时间几乎是一样的。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在整个渲染过程中，主要的消耗还是集中在DOM 树和Render树的构建上。其中DOM树的复杂程度对于整个渲染过程的复杂程度起到关键作用，浏览器解析文档构建DOM树，然后遍历DOM树，计算每个DOM树节点的样式，生成Render树。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>减小页面首次渲染时的 DOM 树节点数，并且在不修改服务端输出逻辑的前提下进行。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>希望提供的方案能够将首屏以下不可见的33个结果，在浏览器首次渲染的时候，在页面的 DOM 树中消失，达到加快首次渲染速度的目的。在其处于可见状态的时候，再将其恢复到DOM树中。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在考虑尽量不修改服务端逻辑的前提下（保障可维护性），笔者最终选择使用TextArea来存放首屏以下的33个搜索结果对应的HTML代码。存放在TextArea中的HTML代码，浏览器会解析识别为TextArea内容，而不会被当作DOM节点进行解析。所以通过这个办法，页面首次渲染的DOM树包含的节点数大幅减少，从而大幅提高首次渲染速度。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在服务端，将一些特殊字符（比如&amp;）进行HTMLEncode转义。而在浏览器端，则需要在首屏以下区域处于可见状态时，将TextArea中的HTML代码取出，将其恢复到DOM树中进行渲染。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>这个方案称为“延迟渲染”。在实际运用过程中，根据不同情况，也可以选择使用Script Tag来存放HTML代码。</p>
</blockquote>
</blockquote>
<h3 id="◆-2-2-seo-ajax" tabindex="-1"><a class="header-anchor" href="#◆-2-2-seo-ajax" aria-hidden="true">#</a> ◆ 2.2 SEO Ajax</h3>
<blockquote>
<blockquote>
<p>针对SEO页面的性能优化，会遇到什么问题呢？● 针对爬虫爬取收录的页面，页面内容必须同步展示。● HTML 代码和实际展现的内容要保证一定程度的一致性，否则有被认作作弊页面的风险。</p>
</blockquote>
</blockquote>
<h3 id="◆-3-1-快速了解网站性能" tabindex="-1"><a class="header-anchor" href="#◆-3-1-快速了解网站性能" aria-hidden="true">#</a> ◆ 3.1 快速了解网站性能</h3>
<blockquote>
<blockquote>
<p>3.1.1 使用YSlow进行性能分析</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>YSlow性能检测插件，它以Firefox插件的形式，几乎成为了所有前端开发人员的标配工具。YSlow遵守基于所有基础的性能最佳实践的规则，快速分析HTML文档代码组成</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>Steve Souders在《高性能网站建设指南》一书中总结了12条基本规则：● 尽量减少HTTP请求。● 使用CDN。● 静态资源使用Cache。● 启用Gzip压缩。● JavaScript脚本尽量放在页面底部。● CSS样式表放在顶部。● 避免CSS表达式。● 减少内联JavaScript和CSS的使用，尽可能使用外部的JavaScript和CSS文件。● 减少DNS查询。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 精简JavaScript。● 避免重定向。● 删除重复的脚本。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>一个页面如果遵守了以上规则进行前端开发，那么抛开网络因素，至少可以保证它在页面加载阶段的性能不会产生问题。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>基本上它将焦点都放在了页面的加载阶段，而在加载完成后的浏览器的解析、渲染阶段是空白的。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>Google开源的优化工具PageSpeed</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>3.1.2 使用PageSpeed进行性能分析</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>PageSpeed 也有它的独到之处。它除了能够提供页面在请求加载层面的优化建议，还能提供页面在加载完成后的一系列解析渲染操作的各部分时间，比如：● 页面包含的JavaScript代码的执行时间。● 合理高效的CSS样式代码的建议。● 页面布局渲染的时间等。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>所以我们需要有一种工具，能够帮我们漂洋过海，去了解某个特定地区用户访问网站的真实性能情况。接下来看一下WebPagetest，看看它能帮助我们做哪些事情。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>WebPagetest通过浏览器访问，基于输入的Website URL，以及选择的国家城市、浏览器类型、网络带宽等信息，启动对应的远程服务器上的浏览器进行性能分析测试。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>WebPagetest就能够提供遍布世界各个角落的代表性城市的页面性能分析测试，基本覆盖亚洲、大洋洲、非洲、欧洲、北美和南美洲。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>据官网介绍，他们提供的不同城市的性能分析测试，是通过真实的部署在对应城市的物理服务器来实现的，而不是通过其他类似代理或者虚拟机等方式模拟实现的。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>除了常规的性能分析及优化建议，WebPagetest还提供了页面加载过程的视频录制、关键节点截图、页面加载瀑布图等诸多重要信息。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在性能报表的概括里，有几个重要的性能指标：● First Byte 表示首字节响应时间，该时间可以综合反映出当前连接的网络状况和服务器的响应处理速度。● Start Render表示浏览器开始渲染的时间。● Speed Index表示一种在WebPagetest中自定义的性能指标。● Load Time表示加载时间。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>使用WebPagetest测试页面时，通过对比页面在不同国家的城市中这几个指标的差别，可以大概知道这些国家的用户访问页面的真实体验是怎样的，用户等待空白页面花了多少时间，整个首屏显示完成又花了多少时间，最终用户等待了多少时间才可以正常浏览和使用页面功能。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● WebPagetest很难对登录态依赖Cookie的网页进行测试，基本上无法为测试页面设置正确的Cookie。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● WebPagetest 上的性能测试服务是由人工触发的。每一次测试服务，都是在特定地区的特定机器上启动特定浏览器进行性能分析的，它的分析结果至多只能作为排查某个国家用户访问性能问题时的一个性能报告参考，并不能真正代表访问网站的真实用户的性能体验。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>如果网站已经过了初创时期，正处在稳步上升的阶段，用户的访问量逐步上升，达到了百万、千万级别，更不可能将网站日常的性能分析和监控，寄望于每日通过 WebPagetest 手动测试得到的样本数量在个位数的性能报告。</p>
</blockquote>
</blockquote>
<h3 id="◆-3-2-真实用户前端性能监控" tabindex="-1"><a class="header-anchor" href="#◆-3-2-真实用户前端性能监控" aria-hidden="true">#</a> ◆ 3.2 真实用户前端性能监控</h3>
<blockquote>
<blockquote>
<p>建立一个基础的真实用户前端性能监控系统，大致包含以下5个系统模块的设计开发工作：● 真实用户前端性能数据采集。● 采集数据存储。● 监控系统指标定义及加工计算。● 数据分析、性能报表产出。● 性能基线定义。</p>
</blockquote>
</blockquote>
<h3 id="◆-4-1-最大qps推算及验证" tabindex="-1"><a class="header-anchor" href="#◆-4-1-最大qps推算及验证" aria-hidden="true">#</a> ◆ 4.1 最大QPS推算及验证</h3>
<blockquote>
<blockquote>
<p>在对某电商的活动页面进行优化时，一开始很多人认为Gzip不是影响CPU的最大因子，直到拿出一次又一次的实验数据，团队成员才开始慢慢地接受。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>4.1.1 RT公式一：RT=Thread CPU Time+Thread Wait Time</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>RT（Response Time，响应时间）可以简单地理解为系统从输入到输出的时间间隔</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>服务器端 RT 的含义是指从服务器接收请求到该请求响应的全部数据被发往客户端的时间间隔。客户端 RT 的含义是指从客户端（比如浏览器）发起请求到客户端（比如浏览器）接收该请求响应的全部数据的时间间隔。需要注意的是，服务器端RT+网络开销≈客户端RT。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>客户端的RT直接影响用户体验，要想降低客户端RT，提升用户体验，必须考虑两点，一个是服务器端的RT，另一个是网络。对于网络来讲，常见的优化方式有CDN、AND和专线，分别适用于不同的场景</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>要想降低RT，就要降低Thread CPU Time或者Thread Wait Time。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>如果系统里只有一个线程或者一个进程（该进程中只有一个线程），那么最大的QPS是多少呢？假设RT是199ms（CPU Time为19ms,Wait Time是180ms），那么1000ms以内系统可以接收的最大请求数就是1000ms/（19ms+180ms）≈5.025。所以单线程的QPS变成了：单线程QPS=1000ms /RT</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>假设只有一个线程，这个线程在执行某个请求时，CPU真正花在该线程上的时间就是CPU Time，可以看作19ms，那么在整个RT生命周期中，还有180ms的WaitTime,CPU在做什么？在理想情况下，抛开系统层面的问题，可以认为 CPU 在这180ms 里没做什么，至少对业务来说没做什么。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 一核的情况</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>由于每个请求的接收，CPU只需要工作19ms，所以在180ms的时间内，可以认为系统还可以额外接收180ms/19ms ≈9个请求。由于在同步模型中，一个请求需要一个线程来处理，因此，我们需要额外的9个线程来处理这些请求。这样，总的线程数就是：（180ms+19ms）/19ms ≈10个多线程之后，CPU Time从19ms变成了20ms，这1ms的差值代表多线程之后线程上下文切换、GC等带来的额外开销（如果是JVM），这里的1ms代表一个概数，你也可以把它看作n。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 两核的情况一核的情况下可以有10个线程，那么两核呢？在理想情况下，可以认为最佳线程数为：2 ×（180ms+20ms）/20ms=20个</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● CPU利用率</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>上面举的都是CPU在满载情况下的例子。有时由于某个瓶颈，导致CPU得不到有效利用，比如两核的CPU，因为某个资源，只能各自使用一半的效能，这样总的CPU利用率变成了50%，在这样的情况下，最佳线程数应该是：50% ×2 ×（180ms+20ms）/20ms=10个根据上面的分析，最佳线程数公式如下：最佳线程数=（RT/CPU Time）×CPU核数×CPU利用率</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>1.最大QPS公式推导假设知道最佳线程数，也知道每个线程的QPS，那么线程数乘以每个线程的QPS即这台机器在最佳线程数下的QPS。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>[插图]</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>从公式可以看出，决定QPS的是CPU Time、CPU核数和CPU利用率。CPU核数是由硬件决定的，很难操纵，但是CPU Time和CPU利用率与代码息息相关。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>多线程下的CPU Time（比如高并发下的GC次数增加消耗更多的CPU Time、线程上下文切换等）和单线程下的CPU Time是不一样的，所以会导致推算出来的QPS和实际的QPS有误差。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>尤其是在同步模型下的相同业务逻辑中，单线程时的 CPU Time 肯定会比大量多线程的CPU Time小，但是对于异步模型来说，切换的开销会变小</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>2.CPU Time</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>是一次请求中所有环节上消耗的CPU时间之和。比如在Web应用中，一个请求过来的HTTP的解析所消耗的CPU时间，是CPU Time的一部分，另外，这个请求中请求RPC的encode和decode所消耗的CPU时间也是CPU Time的一部分。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>CPU Time是由哪些因素决定的呢？两个关键字：数据结构+算法。举几个例子：● 均摊问题。● Hash问题。● 排序和查找问题。● 状态机问题。● 序列化问题。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>3.CPU利用率</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>CPU 利用率不高的情况是时常发生的，以下因素都会影响 CPU 利用率，从而影响系统可以支持的最大QPS。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>（1）I/O能力。● 磁盘I/O● 网络I/O○ 带宽，比如某大促压力测试时，由于某个应用放在Tair中的数据量大，导致Tair的机器网卡跑满。○ 网络链路，还是这次大促，借用了其他核心交换机下的机器，导致客户端RT明显增加。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>（2）数据库连接池（并发能力=PoolWaitTime[1]/RT（client）×PoolSize[2]）。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>（3）内存不足，GC 大量占用 CPU，导致给业务逻辑使用的 CPU 利用率下降，而且 GC时还满足Amdahl定律锁定义的场景。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>（4）共享资源的竞争，比如各种锁策略（读写锁、锁分离等），各种阻塞队列，等等。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>（5）所依赖的其他后端服务QPS低造成瓶颈。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>（6）线程数或者进程数，乃至编程模型（同步模型、异步模型，某些场景适合同步模型，某些场景适合异步模型）。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在压力测试的过程中，出现最多的是网络I/O层面的问题</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>1）Amdahl定律（安达尔定律）</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>Amdahl定律是用来描述可伸缩性的</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>可伸缩性是指在一个系统中，基于可并行化和串行化的组件各自所占的比例，当程序获得额外的计算资源（如CPU或者内存等）时，系统理论上能够获得的加速值（QPS或者其他指标可以翻几倍）</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>如果CPU Time和CPU利用率不变，核数从1增加到4,QPS会相应地增加4倍。但是在实际情况下，当核数增加时，CPU Time和CPU利用率大部分时候是变化的，所以前面的假设不成立，即一般场景下最大QPS不能增加4倍。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>由于线程数增加超过某个临界点会影响CPU Time、QPS和RT，所以很难精确测量高并发下的CPU Time，它随着机器硬件、操作系统、线程数等因素不断变化。笔者能做的就是压力测试QPS，并在压力测试的过程中调整线程数，使QPS达到临界点，这个临界点是QPS的一个峰值点。这个峰值点的线程数即当前系统的最佳线程数，当然如果这个时候 CPU 利用率没有达到100%，那么证明系统中可能存在瓶颈，应该在找到并处理瓶颈之后继续压力测试，并且重新找到这个临界点。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>当数据结构发生改变、算法改进或者业务逻辑发生改变时，最佳线程数有可能会跟着变化。</p>
</blockquote>
</blockquote>
<h3 id="◆-4-2-同步模型与异步模型" tabindex="-1"><a class="header-anchor" href="#◆-4-2-同步模型与异步模型" aria-hidden="true">#</a> ◆ 4.2 同步模型与异步模型</h3>
<blockquote>
<blockquote>
<p>关于什么是同步模型，拿Web开发中的thread-per-client举一个经常碰到的例子：（1）浏览器发起HTTP请求，servlet容器接收请求。（2）servlet容器解析请求，并按照业务逻辑请求remote Cache中的内容（Tair等）。（3）servlet容器拿到Cache的返回结果，并进行逻辑运算。（4）servlet容器将计算结果返回。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>假设用Tomcat作为servlet容器，当一个请求过来时（由于流程较为简单，直接用文字描述流程）：（1）Tomcat中监听8080端口的主线程接收了一个socket。（2）主线程把socket交给Tomcat线程池中的某个work线程。（3）线程阻塞式读取socket中的数据，并解析HTTP，组装成request对象。（4）线程获取request对象中的某个数据作为key。（5）根据上一步获取的key请求Tair，线程会进入阻塞状态，等Tair的value返回。（6）value返回，线程被唤醒，将value返回给response。（7）返回结果以阻塞写的方式写入socket的OutputStream。（8）Tomcat的work线程回池，准备处理下一个请求。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>可以看到，线程分别在（3）、（4）、（5）、（6）、（7）步进入block状态，尤其是第（3）步和第（7）步，如果是在广域网上，这个线程将产生大量的WaitTime。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>这就是传统的同步模型，同步模型基本基于BIO。但是上例中的请求Tair并不是BIO，而是NIO，虽然在I/O上是非阻塞的，但是线程依然进入了阻塞状态，不过是阻塞在countdownlatch或者blockingqueue上。对整个模型来说，这依然是一个同步模型。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>1）同步模型的优点</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>同步模型编程简单，符合人类的思维方式，维护成本也低，尤其当有复杂的业务逻辑时，对线程数的要求会下降到几百个甚至几十个，此时使用同步模型是一个不错的选择。在极端情况下，尤其对于一个CPU强密集型的应用，一个请求过来的时候，有较强的业务逻辑，造成了大量的CPU Time，而Wait Time却只占RT很小的比例。这个时候，通过公式算出的线程数将会维持在接近CPU核数的数量级上，使用同步模型是非常合适的。在存在大量业务并且调用和依赖比较复杂的情况下，使用同步模型开发和维护的成本都比异步模型要低很多。这里的开发和维护成本包含代码编写成本和理解成本，以及故障处理的成本等。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>2）同步模型的缺点</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在某些场景下无法达到最优的QPS，按照前面举的例子（假设CPU Time在1ms以内，Wait Time在100ms以上），为了让QPS最大化，需要设置数千个线程，但是由于线程数多，导致CPU Time又增加了（线程上下文切换、虚拟机的开销），于是QPS和理想的最大值比又会下降。在这种场景下，使用同步模型就比较吃力，举一个现实点的例子，比如DNS中的智能路由，本来一个请求的CPU Time就大大小于1ms，如果使用同步模型，Wait Time中会包含大量的I/O Wait Time，于是需要数以千计的线程，这是程序员们无法接受的。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>全异步模型的特点是可以支持大量的链接，并且在业务执行过程中没有任何地方有I/O 阻塞，这个技术特性基于NIO或者AIO才能实现。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在Java中，也有一个比较出名的协程框架叫作Kilim。使用Kilim可以将if elseif用pause和 resume 进行替换，不过这只是在 Java 源代码里的替换，当源代码被编译成了字节码，其实又回到了if elseif之类的跳转模式。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对于异步模型的特点，笔者用两个词来描述，相信仔细看完上述分析过程后对这两个词将不再陌生：suspend（或者pause）和resume。执行某个协程对象时，在任何异步的地方，调用异步方法，并suspend（或者pause），然后等待异步方法的数据返回后调用该对象的 resume，于是该对象就可以在它 suspend（或者pause）的地方开始执行。这便是对全异步模型最化繁为简的描述。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>1）异步模型的优点</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>（1）特定场景下更高的QPS</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>QPS高低是由CPU Time和CPU利用率决定的，只是为了说明改成异步模型之后，处理业务逻辑线程的I/O Wait Time减少了，所以只要少量的线程就可以做大量的事情，而如果用同步模型，线程的数量会导致提前达到临界点，这个临界点的QPS距理论最高QPS还有一段距离。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>（2）线程数容易固定，导致系统不易过载，高压时只是队列长度增加。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>为什么这样说？因为在同步模型下，要为每个应用都设置接近临界点的线程数，这是要花费大量时间来测试的，而这个投入很多时候产出并没有那么高。这时我们往往会设置一个较大的线程数，比如400，而对于一些应用来说，这400个还没有到达临界点，在突然的高压下系统性能得不到充分的发挥。对于另外一些系统来说，已经超过了临界点，高压时系统的 QPS下降，而RT上升。在异步模型下，线程数的设置较为简单，在突然的高压下，虽然RT也有上升（因为请求在队列中等待处理需要时间），但是可以省掉同步模型中线程不断切换带来的开销，而系统的最高QPS基本不会受到影响。异步系统中的线程将会有条不紊地继续工作。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>（3）限流。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>可以通过队列长度来限流。不做好流控，会造成大量请求堆积，如果得不到及时处理，可能导致full GC频率加快，这是一个特点。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>2）异步模型的缺点● 异步模型相对同步模型而言较为复杂，存在大量的异步回调，虽然Kilim等框架可以在一定程度上解决代码开发和维护成本问题，但是如果交互异常复杂，存在大量异步交互的场景，尤其是业务逻辑经常变更的 Web 系统，即使使用协程，开发成本和维护成本及稳定性依旧是不小的挑战。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>4.2.3 为什么异步模型需要的线程数少1.业务线程数减少的原因</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>为什么同步模型需要那么多线程，异步模型只需要几个线程呢？这主要归因于非阻塞式I/O。在同步模型下，线程必须阻塞在I/O上面，阻塞在I/O上的时间是要计入线程的Wait Time的，所以在同步模型转化为异步模型之后，业务线程在 I/O 上阻塞的时间大大减少，根据最佳线程数公式：最佳线程数=（（Wait Time+CPU Time）/CPU Time） ×CPU核数×CPU利用率</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>一旦Wait Time逼近0（只是逼近，因为程序中肯定还存在串行的地方，比如GC等），那么最佳线程数可以近似看作：最佳线程数=（CPU Time/CPU Time） ×CPU核数×CPU利用率所以这可以视作：全异步模型中线程数=CPU核数×CPU利用率。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>2.I/O线程</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>那么在什么场景下使用异步模型比较合适呢？举两个例子。比如DNS服务，又比如广告服务，尤其是在视频网站上打开一个视频的时候，先来一段广告，这个广告服务使用异步模型比较合适，广告放完之后视频网站会告诉用户的浏览器去哪里下载视频（GSLB），这个视频地址路由服务使用异步模型也比较合适，因为这两种服务一个选择视频地址，另一个选择要播放的广告，具备下述特点。● CPU Time都比较小。● 后面依赖的数据是可以异步获得的。● 该服务在同步模型下Wait Time比较大（线程需要阻塞在网络I/O上读写广域网的请求）。● 在同步模型下QPS与理论值差距太大。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在满足这些条件的场景下使用异步模型效果都会比较好。因为如果在这个场景下使用同步模型，那么Wait Time确实会比较大，而CPU Time本身比较小，需要大量线程才能达到预期的QPS，大量线程带来的影响前面也讲过了，最终会降低系统的最大QPS。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>如果应用本身CPU Time比较大，通过计算或者压力测试，几十个或者四五百个线程就达到了系统预期的QPS，尤其是搜索Web应用这种场景使用异步模型就不那么合适了。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>总之，不管是同步模型还是异步模型都是满足最大QPS公式的，只是因为使用了非阻塞式I/O，降低了整体的Wait Time，导致所需的业务线程数降低了。异步模型虽然在线程数和QPS方面有优势，但是如上所述它也有劣势，我们要根据应用的特点来判断使用同步模型还是异步模型，顺势而为。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>（1）QPS是由CPU Time和CPU利用率及CPU核数决定的。（2）最佳线程数是由CPU Time和Wait Time及CPU核数、CPU利用率决定的。（3）多线程下CPU Time增加，以及Admahl定律中影响加速比的因素。（4）CPU Time是由数据结构和算法决定的。（5）CPU利用率与架构和串行、编程模型和系统中有无其他瓶颈相关。（6）要做性能优化，必须考察CPU Time降低的百分比和CPU利用率提高的百分比。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>（7）性能优化同时要考虑串行和并行的比例。（8）处理某个业务的最佳线程数存在一个临界点，超过这个临界点的线程数，QPS会有所下降，RT也会明显增加。（9）选择同步模型还是异步模型，要根据应用的特征仔细斟酌。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>关键是通过最大 QPS 公式和实际测量数据的协同计算证明：显然在某电商的活动页面上Gzip是影响CPU Time的最大因素，排名第二的是PHP代码消耗的CPUTime，而通过预压缩之后，Gzip的开销和PHP代码的开销都被节省了，所以出现了QPS被提升很多倍的情况。</p>
</blockquote>
</blockquote>
<h3 id="◆-4-3-数据结构对性能的影响" tabindex="-1"><a class="header-anchor" href="#◆-4-3-数据结构对性能的影响" aria-hidden="true">#</a> ◆ 4.3 数据结构对性能的影响</h3>
<blockquote>
<blockquote>
<p>要知道 HashMap 是什么，首先要搞清楚它的数据结构。在 Java 编程语言中，最基本的结构有两种（其他编程语言也类似）：● 数组。● 模拟指针（引用）。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>基本上所有的数据结构都可以用这两个基本结构来构造，HashMap也不例外。HashMap实际上是一个数组和链表的结合体（在数据结构中，一般称之为“链表散列”）</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>一个HashMap就是一个数组结构，当新建一个HashMap的时候，就会初始化一个数组。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>数组中的每一个元素，笔者都会用桶（bucket）来描述。那么使用HashMap的时候是如何操作这个数组和其中的链表的呢？1.往HashMap中put元素时（1）根据key的Hash值，通过Hash算法计算得到这个元素在数组中的位置（即下标），也就是桶，然后就可以把这个元素放到对应的桶中了。（2）如果这个桶里已经存放其他元素了，那么在这个桶里的元素将以链表的形式存放，新加入的元素放在链头，最先加入的元素放在链尾。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>2.从HashMap中get元素时（1）计算key的HashCode，通过Hash算法找到数组中对应位置的那个桶。（2）通过key的equals方法在桶里的链表中找到需要的元素。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>从上述流程可以看到，不管是get还是put，都离不开一个关键字，就是Hash。通过Hash算法计算某个 key 在数组中具体的桶，可以想象，如果每个桶里的链表只有一个元素，那么HashMap的get效率将是最高的</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>什么是碰撞（collision）？拿HashMap举例，如果两个key通过Hash算法计算之后在数组中得到的位置相同，即在同一个桶里，那么就是碰撞了。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>如果在一个有多个元素的HashMap中，通过Hash算法运算之后有6个元素都在同一个桶中</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>当执行get操作时，首先根据Hash算法算出桶的位置，然后需要在这个桶里执行6次equals方法才能找到对应的元素。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>这意味着桶里的数据越多，碰撞得越厉害，get时间越长；桶里的数据越少，get性能越高。尤其在没有碰撞的时候，每个桶里只有一个元素，那么整个Map的性能是最优的。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>所以，我们要做的就是努力减少碰撞，减少碰撞有如下两种方法。（1）优化Hash算法。（2）增加数组的大小。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>Hash算法的目标是：既要性能高，又要碰撞少。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>2.计算key在数组中的具体位置</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>首先计算key的Hash值（这个HashCode并不是hashcode方法返回的，而是HashMap中Hash 方法返回的 Hash 值），然后跟“数组的长度-1”做一次“与”（&amp;）运算。比如数组的长度是24，那么HashCode就会和“24-1”做“与”运算。很多人都有这个疑问：为什么HashMap的数组初始化大小都是2的指数幂时，HashMap的效率最高？</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>2）HashMap中数组的大小</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>总结一下HashMap中Hash算法减少碰撞的两个做法：● Hash方法中存在奇怪的位移运算，以使最终的Hash值中0和1较为均匀。● 使用h &amp; （length-1）来进行桶的定位，length应为2的指数幂。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>HashMap什么时候进行扩容呢？当HashMap中的元素个数超过数组大小loadFactor时，就会进行数组扩容，loadFactor 的默认值为0.75。也就是说，在默认情况下，数组大小为16，当HashMap中元素个数超过16×0.75=12的时候，就把数组的大小扩展为2×16=32，即扩大一倍，然后重新计算每个元素在数组中的位置。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>这是一个非常消耗性能的操作，所以如果能预知HashMap中元素的个数，那么提前设置好元素的个数能够有效地提高HashMap的性能。比如，有1000个元素，就new HashMap（1000），但是理论上new HashMap（1024）更合适，不过上面已经说过，即使是1000也会自动将其设置为1024。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在HashMap的数据结构中讲了get方法的过程：首先计算key的HashCode，找到数组中对应位置的某一元素，然后通过key的equals方法在对应位置的链表中找到需要的元素。所以，HashCode与equals方法对于找到对应元素来说是两个关键点。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>HashMap 的 key 可以是任何类型的对象，例如 User，为了保证两个具有相同属性的 User的HashCode相同，需要改写HashCode方法，比如把HashCode值的计算与User对象的ID关联起来，只要User对象拥有相同ID，那么它们的HashCode也能保持一致，这样就可以找到其在HashMap数组中的位置。如果这个位置上有多个元素，还需要用key的equals方法在对应位置的链表中找到需要的元素，所以只改写HashCode方法是不够的，equals方法也是需要改写的。当然，按正常思维逻辑，equals方法一般都会根据实际的业务内容来定义，例如，根据User对象的ID来判断两个User是否相等。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>HashMap不支持多线程操作，正确的做法是不要让多线程并发操作HashMap，一旦这么做可能出现以下场景。● 数据丢失（想想多线程操作链表或者数组会发生什么）。● 死循环（再想想多线程操作数组或者链表会发生什么）。● 抛出并发异常。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>1.均摊分析原理</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>均摊分析是对一组操作的分析。特别是均摊分析允许处理这样一种情况，n 个操作在最差情况下的代价小于任何一个操作在最差情况下代价的n倍。均摊分析并不是把注意力集中到每个操作的单独代价上，再把它们加起来，而是看一组操作的整体代价，再把整体的代价分摊到每一个单独的操作上。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>2.均摊在Java数据结构中的体现</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>数据结构基本都由两部分内容组成。1）ArrayList</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>2）StringBuilder（StringBuffer）</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>3）ByteArrayOutputStream</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>均摊原本的理念是为了说明把整体的代价分摊到每一个单独的操作上是可行的，而且确实在绝大多数场景中都是合理有效的，但是在实际使用的过程中，在少数场景下还是会出现误用，导致CPU Time增加，Wait Time有时也会增加，这不但降低了系统的QPS，还降低了系统的RT。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>数组是无法扩容的，当数组空间不够的时候，会创建一个更大的数组，把原来数组的数据全部复制到新的数组中，再把新的数据追加到新的数组的后面。这是典型的均摊的实现。</p>
</blockquote>
</blockquote>
<h3 id="◆-4-4-算法设计不合理带来的性能问题" tabindex="-1"><a class="header-anchor" href="#◆-4-4-算法设计不合理带来的性能问题" aria-hidden="true">#</a> ◆ 4.4 算法设计不合理带来的性能问题</h3>
<blockquote>
<blockquote>
<p>根据线程号在jstack中找到线程名（jstack中的线程号是十六进制，所以要把top-H中的十进制PID转换成十六进制）。</p>
</blockquote>
</blockquote>
<h3 id="◆-4-5-综合案例-电商活动页面性能优化" tabindex="-1"><a class="header-anchor" href="#◆-4-5-综合案例-电商活动页面性能优化" aria-hidden="true">#</a> ◆ 4.5 综合案例：电商活动页面性能优化</h3>
<blockquote>
<blockquote>
<p>通过APC使QPS提高近3倍</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在数年前的某次大促中，A应用有多台机器的Load增至100多，还有一台机器的Load增至240，导致宕机，这发生在集群有30台机器的情况下（30台机器只能支撑6000的QPS）。这次大促的成交额预计翻三倍，这样带来的问题是，如果不做优化，A 应用的机器量直接要增至100台以上了。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>首先学习的是PHP语言的工作机制，上网查了一些资料，发现PHP会产生OPCode这样的中间代码，这和Java编译之后产生字节码的道理是一样的（理解字节码对程序员来说是非常重要的技能，研究Kilim的时候，就是通过Kilim产生的字节码了解了实现coroutine的方法）。还有一个重点是，这个OPCode在默认的PHP环境里，每次请求都需要重新产生OPCode，这绝对会导致资源浪费。试想一下，Java类每次执行之前都Javac一下，程序员们会有什么反应，他们一定会说：“真是坑啊。”</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>PHP其实也是可以缓存OPCode的，而且业界有不少组件，其中比较出名的是EACC和APC</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在PHP 5.5以上的版本中，已经自带了这样的功能，但是并非通过APC或者EACC实现，而是另外一个插件OPCache</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>OPCache通过将PHP脚本预编译的字节码存储到共享内存中来提升PHP的性能，存储预编译字节码的好处就是，省去了每次加载和解析PHP脚本的开销。PHP 5.5及后续版本中已经绑定了OPCache扩展。PHP 5.2、5.3和5.4版本可以使用PECL扩展中的OPCache库。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对于部署模型优化，之前对这种“解释型”语言的部署模型有过一点研究，比如Python，可以使用Apache+mod_python，也可以使用Apache或者Nginx+fastcgi的模型。那么PHP是不是也是这样的呢？查阅了一下资料，发现PHP基本也有这两种部署模型。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>CPU Time、CPU核数及CPU利用率决定了QPS，而在我们的场景下，APC片段缓存带来的CPU Time的缩短在Gzip面前简直不值一提（如果片段缓存中的业务逻辑很复杂，那么片段缓存的优化效果会比较显著，关键就看片段缓存生成所占用的CPU Time占整个CPU Time的比例，比例越大，片段缓存优化的效果越明显，比例越小，片段缓存优化的效果越不明显</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>开启了Gzip,Apache（worker）+mod_php 的性能还不到Nginx+fastcgi+APC性能的一半，</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● PHP也是有字节码的。● PHP的字节码默认情况下是不缓存的，每次都需要“解释”。● PHP 5.5以上将自带字节码缓存。● 加上字节码缓存之后QPS提高2倍多，为大促节约了一半以上机器资源。● Gzip在我们的场景下已然成为CPU“杀手”。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>通过APC和部署模型的变化，QPS提升了两倍多。但是之前线下没有开启Gzip的时候，加装APC和实现片段缓存之后，性能可以提高9倍，为什么开启Gzip之后整体效果下降了，只能提高2～3倍了？原因就在于Gzip。进行压力测试时笔者发现，在开启Gzip的情况下</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>有时候Nginx消耗的CPU资源在75%左右</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>一旦把Gzip关闭，Nginx的CPU资源消耗立刻下降</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>如图4-31所示是压缩级别对 QPS 和带宽的影响（92KB、138KB、182KB、248KB、295KB是页面大小）。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>压缩级别一下降，QPS就提高很多，RT也降低很多，但是页面大小会增加，从而导致带宽增加</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>把Gzip关闭或者降低压缩级别会提高带宽消耗，经过详细的压力测试（对不同大小的页面，通过不断地调整压缩级别）和计算，降低压缩级别带来的QPS的提高所节约的机器费用和带宽提升所带来的费用提升不相上下（机器按照3年折旧来算）</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>降低压缩级别的方法看上去很有效，但是却没办法操作。在这个场景下Gzip又是消耗CPU资源最大的用户，我们貌似进入了僵局，优化看上去已经结束了，因为Gzip是避不开的。如果不压缩，带宽的消耗会损失很多钱。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>Gzip在活动页面对性能的影响非常大，因为活动页面没有复杂的计算逻辑，所以消耗CPU资源的大户只有Gzip</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>节约机器的钱和带宽增加带来的成本投入是差不多的。而且降低压缩级别导致的RT 上升也对国际友人的用户体验不好，所以降低压缩级别在这样的场景里也是不可靠的。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>1）压缩后数据放哪里</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 放共享内存中？放在共享内存中就简单得多，但是需要一个工具，这个工具能够让PHP把数据放到系统的共享内存中。最好是现有的CDN集群就支持，有这样的集群吗？有的，就是APC。前文中讲到，APC可以将PHP代码的字节码缓存起来，但是笔者没有讲的是，APC其实还有一个功能，叫作User Cache。何为User Cache？即可以把用户数据存储在这里的一种Cache，而不只是PHP的字节码数据。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>APC的缓存清空是跟TTL相关的，而不是LRU，所以先进缓存的数据，即使没有人使用，不到时间不会被清除，这会导致先进缓存的数据在缓存过期之前一直在缓存中，所以 user_ttl时间不要设置为0，且gc_ttl也要大于0，这样长时间不被访问的页面会被清出缓存，这是不错的选择。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>如果内存不足，APC会返回失败，告知PHP进程。也就是在最差情况下，当共享内存不够时，页面就得不到缓存，那么就需要每次都做Gzip，这个最差的结果和目前的情况是一样的，也就是说最差也不过就是回到现状，只不过打点的工作需要PHP代码来实现了，这还是可以接受的。所以笔者决定用APC来存储压缩后的HTML页面。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>由于返回的HTML已经被PHP压缩过了，那么Nginx或者Apache再压缩一遍其实是浪费，而且不光是浪费，在 Firefox 下，重复压缩的数据还不能正常显示。我们不能简单粗暴地关闭Nginx压缩，因为不使用这套方案的PHP页面或者Nginx后面的其他进程，比如Node.js之类的，还是需要用到Nginx压缩的，最好的方案就是在返回端有一个标识，有了这个标识之后，Nginx就不再压缩返回数据，而且不影响浏览器显示。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>可以在 gzip_types 上做点设置吗？可以，比如，只要返回 content-type=text/plain 就不执行压缩。普通的PHP页面没有使用指定的content-type，默认使用text/html，并且默认会压缩，这也不失为一种方案。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● gzip_min_length比如压缩过的页面都是小于50KB的，那么可以设置为大于50KB才压缩，小于50KB不压缩。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 一定要5分钟之后生效吗当然不一定，尤其现在的大促活动页面都是定制的，如果有紧急信息发布，只需要在修改时将缓存的key改一下即可，比如，原来APC User Cache中存储的Gzip对应的key是123，那么在紧急发布时，新页面的key是456即可，原来的压缩数据在5分钟之后会被放入GC队列，然后等待被GC回收。● 很担心PHP的压缩效率不用担心，PHP的压缩和Nginx的压缩都调用相同的库，都使用zlib库，而且如果将页面全部缓存，同一个页面几分钟才需要做一次压缩，所以PHP的执行效率在这个场景下是不用担心的。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>如果以下这两个条件有一个不满足，那么就无法使用Web Cache来缓存压缩之后的数据。第一，需要在CDN集群上部署Web Cache，现在是没有的，不过部署起来也不是难事。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>第二，对于PHP代码中出现根据user-agent等header属性决定显示什么样的HTML来说，这个需求直接用代码压缩的方案来实现就很方便了，根据 user-agent 中的部分核心属性（为什么是部分核心属性，因为user-agent太多了，如果每个user-agent都作为一个key，则同一个页面会产生大量的副本，对需求来说只是为了分辨出是Mobile还是PC，所以只要为数不多的几种key而已，而且每种key对应的HTML也是不一样的，不存在同一个页面有不同副本的问题），渲染出不同的HTML，然后压缩并通过不同的key缓存在共享内存中。就好像这个需求和方案是天生一对一样，如果用SWIFT来缓存不同user-agent的页面，同一个PHP页面，将会产生很多份缓存，这样热点就不明显了，命中率也会受到影响。因为在WebCache中间件上是完整的user-agent来做key的一部分的，所以user-agent越多，副本就越多。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>笔者简单列一下：（1）知道在一些场景下，Gzip可能是消耗CPU资源的大户，大家可以观察一下自己的应用。（2）预先把浏览器需要的数据压缩之后放入缓存会带来 QPS 的极大提高（多高？笔者这个场景是10倍，取决于Gzip在整个CPU Time中的比重，读者的场景未必有这么多，也有可能更多）。（3）虽然笔者是预压缩的HTML，但是不代表不能压缩Ajax返回的Json，对应返回的Json数据超过100KB的，每次都压缩一下Json和把Json压缩完放在内存的效率就不再多阐述了。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 100KB压完之后，只有17KB左右，内存占用少。● 长时间内，只压缩一次，CPU Time占用很少，提高QPS。● 没有打点的问题，操作起来非常简单。</p>
</blockquote>
</blockquote>
<h2 id="◆-第5章-tcp优化" tabindex="-1"><a class="header-anchor" href="#◆-第5章-tcp优化" aria-hidden="true">#</a> ◆ 第5章 TCP优化</h2>
<blockquote>
<blockquote>
<p>大型网站不仅有图片还有动态请求，经过TCP层面优化，可以让网络耗时变短。而在CDN层面，由于大型网站还有大量的图片和JS等资源，这些静态资源占整个页面请求的90%以上，所以只要性能提升10%，整体的性能体验改观就相当明显。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>跨境最大的挑战是地理距离长，网络延迟天然巨大，例如北美洲到欧洲的网络距离RTT在250ms以上。亚洲到北美洲的网络距离RTT在200ms以上。根据数据初步估计，南美洲的某个国家到达北美洲的美国的请求丢包率高达6%～10%。一旦丢包，大部分都会发生超时重传，基于3次重复ACK的快速丢包发现算法，在很多情况下没有起到作用。减少丢包时的网络延迟，对跨境业务来说，比淘宝在国内得到的效益将更加可观。TCP协议栈优化涉及的地方非常多，从增大初始拥塞窗口到减少默认的RTO、PRR、Early Transmit</p>
</blockquote>
</blockquote>
<h3 id="◆-5-1-tcp传输原理" tabindex="-1"><a class="header-anchor" href="#◆-5-1-tcp传输原理" aria-hidden="true">#</a> ◆ 5.1 TCP传输原理</h3>
<blockquote>
<blockquote>
<p>TCP通过“发送－应答（ACK确认）－重传机制”来确保传输的可靠性，它是端到端进行传输的。对于大型网站，响应报文从服务器端给客户端浏览器进行报文的传输，所以在服务器端，可以通过优化来降低响应给客户端的网络耗时。传统的TCP底层实现，在很多时候会导致TCP传输效率变低，特别是网络带宽的扩充和整体网络硬件技术的提升，使得网络传输速度有很大的变化。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>TCP传输是分段的，一个HTTP响应报文会被操作系统切成多个MSS大小（一般为1460 B）的段，发送端每次只会发送若干段。能够发送多少个数据包，由拥塞窗口和接收端窗口共同决定，直到接收端接收到完整的报文为止。在此过程中，报文分段按照顺序进行发送，每个报文段在发送时，会做顺序编号，以便能够完整正确地组装，所以当HTTP的请求响应模型将请求发送给服务器时，服务器响应都需要多个RTT的传输，物理距离越远，总体网络耗时越长。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>每个报文发送多少段，就是由TCP的底层拥塞控制算法来进行控制的。报文越大，受拥塞控制算法的影响也越大，这也是本章可以重点优化的地方。对于大型网站来说，一般有很多页面，这些页面经过HTTP的压缩之后，仍然会高达数十KB，甚至数百KB。而当远距离传输时，一个数据包的来回网络耗时少则数十毫秒，多则数百毫秒，优化空间更大。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>滑动窗口本质上是描述接收方的TCP数据报缓冲区大小的数据的，发送方根据这个数据来计算自己最多能发送多长的数据。如果发送方收到接收方的窗口大小为0的TCP数据报，那么发送方将停止发送数据，等到接收方发送窗口大小不为0的数据报的到来。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>关于滑动窗口，介绍3个术语。● 窗口合拢：当窗口的左边缘向右边缘靠近的时候，这种现象发生在数据被发送和确认的时候。● 窗口张开：当窗口的右边缘向右边缘移动的时候，这种现象发生在接收端处理数据以后。● 窗口收缩：当窗口的右边缘向左边缘移动的时候，这种现象不常发生。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>TCP就是用这个窗口，慢慢地从数据的左边缘移动到右边缘，把处于窗口范围内的数据发送出去（但不用发送所有数据，只是处于窗口内的数据），这就是窗口的意义。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>接收端流量控制在局域网内做流量控制是可行的，但是在公网上就会出现问题。网络数据包在传输过程中，要经过很多路由器的转发，而这些路由器的带宽和缓冲区大小是不确定的。路由器的缓冲区太小，会造成数据包大量堆积甚至拥塞，而接收端的缓冲区一般很大，此时会造成大量的网络拥塞，从而加剧整个网络的拥塞，甚至造成整个互联网不可用。为了解决这个问题，TCP发送方需要确认连接双方线路的数据最大QPS是多少，这就是所谓的拥塞窗口。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>拥塞窗口的原理：TCP发送方首先发送一个（或者数个）数据报文段，然后等待对方的回应，ACK回应后就把这个窗口的大小加倍，然后连续发送两个数据报，对方回应以后，再把这个窗口加倍，这个机制就是发送端拥塞控制的慢启动算法。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对慢启动算法的简单理解：先发送少量的数据报文段，得到确认后，再将发送报文段的个数增加，直到出现超时错误或者丢包；发送端因此了解到网络的承载能力，也就确定了拥塞窗口的大小，发送方就根据这个拥塞窗口的大小发送数据。在下载文件时，一般开始比较慢，后面会慢慢变快，直到达到一个稳定的速度。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>拥塞避免：当发生超时没有得到对方的确认时，发送的报文段个数会线性增长，这就是拥塞避免阶段</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>快速重传算法改进：等待超时定时器溢出会造成重传数据包的时间过长，通常在数秒，因此为了避免这个问题，TCP专家们发明了快速重传算法，即如果发现3个重复的ACK确认，那么立即发起重传，从而减少网络耗时。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>5.1.4 传统TCP拥塞控制问题● 问题一：慢启动会造成网络耗时变长，拥塞窗口默认较小，无论多大的带宽，默认初始值都为3。● 问题二：当偶尔出现超时时，发送报文段的个数会出现指数退避，直接减半，在某些情况下会导致网络利用率不足。● 问题三：TCP建立连接需要三次握手，但是如果握手期间发生超时或者丢包，只能等待超时定时器溢出，一般需要3s，这会导致连接耗时过长。</p>
</blockquote>
</blockquote>
<h3 id="◆-5-2-linux内核升级中的tcp优化技术" tabindex="-1"><a class="header-anchor" href="#◆-5-2-linux内核升级中的tcp优化技术" aria-hidden="true">#</a> ◆ 5.2 Linux内核升级中的TCP优化技术</h3>
<blockquote>
<blockquote>
<p>从传统的FEO优化、用户流程优化、业务优化到内核优化，是格局和思想的转变和扩充，打开了视野，在AliExpress全球化用户体验提升过程中，LinuxTCP优化已经作为非常重要的手段来解决丢包引起的超时问题，并且取得了非常好的效果。尤其是一些物理距离特别远的国家，其用户在访问AliExpress网站时有着非常高的丢包率，所以有非常多的网络延迟超大的情况。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>我们平时经常会遇到这种情况，租了100MB电信的宽带，但是下载速度并没有提升多少，还是每秒几兆字节的速度。这里面有多种原因，例如下载服务器的下行带宽有一定的上限，在高并发的同时进行下载，可能会造成带宽增至上限；还有中间路由器的缓冲区大小受限，以及运营商内部的某些限制，都会造成理论带宽和实际的下载速度不成比例，除这些因素，主要原因是接收窗口的rwnd设置不合理。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>实际上接收窗口rwnd的合理值取决于BDP的大小，也就是带宽和延迟的乘积。假设带宽是100Mb/s，延迟是 100ms，那么计算过程如下：BDP=100Mb/s ×100ms=（100 /8）MB/s ×（100 /1000）s=1.25MB</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>如果想最大限度提升QPS，接收窗口rwnd不应小于 1.25MB。说点引申的内容：TCP使用16位来记录窗口大小，也就是说最大值是64KB，如果超过它，就需要使用TCP_Window_Scaling机制。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>此优化方法在很多时候效果并不是很明显，可以尝试一下，因为实际上网络访问速度取决于多种条件，比如用户和机房的距离、中间路由器的拥塞状况，又如前面描述的能够发送报文段的个数由接收端窗口和拥塞窗口共同决定，而经过多少路由器是不能控制的，当然也可以和运营商谈，优化路由，但是成本可能会很大。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>初始拥塞窗口的调整，可使TCP的传输效率得到极大提升，这是目前最好的方法。TCP可以调整的参数达百项，大部分默认选项是适用于大部分情况的。Google有篇论文完整地论证了拥塞窗口调大传输效率可提升30%左右，网络延迟可以减少约30%。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>HTTP报文会按照MSS大小（一般为1.46KB）分成多个TCP报文段进行传输，TCP 连接建立后，首次可以并行连续发送报文段的个数是由初始拥塞窗口大小决定的，对于14.6KB大小的HTTP报文，Linux早期版本设置的初始拥塞窗口大小是3，至少需要4个RTT才能完成传输。如果初始拥塞窗口是10，那么只需要一个RTT 即可发送完毕。这个窗口大小也适用于无线TCP优化的场景。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>Linux 2.6.38跳跃性地将初始拥塞窗口和初始接收窗口（Inital Receive Window）从3升到10，这是非常简单而有效的方式。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>1.快速重传的问题快速重传通过3次重复ACK，根据ACK报文中sack选项的内容，快速发现丢了哪个包，而不用等待超时定时器溢出时才发现，从而减少网络延迟。这个延迟不是由网络转发造成的，而是由TCP的重传机制造成的。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在下列情况下，快速重传不能发挥作用。（1）当TCP拥塞窗口大小小于4时，如果第一个报文段丢失，由于没有足够的重复ACK，造成ACK不足3个，快速重传将不能起作用。在Linux低版本下，初始拥塞窗口默认为3，在TCP三次握手之后，连续能发送3个TCP报文段，当第一个报文段丢失，100%只能靠重传定时器溢出来进行重传。（2）ACK丢失，在上面的情况下，窗口不够大（例如4个），也会造成ACK不足。（3）当 TCP 报文段在窗口结尾处丢包时（没有足够可用的报文段），由于本身没有后续的报文段发送，造成ACK不足。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在 TCP 三次握手过程中（SYN、SYN+ACK、SYN），如果出现 SYN 报文丢失，第一次RTO无法度量，会取Linux默认的RTO，作为重传定时器的超时时间。在跨境全球化的背景下，由于地理距离长，再加上BGP选路的复杂性（出于成本考量，运营商可能会采取限速，也可能采取网络路由绕路的方式降低成本），非常容易丢包。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>TFO是TCP Fast Open的简称，传统的TCP需要经过三次握手才能传送数据，而TFO在第三次SYN报文发送的时候会把请求报文段也随着SYN报文发送给接收端。Google研究发现，TCP三次握手是页面延迟时间的重要组成部分，所以他们提出了TFO：在TCP握手期间交换数据，这样可以减少一次RTT。根据测试数据，TFO可以减少15%的HTTP传输延迟，全页面的下载时间平均节省10%，最高可达40%。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>第1步，用户向服务器发送SYN包并请求TFO Cookie。第2步，服务器根据用户的IP地址加密生成Cookie，随SYN-ACK发给用户。第3步，用户缓存TFO Cookie，并向服务器发送SYN包并携带TCP Cookie，同时请求实体数据。第4步，服务器校验Cookie。如果合法，向用户发送SYN+ACK，在用户回复ACK之前，便可以向用户传输数据；如果Cookie校验失败，则丢弃此TFO请求，视为一次普通SYN，完成正常的三次握手。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>目前TFO被植入了Linux 2.6.34内核，但是并没有被默认开启，在Linux 3.13内核中默认被开启。由于TCP是双工协议，客户端必须能够支持TFO的协议才能完成TFO的过程。Chrome浏览器支持TFO，但是在Linux、Chrome和Android操作系统中，TFO默认是关闭的。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>TSO（TCP Segment Offload）和GSO（GenericSegmentation Offload）并不是优化TCP传输效率的方式，而是网卡层面的优化，其目的是减少内核的CPU消耗。传统的TCP报文分段是在内核中完成的，而TSO和GSO将报文分片工作留给网卡来做，以减少内核的CPU消耗</p>
</blockquote>
</blockquote>
<h3 id="◆-5-3-time-wait问题案例分析" tabindex="-1"><a class="header-anchor" href="#◆-5-3-time-wait问题案例分析" aria-hidden="true">#</a> ◆ 5.3 TIME_WAIT问题案例分析</h3>
<blockquote>
<blockquote>
<p>5.3 TIME_WAIT问题案例分析</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>实际系统在线上运行时，经常在高并发时大量的Socket处于TIME_WAIT状态，导致TCP连接不可用。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>线上压力测试时出现大量的错误：cannot assign requestedaddress:proxy:HTTP:attempt to connect to……QPS达到500时就出现大量这样的错误。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>压力测试的用例非常简单：施压机发送一个URL到Apache,Apache经过Rewrite先代理到80端口，再进行Rewrite并通过Mod_JK转发给JBoss进行处理</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>通过 netstat 统计命令查看 TCP 连接的使用情况，从下面的情况可以看出存在大量的TIME_WAIT状态连接，TIME_WAIT一般由主动关闭端产生，可以确定是短连接。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>TCP运用了可靠连接关闭，即经过双方的确认后再关闭连接，避免双方因不知道连接关闭造成业务问题。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>如果客户端主动关闭连接，客户端先发送FIN包给服务器端，服务器端收到FIN包，回应ACK给客户端，表示服务器端已经准备关闭连接了，此时服务器端将连接状态设置为CLOSED_WAIT，为了确认回应给客户端的ACK已经收到，服务器端再发送一个FIN包，服务器端的连接此时处于LAST_ACK状态，客户端接收到服务器端发送的FIN包之后，将连接状态设置为TIME_WAIT,TIME_WAIT状态存在的主要目的是防止迷途报文重现，影响新连接，所以要等待足够长的时间。特别是在公网传输过程中，有时发送报文后，如果接收端一直没有收到，那么可能会出现 ACK 不能回应给发送端的情况。客户端将 FIN ACK 回应给服务器端，服务器端收到后，将连接设置为CLOSED状态，此时将不再接收客户端发送的任何报文。客户端经过2个MSL大约60s将连接也设置为CLOSED状态。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>注意TCP是端到端的协议，如果连接不成功，可能是由于端口数量到达上限。查看服务器的端口范围可以使用如下命令：cat /proc/sys/net/ipv4/ip_local_port_range 32154 61500</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>本地启用的端口数量最多是28000，也就是服务器能够建立的TCP连接数不能超过这个数量，可以看出本案例中端口的数量已经达到上限。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>针对这种情况，很明显要在TIME_WAIT上下工夫，要么减少TIME_WAIT状态的连接数量，要么定位到短连接的原因。在紧急情况下，如果对业务不清楚，而线上已经发生故障，需要在最短的时间内解决问题。开启TIME_WAIT状态的TCP连接重用，可以看到TIME_WAIT状态的连接数量大幅下降，TIME_WAIT状态的连接数量取决于连接的使用情况：vi /etc/sysctl.conf编辑：net.ipv4.TCP_tw_reuse=1 net.ipv4.TCP_tw_recycle=1运行生效：/sbin/sysctl-p</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>短连接的问题上，Apache的跳转规则如下：RewriteRule ^/e/（.<em>）$ http://localhost:80/getKey.htm?token=$1[L,QSA,P]P表示代理模式，是本机发起到本机的代理，通过建立TCP连接进行通信。有没有可能通过走本机的方式（不走TCP连接），更大幅度地减少TIME_WAIT状态连接的数量呢？从Apache官网发现，其实可以将Proxy[P]修改成pass through to nexthandler[PT]。P模式是强制代理网络连接，而PT模式则只是做路径转换。这样修改之后就会走Mod_JK，不需要采用本地连接，代码如下：RewriteRule ^/e/（.</em>）$ http://localhost:80/getKey.htm?token=$1[L,QSA,PT]这样TIME_WAIT状态连接的数量基本降到0,Apache和JBoss通过长连接的方式进行通信，因而不会造成TCP的主动关闭。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>一般TIME_WAIT状态连接容易过多，虽然这个状态对资源的消耗并不大，但是这种连接过多会造成端口数到达上限，这往往是由处理方式的不合理引起的。修改重用TCP连接，可能会造成迷途报文影响后续的请求，造成报文被破坏，一般在内网服务器之间可以如此处理，如果在公网上如此处理，迷途报文重现的可能性会大很多，所以任何问题都需要寻根究底找到根本原因。</p>
</blockquote>
</blockquote>
<h3 id="◆-5-4-总结" tabindex="-1"><a class="header-anchor" href="#◆-5-4-总结" aria-hidden="true">#</a> ◆ 5.4 总结</h3>
<blockquote>
<blockquote>
<p>● 尽量将操作系统进行版本升级。高版本的Linux操作系统对TCP内核进行了多项优化，包括拥塞窗口变大、Early Transmit等算法优化。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 尽量通过监控工具发现问题，例如LastMile。远距离的传输容易丢包，可以通过LastMile来发现问题。在建立连接的时候丢包，为了避免定时器溢出再进行报文的传输，可以通过调整初始RTO来减少网络耗时。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 用户体验要注意 RTO。服务器端耗时占用整体耗时的一小部分，要把主要精力放在网络耗时的减少上。如果是小型网站，服务的用户群相对比较集中，就不要考虑 TCP 优化，因为带来的收益往往很小。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 尽量减小HTTP报文的大小。因为HTTP报文越小，传输所需要的RTT次数越少，耗时越短，通常Ajax异步化是减少HTTP同步报文大小的主要手段。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 尽量减少HTTP头的大小。HTTP的Gzip压缩只针对HTTP主体，而HTTP头是不能被压缩的，要尽早对Cookie大小进行控制，因为Cookie往往是造成HTTP头偏大的主要因素，Cookie大小在很多时候都接近4KB的上限。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 尽量将机房部署在离核心用户近的地方，通过减少网络距离来减少网络耗时。● 尽量选择大的运营商。小的运营商往往在BGP选路时会绕路，造成网络耗时增加。</p>
</blockquote>
</blockquote>
<h2 id="◆-第6章-dns优化" tabindex="-1"><a class="header-anchor" href="#◆-第6章-dns优化" aria-hidden="true">#</a> ◆ 第6章 DNS优化</h2>
<blockquote>
<blockquote>
<p>在人们的预想中，因为缓存的存在，TTL 内的 DNS 查询就在浏览器本地或者操作系统本地，不会向远程的域名服务器进行 DNS Lookup。但是在实际构建过程中，如果DNS架构不合理，会带来极差的用户体验。DNS对于用户体验的影响如下</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 对于页面请求域名，如果DNS Lookup时间过长会引起白屏时间过长。● 对于图片对应的域名，如果DNS Lookup时间过长，白图的时间也会过长。</p>
</blockquote>
</blockquote>
<h3 id="◆-6-1-dns基本原理" tabindex="-1"><a class="header-anchor" href="#◆-6-1-dns基本原理" aria-hidden="true">#</a> ◆ 6.1 DNS基本原理</h3>
<blockquote>
<blockquote>
<p>1.根域名服务器（简称根域）根域名服务器（Root Name Server）是互联网域名解析系统（DNS）中最高级别的域名服务器，负责返回顶级域名的权威域名服务器的地址。全球共有13个根域名服务器，共504个域名服务器（NameServer,NS），分散在世界各地，大部分采用Anycast技术，对外宣告同一个IP地址。在DNS查询过程中，由路由层面（BGP）寻找到具体承担解析任务的NS进行就近解析，以便提高解析效率。根域名服务器存放了顶级域的NS列表。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>2.顶级域名服务器（简称顶级域）通常顶级域名指的是.com（商业机构）、.net（网络提供商）、.edu（教育机构）、.cn（中国域名），顶级域名服务器是互联网域名解析系统中次高级别的域名服务器，负责返回权威域名服务器的地址。同样，全球共有13个.com（或者其他几个）顶级域名服务器。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>3.权威域名服务器（简称权威域）一般是指能够提供最终IP地址解析能力（最终IP地址是由这个NS来解析的）的一组服务器，或者能够控制最终IP地址解析结果的域名服务器（由权威CNAME给其他的NS来解析）。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>4.Local DNS用户上网需要通过ISP的网络接入互联网，ISP会分配给用户一个DNS服务器，该服务器被称为Local DNS，这个DNS代理域名解析请求给最终的权威DNS，它具有Cache的能力，如果Local DNS在TTL时间内有Cache，不会迭代向根域、顶级域和权威域发送DNS查询请求。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>5.DNS no glue在顶级域名授权中心注册一个新的域名时，需要提供（权威）NS的列表，通常是一组域名。如果只是提供NS的域名，而没有NS的IP地址，那么在DNS解析过程中，需要解析NS的域名，并最终得到NS的IP地址，这样UDP查询才能知道把域名解析请求发送给谁。这种只是提供域名、没有提供IP地址的方式称为noglue。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>6.DNS TTL简单地说，TTL（Time-To-Live）表示一条域名解析记录在DNS服务器上缓存的时间。当各地的DNS服务器接收到解析请求时，就会向域名指定的DNS服务器发出解析请求从而获得解析记录。在获得这个记录后，记录会在DNS服务器中保存一段时间，这段时间内如果再接到这个域名的解析请求，DNS 服务器将不再向域名指定的 DNS 服务器发出请求，而是直接返回刚才获得的记录，这个记录在DNS服务器上保留的时间就是TTL值。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>注：所谓CNAME，就是将一个域名的解析请求转化成另外一个域名的解析请求（也可以称为委托，通常用于将网站性能要求委托给 CDN 提供商进行解析），另外一个域名请求的结果就是开始要解析域名请求所要的内容。例如，虽然发起的是www.aaa.com的域名解析请求，但是最终被域名解析服务器转化成www.aaa.com.lxdns.com的解析请求，所以经过CNAME之后，就解析CNAME的那个域名了。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>ping ServerIP功能：从ping操作所在的客户端默认发送一个32字节的包给路由器来测量RTT。通常RTT越大距离越远，可以大体测量网络延迟，以便选择合适的地点建立DNS解析点。</p>
</blockquote>
</blockquote>
<h3 id="◆-6-2-实战案例-超远距离dns性能问题分析和优化" tabindex="-1"><a class="header-anchor" href="#◆-6-2-实战案例-超远距离dns性能问题分析和优化" aria-hidden="true">#</a> ◆ 6.2 实战案例：超远距离DNS性能问题分析和优化</h3>
<blockquote>
<blockquote>
<p>6.2.3 DNS解析性能解决方案</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>1.设置合理的TTL时间</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>TTL时间一旦过期，请求会返回到权威DNS上，超远距离提供服务的网站，用户分布在离机房位置超远距离的国家，这些国家实际上与权威DNS所在的地区机房距离很长，一旦请求发送到美国的权威DNS上，加上丢包因素会造成DNS Lookup时间非常长。众所周知，TTL设置短的目的是为了高可用性，无论是CDN还是淘宝这样的网站的动态请求，设置很短的TTL时间，在一个地方出问题时，可以通过简单的域名切换到另外一个地方，但是如果断定没有第二条路可走，TTL设置过短实际上对性能有很大的损耗。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对于像淘宝这样规模网站的主域名，TTL设置可以短一点，对于大型网站，异地容灾基本都是标配，一旦机房A出现问题，就将域名解析的A地址从地区A的虚拟IP切换到地区B的虚拟IP，这样能够起到异地容灾的作用，大大提高系统的稳定性。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对于静态资源相关的域名，跨境全球化网站的 CDN 解析路径比较长，这有客观原因，海外CDN目前可能是多个CDN厂商共存的状态，各CDN厂商在全球的部署节点数有所不同，服务优势有差异。首先要CNAME给CDN提供商A的解析服务器（其目的是根据国家来确定是否CNAME给该CDN厂商进行静态加速），再由AliCDN的解析服务器决定是否CNAME给Akamai的全局调度解析器。CNAME的TTL时间设置过短会导致一次完整的DNS的递归查询，需要多个RTT往返，这对于跨境全球化网站的性能而言是一场灾难。</p>
</blockquote>
</blockquote>
<h3 id="◆-6-3-总结" tabindex="-1"><a class="header-anchor" href="#◆-6-3-总结" aria-hidden="true">#</a> ◆ 6.3 总结</h3>
<blockquote>
<blockquote>
<p>DNS优化的关键在于，能够提供就近部署解析的架构，让用户可以就近部署解析。设置合理的缓存是优化的后手。</p>
</blockquote>
</blockquote>
<h3 id="◆-7-1-cdn优化概述" tabindex="-1"><a class="header-anchor" href="#◆-7-1-cdn优化概述" aria-hidden="true">#</a> ◆ 7.1 CDN优化概述</h3>
<blockquote>
<blockquote>
<p>CDN的中文全名是内容分发网络，其功能是将内容“发布”到离用户最近的服务器上，有效地避免网络拥塞（越远的距离越容易遇到拥塞）。大型网站一般分布较广，用户地域跨度大，而网站机房的位置离用户的距离有远有近，CDN提供就近访问的能力，消除了由于用户离机房的距离不一样带来的体验差异，是大型网站不可缺少的基础组件，CDN的优化对于提供高性能的用户体验起到了关键的作用。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>CDN对于一个大型网站而言，主要提供了6种能力。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>1.静态加速能力</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>通过本地化缓存加速能力给用户提供一个尽力而为的就近访问的高性能访问架构，将用户访问的内容缓存在边缘节点上，消除由用户地域差异而导致的用户体验不一致，提供不同地区用户的相对一致的高性能访问体验。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>2.卸载源站能力</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>CDN将资源缓存在它的服务器上，访问是在用户和CDN之间进行的，原来用户的直接请求都发送到网站服务器上，移交到 CDN 上后，源站的访问量和带宽占用都会大幅度减小。特别是对大型网站而言，图片等静态资源占了网站所有请求的90%以上。图片访问量对于大型网站来说是巨大的，服务器要提供具备相应吞吐能力的服务，其架构设计、运维规划、监控和预警要十分完善，否则很容易出现稳定性问题。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>CDN的命中率对于减小源站的压力十分关键</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>CDN的存在大大减小了源站的压力，提高了网站的稳定性。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>3.防攻击能力</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>一般比较成熟的 CDN 提供商至少有数百个 CDN 节点，甚至数千个，而把资源放在 CDN上，对网站的恶意攻击大部分都会将目标放到CDN节点上，CDN是一个天然的跨地区甚至跨洲的大型分布式系统。大量CDN节点的存在，可以有效地将攻击由中心化分散到CDN的边缘上，从而有效地阻止或者减小攻击造成的危害。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>4.动态加速能力</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>CDN提供静态加速能力的原理是通过将资源缓存在CDN边缘节点上，让用户访问资源的网络距离变短，从而实现性能的优化。CDN不仅适用于可缓存资源的静态加速，而且可以用于动态请求的加速，其原理是通过7层路由路径的优选，克服BGP选路的缺点，实现动态加速能力</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>5.用户访问序列优化能力</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>CDN 能够做的事情超乎我们的想象，CDN 如果能获取网页的 HTML，就可以实现访问序列的优化。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>1）页面内资源预取</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>获取网页的HTML后，CDN服务器会将HTML中的图片、CSS和JS标签的资源提前预取到 CDN 的边缘节点上，等浏览器下载序列到相对应的资源时，边缘节点上已经存在该资源，从而避免回源站处理并获取资源，减少性能损耗。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>2）页面间资源预取</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>页面间资源预取指的是在用户访问当前网页时，将要访问页面的资源提前预取到CDN 边缘节点上，等浏览器下载序列相到对的应资源时，避免回源或者从 CDN二级缓存节点上获取资源，以减少网络延迟。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>6.定制化模块开发能力CDN不仅提供各种标准功能，而且提供定制化的功能开发，这些功能模块中有不少已经标准化，例如边缘化的图片压缩、边缘化图片格式转换、自适应图片下载等功能。</p>
</blockquote>
</blockquote>
<h3 id="◆-7-2-cdn的相关术语" tabindex="-1"><a class="header-anchor" href="#◆-7-2-cdn的相关术语" aria-hidden="true">#</a> ◆ 7.2 CDN的相关术语</h3>
<blockquote>
<blockquote>
<p>1.边缘服务器（Edge Sever）对于边缘服务器，CDN提供了就近访问的能力，边缘服务器节点就是实际提供给用户就近连接、访问的服务器。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>2.CDN命中率CDN 一般提供的是静态加速能力，静态加速能力通常通过缓存架构来实现，CDN命中指的是CDN服务器有该资源缓存存在，请求到达CDN节点时，CDN服务器可以在本地缓存获取资源直接返回给客户端，如果没有命中，则需要通过CDN节点到源站获取资源。CDN命中的概率即CDN命中率。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>3.回源当CDN没有命中缓存时，需要到源站去获取资源，这个过程称为回源，回源需要从CDN节点层层代理访问，最终到源站获取资源。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>4.中间层服务器（Midgress Server）边缘节点比较分散，因此存在缓存穿透的问题。为了避免回源引起的性能大幅下降，在CDN的中间层服务器中将多个CDN节点的访问进行收敛，从而大幅提高命中率。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>5.L2 CacheL2 Cache也是CDN的中间层服务器，通常也称Midgress Server为二级缓存。L2Cache通常空间更大，是多个CDN边缘节点的收敛层。Edge Server被称为L1Cache,L2 Cache层是CDN架构避免回源的常用组成部分之一。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>6.卸载率（Offload Rate）CDN命中率通常也称为卸载率，表示卸载源站访问的程度。</p>
</blockquote>
</blockquote>
<h3 id="◆-7-3-从应用看cdn的基本原理" tabindex="-1"><a class="header-anchor" href="#◆-7-3-从应用看cdn的基本原理" aria-hidden="true">#</a> ◆ 7.3 从应用看CDN的基本原理</h3>
<blockquote>
<blockquote>
<p>需要知道 CDN哪些能做、哪些不能做，用了 CDN 后会带来什么问题等。了解 CDN的原理，能够更好地帮助我们做设计，更好地促进业务的发展。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>7.3.1 CDN基本架构</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>用户访问图片的一般路径如下：（1）在DNS Lookup时，由全局负载均衡器调度将离用户近的节点发给用户的浏览器。（2）浏览器与CDN的边缘节点建立TCP连接，并将请求发送给边缘服务器。（3）如果边缘服务器没有命中缓存，会将请求代理给CDN的L1 Cache节点。（4）如果CDN的L1 Cache服务器也没有命中，会将请求继续发送给源站。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>CDN L1 Cache节点通常将相邻的几个CDN节点的请求进行收敛，当同一资源在一个节点预热过，又从另外一个节点访问时，可以在L1 Cache层命中缓存，避免回到源站去获取资源，从而避免性能大幅下降。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>（5）源站接收请求，并通过源服务器的处理，给CDN L1 Cache服务器响应，直到最终将响应逐层返回用户端浏览器。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>简单地说，用户浏览器获取图片或者静态资源的一般过程是，先通过 CDN 的全局负载均衡器的调度获取离用户“最近的”CDN节点的VIP（虚拟IP），并建立TCP连接，发送请求给CDN的边缘节点，如果在CDN节点上没有命中缓存，请求会逐步传递给最终的源服务器，并最终由源服务器回应给用户浏览器，并在各个CDN节点上建立各自的本地缓存。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>7.3.2 CDN全局调度</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>CDN全局调度的基本结构和全局负载均衡器的实现原理比较简单，它能够接收在域名查询请求时，根据用户IP地址的（Local DNS）来源，返回给Local DNS和用户IP地址区域匹配的CDN边缘节点的VIP。通常一个地区有两个CDN集群节点，两个CDN节点同时给不同的用户提供服务（容灾和负载均衡）。为了防止DNS Cache过长，导致一个节点过载，通常CDN节点的TTL控制在一分钟以内，当一个节点发生问题时，可以在较短的时间内切换到另外一个节点。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>常见的CDN全局调度器有F5的GTM设备和其他智能DNS软件。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>全局调度器从本质上说是一个智能的DNS解析工具，无非是做得更加智能、功能更加强大。全局调度器主要有3个元素：Region（区域）、POOL（CDN 节点池）、Member（CDN 节点VIP）。当域名查询请求发送给全局调度器时，全局调度器能够匹配用户的IP地址（一般是Local DNS的IP地址）和Region的对应关系，找到对应的POOL，根据Score（分数）的大小，找到合适的POOL，再通过POOL找到对应的Member,Member里面的VIP根据配置的权重，最终向用户返回CDN节点VIP。过程比较容易理解，但是如何做到更加合适、更加稳定，妥善处理各种异常情况，例如负载单点过重的情况、不可用的情况等，做到100%高可用，并不简单。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>7.3.3 CDN基本调度方式</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>CDN厂商需要在成本和体验之间做出权衡，例如CDN提供商在美国有数千个集群，而在乌克兰、中国台湾才20个节点，为了节省成本和容量，CDN提供商会将流量调度给美国的CDN节点，让用户浏览器进行访问，这实际上和 CDN 厂商号称的就近访问有很大的不同，但是商业上就是如此，必须做出权衡，在成本投入上做出选择。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>1.基于Local DNS的静态调度</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>静态调度是CDN调度的基本方法，全局调度器根据Local DNS的IP地址（或者终端机器的 IP 地址，EDNS 协议），在其配置里面找到对应 IP 地址所在的 region，并最终找到合适的CDN节点</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>[插图]</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>2.基于RTT的调度</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>基于RTT（Round Trip Time）的调度，是全局调度器根据Local DNS的IP地址，将预调度、多个候选的CDN节点和Local DNS之间的RTT进行比较，全局调度器会将RTT短的CDN节点调度给用户</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>[插图]</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>3.基于成本和带宽的调度</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>基于成本和带宽的调度是指全局调度器根据 CDN 节点出口带宽的大小来决定使用哪个CDN节点进行访问。基于成本的调度是指CDN厂商从全局出发，在某些业务少的地区，调度器将访问调度给 CDN 全局分布点较多的地方。因为往往不会考虑在业务少的地区投入过多的硬件成本（自建或者租借机柜、服务器、带宽）。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>4.基于服务等级的调度</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>基于服务等级的调度是指，CDN服务提供商为了保障服务等级更好的网站客户的访问，通常会将网络延迟更小（用户访问延迟小）、运行更加稳定的节点给这批用户。例如eBay和Amazon同时购买了CDN的服务，但是Amazon购买了服务等级更高的用户许可协议，这样CDN会提供响应更好、网络延迟更小的节点给 Amazon，甚至为了绝对保障服务等级高的客户的利益，CDN提供商会强制将一定百分比的流量调度给机器充足的远距离节点进行访问。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>上面是最常见的4种调度方法，CDN的调度通常是上述调度方式的结合，在成本和容量足够的情况下，一般是就近选择RTT调度或者静态调度。但是RTT调度不容易实现和做到精确调度，特别是在首次访问的时候，没有大量的样本数据的积累，很难判断哪个 CDN 节点离用户最近、RTT更短，同时为了保障更高级别客户的利益，对于低级别的客户，会强制牺牲一部分用户的用户体验。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>如果想让某个资源或者某个域进行CDN加速，一般要经过下面几个流程，如图7-4所示。[插图]</p>
</blockquote>
</blockquote>
<h3 id="◆-7-4-cdn优化常见策略" tabindex="-1"><a class="header-anchor" href="#◆-7-4-cdn优化常见策略" aria-hidden="true">#</a> ◆ 7.4 CDN优化常见策略</h3>
<blockquote>
<blockquote>
<p>CDN优化常见策略</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>静态化缓存优化</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>CDN提供的最基本功能是静态缓存功能，所以Cache服务器是CDN软件系统的标配。从软件架构上来说，一般分为3层</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>第一层：高效的4层负载均衡。众所周知，LVS和F5提供了高效的4层负载功能，4层的效率高主要在于解包的速度快，在OSI 7层网络模型中，TCP在第4层，当HTTP请求包发送到服务器时，基于4层的负载均衡能够快速将包转发，并且起到均衡负载的作用。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>第二层：基于命中率而设计的7层负载均衡。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>该设计可以针对HTTP相关的属性进行负载均衡，如Cookie、URL、Method，甚至Parameter，而CDN一般针对的是HTTP访问网站的加速，所以7层负载均衡能够根据HTTP报文里面的内容进行负载分担，分担LVS通过NAT转发过来的数据包。如果直接到缓存服务器，由于LVS一般只提供有限的负载均衡方法，例如Roundrobin，或者通过5元组（客户端IP地址、port、目标IP地址、port、LVS地址）哈希到真实IP地址上，不能保证高命中率，同一个HTTP URL （如图片），不能保证一台缓存服务器的命中率特别高，特别是对一些近似长尾的访问。为了解决这个问题，中间架设了7层转发服务，它能够根据 URL 运用哈希算法，从而保证同一个URL基本上都在同一个服务器上访问，进而保证了最终的命中率。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>第三层：本地化Cache服务器。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>能够提供本地化Cache能力的服务器非常多，如Varnish、Squid、Traffic Server等。由于Varnish性能好、稳定性佳，得到了广泛的应用，特别是相对比较老的Squid，性能要高出几倍，稳定性更佳。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>通常CDN静态加速的对象是图片、JS、CSS等静态资源，静态资源具有可以被缓存的特性，在一定时间内不会更改，对实时性要求也比较低，同时也比较容易通过修改URL来获取页面的最新内容。由于动态页面本身（如JSP、ASP、PHP）的特别要求（如业务打点，统计PV、UV）、实时性（如商品价格）等，一般都需要从源站获取内容，但是在某些场景下，页面大部分的HTML内容（JSP、ASP、PHP服务端渲染和执行后的结果）可以缓存（或者局部可以缓存），而实时的部分可以用CSI（Client Side Include）或者ESI（Edge Side Include）来实现。动态HTML因为涉及首屏、白屏等用户体验相关的内容，所以动态内容如果能放在离用户比较近的地方，可以大幅提升用户体验。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>所谓的CSI其实就是通过浏览器端发起Ajax请求，从源站获取实时性和一致性要求高的数据，例如价格信息，网页的其他部分全部可以缓存到 CDN 的边缘服务器上。典型的电子商务的场景是商品详情页面，页面的大部分HTML内容都可以缓存，价格的信息要求强一致性，因此可以将获取价格的信息重新改造成Ajax，只要设置max-age就可以确定CDN缓存时间（如果只想在CDN上缓存，不想在浏览器本地缓存HTML内容，可以使用s-maxage响应头）。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>ESI 通过使用简单的标记语言来区分可以缓存和不可以缓存的片段并进行描述，不可以缓存的部分通过边缘服务器实时地从源站获取，可以缓存的部分从边缘服务器本地缓存中获取，并在返回给用户端浏览器之前，组装成完整的HTML给浏览器。通过这种控制，可以有效地减少从服务器抓取整个页面的次数，只从源站中提取少量不能缓存的片段，因此可以有效地降低源站的负载，同时缩短用户访问的响应时间。ESI是一个简单的标签</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>第一次CDN缓存没有命中时，从源站获取HTML内容（源站不渲染ESI标签的内容），获取如上的HTML内容后，将内容缓存到本地，再将ESI标签SRC属性中的URL解析出来，并发起对这个URL的访问，源站响应的内容和本地的内容进行拼接和组装后返回给浏览器。第二次访问时，CDN边缘服务器只要将ESI标签的内容解析出来，并发起请求将动态内容和本地缓存中的内容组装后返回给浏览器。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>ESI实现的HTML内容的缓存由CDN的边缘服务器负责动态内容和静态缓存中的内容的组装</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>当页面要依赖SEO引流时，必须使用ESI技术，因为搜索引擎的爬虫无法获取异步Ajax的内容</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>ESI实际上有一部分内容需要回源，缓存的内容在CDN的边缘服务器，响应给浏览器的内容是在CDN边缘节点或者二级节点进行拼接的。在容错方面，ESI更强，如果回源部分的请求出现404错误或者500错误，CDN上可以处理成相应的错误码和内容给浏览器。例如，在发生404错误时，CDN需要忽略缓存内容，直接响应给浏览器源站响应的404错误内容。CSI是CDN先将缓存的内容响应给浏览器，然后浏览器发起实时的部分请求，再通过修改DOM节点内容进行更新。但是一旦Ajax请求发生问题，页面的内容不会发生变化，如果是关键内容，如折扣信息或者价格信息，将无法弥补。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>如果实时性要求很高的内容通过CSI来获取，网页会出现闪烁的效果，特别是源站返回慢的时候，这种问题将更加明显。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>综合来说，ESI 更适合于对重要性内容需要实时化和强一致性的内容静态化的解决方案，CSI 的实时处理部分通常适合对网页重要内容不做更改的情况，如业务打点请求。使用 ESI 还是CSI需要考虑对用户体验的影响、实时部分的重要性，以及容错处理能力。但是ESI的处理过程相对复杂，改造成本相对较高，源站需要自己实现HTML的拼接逻辑，并做各种容错处理。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>综上所述，两者比较如下：● ESI实现成本较高，容错能力强，页面加载性能较差。● CSI实现成本较低，容错能力弱，页面加载性能较好。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>静态加速一般指的是，通过CDN 的边缘服务器（Edge Server）缓存资源，让用户访问时可以直接到离用户最近的服务器上获取缓存的资源。所以在静态加速过程中，资源具有可以被缓存的特性，也就是说，在缓存的时间内，只需要回源站获取资源一次，然后就可以到边缘服务器上获取资源，从而起到加速的作用。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>使用动态加速进行优化时，用户的请求仍然会发送到源站，它将通过某种方式减小源站端到用户端的网络延迟。在网站访问过程中，大家都知道从请求发送给服务器、服务器响应，到浏览器接收，大部分时间都消耗在网络上，服务器端的耗时一般比较短，在机房内部的处理，无论是CPU的处理时间还是机房内部的网络转发，丢包率都比较低，耗时都比较短。这和报文传输机制相关，无论是发送报文还是接收报文，都需要经过网络层、TCP传输层，网络转发需要经过BGP路由选路的过程，实际的地理距离和网络距离存在很大差距，这会造成网络转发的延迟时间变得比想象中要长很多。TCP传输带来的网络延迟和TCP处理机制相关，特别是TCP的拥塞处理机制，以及TCP为了保持可靠性而设置的TCP重传机制。TCP一般都是在TCP重传定时器溢出时重新发送丢失的报文，而TCP重传定时器的超时时间如果很长，同样会造成很大的网络延迟。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>无论是静态加速还是动态加速，大部分都通过减小网络延迟来进行加速。静态加速是通过把资源缓存在离用户最近的地方来实现优化的目的，动态加速是通过优化传输、优化网络转发等方式来实现优化的目的</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>动态加速的请求具有每次请求回源的特性，每次请求都会到达源站，不会改变用户到机房的地理距离，但是会减少网络上的距离</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>目标都是通过某些方式减小网络延迟，常用方法</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>专线的优势是具有更小的网络延迟和更低的丢包率，这两个特性能够减小TCP因为重传而导致的超大的网络延迟，网络延迟也更加固定。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>实现动态加速的非常重要的方式是路由优化，这是网络路由的特性决定的，网络路由并不是按照最近的原则选路，这给路由线路的优化留下了空间。这个空间不是故意预留的，而是因为全世界网络的互联涉及众多的运营商，网络的路径由众多的网络运营商主导完成，各个运营商之间的网络流量的相互流动、运营商之间的流量流动的不对等，会造成一些小的运营商不得不在考虑成本的情况下进行路由选择。例如，中国网通从中国电信的网络走的流量，比中国电信从中国网通走的流量大很多，中国电信可能会向中国网通收费，中国网通可能为了减少费用而限制流量或者绕路路由。限制流量会造成丢包，绕路会造成网络延迟变大，这就是流量不对等引起的运营商成本变高，这个看起来人为的因素会造成BGP选路的延迟变得很大。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>流量互通的不对等是BGP路由选路的网络延迟大的主要原因，特别是跨境的网络路由转发更容易出现这种问题。通过测试发现，巴西离加拿大的距离大概为8000km，按照光速的传输速度，传输一个TCP包需要27ms左右，实际测试发现网络延迟达到200ms。这就是BGP选路引起的网络延迟。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>路由层面的东西完全是由运营商控制的，路由优化不是对众多的运营商网络进行的，而是利用CDN边缘节点组成的虚拟网络节点来进行优化，所以一般来说，只有CDN这种规模的节点部署能力才能达到路由优化的目的。路由优化是利用CDN 节点7层代理转发改变BGP选路的路径来进行优先选择的。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>用户序列优化是CDN根据用户请求的HTML内容和浏览器下载的序列进行提前解析，将浏览器需要的资源提前预取到CDN节点上，等到浏览器下载该资源时，该资源已经提前在CDN节点上了</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>域名合并是常见的CDN优化手段之一，对于Web网页来说，为了提高浏览器的并行下载能力，往往会使用多个域名。CDN在缓存配置上有很多策略，能够根据业务的变化进行灵活配置，而域名合并是其中一种优化方式。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>例如下面的URL，在未调整之前，Cache key是整个URL，缓存不能重用。域名合并之后，Cache key是除了域名之外的URL，两个图片缓存可以重用，从而命中率得到提升。● HTTP://z00.aaaa.com/support.jpg● HTTP://z01.aaaa.com/support.jpg</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>7.4.6 多级缓存架构优化</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>网站都有大量的302跳转的逻辑存在，例如很多网站对于PC访问、Mobile设备访问使用统一的域名。当Mobile设备访问时，采用301、302跳转到Mobile 站点，跳转从浏览器端发起，因此至少会浪费一次RTT，对性能会有较大的影响。用CDN代理域名，可以在CDN的边缘服务器上进行跳转，从而让用户网络访问链路变短，提升性能。</p>
</blockquote>
</blockquote>
<h3 id="◆-7-5-cdn优化实战" tabindex="-1"><a class="header-anchor" href="#◆-7-5-cdn优化实战" aria-hidden="true">#</a> ◆ 7.5 CDN优化实战</h3>
<blockquote>
<blockquote>
<p>分析304请求问题为什么会有如此长的耗时，如果是回源引起的，304请求问题会引起回源吗？</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>先回顾一下304请求问题的基本原理：浏览器在访问同一资源时，会向服务器发送请求，让服务器判断是否用浏览器本地缓存的资源，服务器如果发现客户端本地缓存的资源是最新的，那么会响应304请求给浏览器，告诉浏览器可以使用本地缓存的资源，这样可以减小网络的消耗</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>浏览器第一次访问某个资源时，会在响应头里面加入Last-Modified字段，标识此资源在服务器上的最后更新时间。当浏览器第二次访问同样的资源时，在请求头里面加入If-Modified-Since字段，这个字段的值是第一次请求服务器给浏览器的Last-Modified，服务器以这个时间和文件的最后更新时间做比较，如果浏览器记录的时间比服务器记录文件的更新时间早，说明文件内容已经发生了修改，服务器会给浏览器最近的资源，如果文件内容没有更新，则给浏览器304的响应，不带文件内容。所以从304类型的请求来看，由于没有文件内容需要下载，因此耗时长的原因可能跟CDN的特殊处理过程有关系。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>第一次请求某资源的时候，CDN全局调度器会调度一个离用户近的CDN边缘节点，然后经过CDN的L2 Cache节点、网站源站获取该资源，并缓存在各自的本地，包括浏览器本地。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在第二次重复请求该资源的时候，CDN 为了负载均衡的设计和需要，通过设置足够短的TTL（一般在30s以内），将负载均衡分配给另外一个CDN边缘节点，所以只要第二次访问和第一次访问调度给不同的CDN节点就会出现下面两种情况。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 第一种，如果在第二个 CDN 节点上没有请求的资源，那么请求会回源到 CDN 的 L2 Cache去获取，如果L2 Cache上没有资源，那么请求继续去源站获取，直到边缘节点上存在此资源文件，再把获取图片的Last-Modified时间和请求头带过来的时间进行对比，如果发现客户端的文件是最新的，服务器给客户端304请求类型的响应。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 第二种，如果在第二次调度的CDN节点上有请求的资源，但是浏览器本地的资源比CDN节点上的资源还新，此时需要从CDN L2 Cache节点获取资源，如果CDN L2 Cache上也没有，那么回源站获取。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>回源问题的解决方案——多级缓存架构升级</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在2012年的时候，AliExpress使用的AkamaiCDN只有一级缓存，缓存命中率只有30%左右，随后由于访问量和大量长尾冷访问的商品数量增加，导致 L1层的命中率大大减小，使得源站的压力大大增加、性能大幅下降，业务数据表现变差。后来Akamai CDN使用了多级缓存架构，2012年调整的多级缓存架构使得命中率从40%提高到70%。而当时的架构是典型的二级缓存架构，对于全球化的网站而言，把多个地区的访问收敛于一个大的缓存层，对于缓存的命中率提升相当关键，它可以把一级缓存未命中的请求穿透到L2层，L2层重用了L1层穿透的请求，显然命中率会提高很多。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>命中率优化实战之二——图片、JS、CSS命中率从70%提高到90%</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>二级缓存的命中率对于低热度近似长尾的数据访问，仍然有穿透到源站的可能。所以建立了L3层，将近似长尾的访问收敛于L3层，这样可以大幅卸载源站的压力。为了进一步提升性能，网站使用Akamai的license升级，针对命中率提升，Akamai加了三级缓存L3层，可以把L2层漏掉的访问收敛于L3层，在L3层可以命中，这样图片的命中率从70%提高到了90%</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>CDN缓存的时候要区分动态请求和静态请求，对于动态请求 CDN 默认会回源站请求，通常这种区分在CDN 内部有默认的配置，根据请求URL 的扩展名来决定是否将请求回源，详细过程如下：找到请求URL 中的“?”,“?”前面就是文件的扩展名，如果不带扩展名，CDN系统默认这个URL属于动态请求，那么直接穿透到源站。如上面的 URL，当发现有两个“?”前面没有任何的扩展名时，CDN 默认它属于动态请求，不缓存，直接将请求回源。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>用CDN URL匹配规则，将“??”任意匹配“?”改成一个“?”进行精确匹配，这个URL就属于CSS文件（如上例），就可以缓存了。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>这个规则修改之后，命中率大幅提升</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对TCP传输来说，有如下两个非常重要的现象。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>1）越大的文件越容易丢包</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>Ajax请求多的应用TCP重传率很低，HTML级别的访问，TCP重传率要高很多。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>2）越长的距离越容易丢包</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>距离越长的访问，网络转发时经过的路由器也越多，网络拥塞的概率也越大。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对于跨境的访问，最大的问题还是路由的问题，运营商之间的对等路由交换、流量上的不均等，会导致默认的 BGP 选路并不是最佳路径。BGP 选路从来不会承诺选择网络延迟最小的线路，BGP选路最基本的承诺是路由可达，路由线路的优劣直接影响网络延迟的大小，而动态加速方案能够解决路由选路的问题。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>与源站建立TCP连接和发送请求的过程是封包的过程，将IP地址放在HTTP header里面，再封装成TCP报文，然后将IP报文发送给源站</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>但是对于HTTPS的情况就完全不一样了。首先，CDN代理服务器无法解开用户浏览器发送过来的HTTP报文，全部是加密过的，CDN代理服务器无法将IP地址放到报文里面。所有的HTTPS加速都会遇到这个问题。通常的解决方案是将源站的HTTPS证书交给CDN进行托管，但是对于支付宝这种安全性要求极高的网站，不会将服务器端证书交给CDN代理服务器，这给HTTPS的加速带来了难度</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>CDN静态化可以达到两个目标：其一是提升用户体验，提升首屏加载性能和减少白屏时间；其二是卸载源站的压力，提高吞吐量。</p>
</blockquote>
</blockquote>
<h3 id="◆-7-6-总结" tabindex="-1"><a class="header-anchor" href="#◆-7-6-总结" aria-hidden="true">#</a> ◆ 7.6 总结</h3>
<blockquote>
<blockquote>
<p>CDN最重要的使用原则总结如下</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>1）尽早了解CDN的基本工作原理</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>2）尽早了解license的细节，比较不同服务等级license的差别</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>CDN提供商提供的license版本很多，需要和售前人员清晰地了解license提供的功能和SLA的细节，这样可以综合ROI多方面来确定使用哪个版本，也可以对某些高级功能进行测试和线上效果实测。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>3）做好上线前的测试验证工作</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>测试验证工作，需要有比较健全的功能验证流程。在沟通过程中，经常遇到各种口头“承诺”，双方在口径和理解上存在一定的误差。例如在WebP性能优化的项目中，首先笔者和售前人员沟通了方案的可行性，准备将WebP格式的图片上线替换JPG格式的图片，售前人员说这是可行的，即CDN提供商有能力缓存WebP格式的图片。上线测试时，我们发现性能并没有好转，一直猜测是由于上线新格式图片的范围太小，导致热度不够，热点不突出，最终导致命中率降低而引起的。后来去源站查看，发现WebP格式的图片大量回源，而且几乎每次访问都产生了回源，再跟售前人员进行沟通，发现是配置没有启用。其实发生过多次类似的问题，如前面提到的命中率超低的问题也是配置不正确引起的，这些问题都需要做好沟通和测试工作。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>4）尽早做好监控规划</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>细化监控对于发现问题非常关键，监控还需要做一定的规划，针对主要的域名最好做细分。针对不同类型的命中率需要分开监控，根据命中率从低到高的排行数据，能够对回源站的请求进行归类和分析。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>5）避免全量发布新图片格式</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在使用 CDN 的过程中命中率越高对网站的卸载能力就越好，一般回源意味着性能会急剧下降，回源数量突增，非常可能造成系统故障。而全量发布新图片格式，会造成大量的回源，我们的网站已经因这个问题发生过多次故障。例如在搜索列表页面，商品图片多达数十个，搜索访问量非常大，网站流量的很大比例来源于搜索，因此一旦有新图片格式，相当于在 CDN节点上的缓存全部被击穿，回到源站，造成大规模的性能下降。对于这种情况，一般在发布时可以通过将发布周期变长来解决。例如某个应用有100台机器，根据源站能够支撑的容量，逐步切换机器来增加权重，同时观测监控系统的变化情况，关注关键指标是否发生变化。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>6）尽量对图片只新增不修改图片一般很少会涉及修改，在设计时，尽量和图片名称解耦，这样可以做到图片只会新增而不用修改。图片新增设计对于提高图片的命中率将会起到非常重要的作用，涉及修改需求时，只需要修改图片的名称，也就是通过新增图片的方式来让图片过期，可以将图片的过期时间设置成3年，甚至10年（不能设置过长的过期时间，因为CDN的存储空间有限，会导致图片被淘汰、性能下降，特别是针对热度相近的场景）。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>7）尽早设计好源站架构</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>8）尽早了解CDN的配置规则CDN里面有些默认的配置规则，如默认用户真实IP地址的字段使用的是TRUE_CLIENT_IP，默认没有配置的图片格式使用的是WebP，这些配置是否开启需要CDN提供商进行准确的确认。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>11）CDN高命中率是CDN应用的基本要求</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>CDN不仅能提高性能，也能提高稳定性，由于图片、JS、CSS等静态资源访问量极大，一个页面请求可能达到数百个静态请求，源站的压力极大。在网站从小到大的发展过程中，由于机房建设的基础设施很难到位，把如此大的压力放到源站，源站的稳定性会受到极大的影响，所以高命中率是CDN应用的基本要求。从应用实践来看，CDN的命中率对于用户类型的访问应该接近100%，如果没有达到100%，那么说明优化空间非常大。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>12）应用CDN需要考虑ROICDN的优化方案非常多，不是所有的CDN优化方案都适合在项目中使用，要把合适的方案应用在合适的地方。通过ROI分析，可以比较清楚地知道，哪些方案是有用的和合适的。CDN的应用有时候会带来架构的复杂性，这些复杂性的引入不仅浪费大量的人力和物力，还会造成用户体验的下降。CDN的应用一定要达到理想的目标，否则就是过度应用。</p>
</blockquote>
</blockquote>
<h3 id="◆-8-1-监控设计" tabindex="-1"><a class="header-anchor" href="#◆-8-1-监控设计" aria-hidden="true">#</a> ◆ 8.1 监控设计</h3>
<blockquote>
<blockquote>
<p>其次看关键链路的调用次数是否变少了，调用次数变少的可能原因如下。● 可能原因之一：外部流量减少。○ 正常的业务影响：活动或者节假日影响。○ 网络链路接入异常：如外部接入核心交换，通常多个运营商接入，某些运营商出现网络问题，这个问题一旦出现会导致整体流量下滑。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 可能原因之二：链路耗时变长，导致在一定时间内处理的请求数变少了。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>○ 交换机流量不均，某些机柜的流量被打满，TCP重试，导致耗时变长。○ 服务器流量不均，导致流量打满，TCP重试，导致耗时变长。○ 应用集群个体异常，某些服务器存在问题。○ 集群系统指标个体异常，集群中个别服务器存在问题，可能配置不同（比如CPU核数不同）。○ 依赖方个体异常，某些依赖存在问题，导致访问异常。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>从上面的问题排查思路看，监控应该具备下列能力：● 关键指标的监控发现能力。● 对集群个体、整体、依赖方全方位的监控能力。● 粗细结合的监控能力。粗粒度的监控主要为了快速地发现问题，细粒度的监控能够快速地定位问题。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>8.1.3 监控的设计步骤</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>监控的根本目的是发现并定位问题，所以监控的体系化思路可以从发现问题到定位问题要做的工作出发考虑。监控的设计一般包含以下几个步骤。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>1.关键监控项梳理</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>关键监控项梳理应该符合以下原则，这些原则的依据是排查问题的思路，当问题发生时，人们总会依据从发现问题到定位问题的思路进行梳理。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>从重要到非重要排序</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>从汇总到细分排序</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>监控角度分为两类，一类和发现问题相关，另一类和定位问题相关，业务出现问题，要么和业务本身相关，要么和系统相关。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>[插图]</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>2.关键监控项的报警抽离分级● 临界点报警。● 业务预警做重点关注报警，可以通过收集短信的方式进行预警。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>3.监控布点开发和设计</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 捕获关键异常，对关键异常进行日志记录，并在记录时进行分级。● 对直接依赖的运行耗时进行日志记录。● 对反映业务指标的运行情况进行日志记录。● 正确处理错误码。很多时候，我们会将错误的HTTP返回码处理成正确的响应，如404错误，如果在Response返回时没有设置，会被处理成200的状态码。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>4.监控展现设计监控展现通常容易被人忽略，一个好的监控展现能够缩短问题定位的时间。监控展现要体现一目了然和集中化的原则，切忌分散，否则会导致排查根本问题时，需要从不同的入口逐个跟进。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>监控通常只需要3个大盘进行展现。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>第一，业务大盘，定位于以终为始地快速发现问题。业务大盘展现的目的是将业务关键指标的运转情况进行展现，业务大盘通常需要放在首要的展现位置，当业务发生问题时，能够快速地获知是否对业务有影响和影响有多大。由于业务通常有周期性的特点，为了获知问题发生时的影响，通常需要做周同比、月同比。为了提高业务大盘的可读性，通常遵循以下几个原则。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 趋势化监控比列表型展现更一目了然。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 确定合适的监控粒度，长短间距相结合。短粒度是为了确保当业务发生极大的拐点时，能够感知到变化，形成报警，通告相关方。● 确定业务周期性规律，做环比或者同比。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>第二，应用大盘，定位于从汇总的角度来看是否是应用本身的问题。顺序是从粗到细，从汇总监控到细分监控，从集群总量监控到单机维度的细分监控。第三，系统大盘，定位于当系统发生问题时使用。同样，系统大盘应该遵循从汇总到细分监控的原则。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>5.调试和调整报警规则</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>通过一段时间的实践可将阈值进行逐步调整。阈值的设置要考虑如下因素。● 活动引流。● 节假日。● 业务调整。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 业务自然高峰和低峰期，在业务低峰期，同比和环比阈值变化特别大，所以在业务低峰期，预警可以不设置（当然和具体的业务相关）。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>8.1.4 监控常见法则总结● 对监控进行体系化设计。● 梳理和部署关键指标时，能够有的放矢，对每项指标监控的目的要非常清楚，在发现问题或定位问题时能够通过监控这个指标达到目的。● 对关键指标的预警进行演习。● 对监控的部署结果进行审核，并且能够得到组织保障。● 以终为始地发现问题，将能够直接影响业务的监控项进行优先部署，通常对关键的业务指标进行梳理和部署。● 采用合理的时间间隔粒度，长短时间间距相结合，对每个间隔的业务影响能够一目了然，通常的监控时间间隔粒度包括小时监控、天监控和5分钟监控。● 对用于定位问题的应用和系统的关键指标进行监控。● 完善监控埋点，对关键业务和流程进行日志记录，特别是异常。● 调试报警规则，根据业务周期性规律设置报警规则，并且对报警阈值不断地进行调整。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 对报警进行分级。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>○ 按照业务高低峰时间段进行分级，在低峰、低业务量时调整对比时间。○ 报警方式分级：对直接影响业务的监控项进行短信报警，对系统的问题进行IM报警，不仅可以节省成本，而且可以让工作更加高效。</p>
</blockquote>
</blockquote>
<h3 id="◆-9-1-容量评估概述" tabindex="-1"><a class="header-anchor" href="#◆-9-1-容量评估概述" aria-hidden="true">#</a> ◆ 9.1 容量评估概述</h3>
<blockquote>
<blockquote>
<p>容量评估是指通过一定的测量、推理方法来确定网站的容量现状，为运维预算、运维设备采购和解决系统瓶颈问题进行预警，并最终作为系统的伸缩性和水平扩展性的依据。简单来说，容量评估解决的是机器扩容、什么时候扩容的问题。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>一般的容量评估需要经过两个过程：</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 单机峰值QPS的评估（单机峰值吞吐量）。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 评估每个集群需要的容量。包括每个应用、中间件需要提供的峰值QPS（吞吐能力），通过订单、PV和UV等关键数据推导出各个应用、中间件、数据库和存储所需要的容量。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>一般而言，容量评估的过程有以下特点。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>1.峰值保障容量评估是为了保障峰值时用户访问的稳定性和可用性，峰值保障策略并不是以异常点尖峰时刻的值作为评估的结果，峰值通常指的是维持时间较长并且无抖动时的高点。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>2.去噪保障</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>大促容量保障是为了保证合理的用户访问的可用性和稳定性，并不是为了保障攻击类的容量需求，所以要有去躁保障。攻击类的需求往往在安全架构层面进行保障，所以大促期间可能发生SYN Flood攻击、XSS攻击、CC攻击等，这需要在大型网站准入控制上做很多工作，例如4层防攻击、7层防攻击等准备工作和演习。</p>
</blockquote>
</blockquote>
<h3 id="◆-9-3-单机峰值qps的测算" tabindex="-1"><a class="header-anchor" href="#◆-9-3-单机峰值qps的测算" aria-hidden="true">#</a> ◆ 9.3 单机峰值QPS的测算</h3>
<blockquote>
<blockquote>
<p>单机峰值QPS的测算</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>单机峰值QPS是计算机器数的一个重要因子，其大小体现了单机吞吐能力</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>总体单机吞吐能力的测算过程是通过逐步压力测试来完成的，当出现资源瓶颈时，压力测试停止，峰值QPS就是出现资源瓶颈时的QPS。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>单机测算方法如下。1.线下压力测试</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>压力测试到对应的资源瓶颈，通常使用Jmeter工具对应用访问的URL逐步加压进行测试，当超过阈值时可以停止，可以根据经验进行测试，以便提高测试效率。在线下压力测试时，需要注意JIT是否启用，-XX:CompileThreshold的默认值是10000次，也就是超过10000次JIT才会启动，直接将URL编译成机器码，否则由于JVM是解释执行的，所以在压力测试时需要经过一定的时间进行预热，以便获取比较精确的QPS测算值。在线下压力测试时，可以选择对线上的访问日志（access_log）进行压力测试，这样可以保证正确的配比，因为不同的 URL 对资源的消耗不同，所以线下模拟的环境要和线上环境保持一致（一致的比例，一致的URL）。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>2.线下脚本压力测试</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>线下脚本压力测试一般适合预发布机器，预发布机器一般没有用户的真实流量，用线下的Jmeter压力测试脚本进行压力测试同样要注意JIT是否启用的问题，以确保最真实的测试效果。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>3.线上引流测试线上引流测试是主要的测试方式，它通过修改负载均衡器的权重因子，逐步将流量引入某台指定的线上机器。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>两种常用的引流压力测试方法</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>1.4层负载均衡器权重比引流</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>线上引流测试单机吞吐能力，首先将负载均衡器的负载均衡算法修改成round-robin weight，接着在lvs所在的物理机上开发一个能够修改lvs配置的脚本，然后公开一个（HTTP）接口，让线上压力测试系统进行调用，线上压力测试系统会根据经验值逐步发起HTTP接口的调用，将引流的权重逐步加大。线上压力测试系统提交给接口的参数包含权重和机器名，告诉 lvs 要多少权重、到哪台机器，可以手动停止也可以自动收集机器上的资源情况，如CPU、CPU Load、内存、异常、GC次数、RT等。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>2.7层负载均衡器代理地址变更引流</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在大型网站架构中一般4层负载均衡器主要用于均衡负载，高效地转发数据包。7层负载均衡器代理主要用于日志采集，线上压力测试系统会调用Ngnix所在物理机上的能够修改代理转发的服务器地址（默认是本机）的接口（修改ngnix.conf中的proxy_pass配置），线上引流压力测试系统会根据一定的时间周期，将应用集群中Nginx的配置地址逐步修改到目标机器上</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对于7层负载均衡的引流测试方法而言，机器数量越多，引流的效果越好。引流可以从小流量开始逐步加大，直到峰值QPS出现。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>需要从以下4个方面来避免单机压力测试出现问题</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>1.适当的流程保障在单机引流压力测试期间需要专人“盯防”。针对某些服务的压力测试，需要多方一起保障。对于应用的压力测试，一般只需提请压力测试的人员在场，并且在压力测试结束的时候恢复现场（恢复到原来的流量比例），这些基本流程可以固定下来，作为一种制度来进行规范化的保障。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>2.时刻关注报警在引流压力测试期间，单台机器上的流量会快速增加，当资源出现瓶颈时，超时（CPU、Load、远程、丢包）异常开始出现，这些异常的出现往往意味着线上开始出现问题。所以要时刻关注报警，必要时立即停止流量压力测试。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>3.登录到线上服务器关注异常一般监控系统很难做到非常短的间隔时间的监控，另外监控存在一定的延迟，因此要确保没有问题，需要登录到引流的目标机器上观察日志和各种资源的表现。发现表现异常时，及时进行处理。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>4.慎重对待公共服务压力测试某些公共服务，例如会员服务，全网站都会依赖，当对会员服务进行压力测试单机容量时，大量应用对服务的流量会引流到某个单点的公共服务器上，导致下单关键链路上的应用出现大面积异常，进而影响用户下单。所以针对服务的压力测试，一定要将优先级调整到最高，并且需要关键应用的所有者进行线上观测，避免发生故障。</p>
</blockquote>
</blockquote>
<h3 id="◆-9-4-大型网站常用的容量评估方法" tabindex="-1"><a class="header-anchor" href="#◆-9-4-大型网站常用的容量评估方法" aria-hidden="true">#</a> ◆ 9.4 大型网站常用的容量评估方法</h3>
<blockquote>
<blockquote>
<p>大型网站常用的容量评估方法</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>二八原则评估法——新业务评估的基本方法</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对于一个普通的营销场景普遍可以用二八原则评估法进行容量评估，当然秒杀、限时抢购、限时活动除外。对于秒杀、限时抢购活动，可以根据业务预计的时间进行调整。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>评估QPS=（业务量/24×3600）×80%/20%</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>有历史数据参考的容量评估——GMV线性比例评估法和GMV转化评估法</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在做大促的过程中，往往会有两个关键的指标：总GMV和总PV。通过这两个指标来得到各个应用、中间件和存储需要提供的容量。容量评估方法一般针对每个应用集群评估需要提供的容量。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>1.GMV线性比例评估法</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>监控系统给出的每个应用的峰值QPS（Peak QPS），一般从分钟维度计算Cookie Log，得到每秒处理完成的最高请求数。预留的Buffer指的是业务保障目标和技术保障目标的比例，一般技术保障目标会预留30%的Buffer给业务，也就是预留Buffer=1.3，这样可以避免因为业务评估过小导致影响大促。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>线性评估法简单、评估速度快，但是准确度极差。当网站流量变大时，会带来非常大的误差，不仅造成机器资源的浪费（钱能解决的问题不是大问题），而且造成大量人力资源的浪费（需要做瓶颈的排查和定位工作），在较短时间内完成压力测试目标非常困难。线性评估的主要不足在于忽视了链路的差异，导致评估严重失衡。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在一次大促过程中，各种流量分布并不均匀，后台应用如订单管理、前台应用首页，以及活动页面的流量分布是不同的，前台应用之间的流量分布也是极其不均衡的，如活动页面和Detail 页面的流量可能比搜索 List 页面的流量要高很多，但是如果按照相同的比例评估就会造成评估不均衡，甚至严重失衡，非常可能出现搜索List页面的容量评估过高，而Detail页面的容量评估过低，这种形式的评估可能导致大促失败。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>2.GMV转化评估法</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>它能够根据实际链路到订单的转化率来推测页面的QPS。它还能够根据业务的表现推测出每个业务需要提供的QPS。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>可以将各个系统按照转化率的方法进行推理。第1步，订单QPS预估。GMV/单价=订单数，即一天内完成订单的请求数，再根据历史数据得出峰值订单QPS。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>第2步，根据转化率预估关键页面QPS。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>关键页面QPS=该页面渠道到订单的QPS/到订单的转化率。由于订单QPS由多个渠道聚合而成，对于Detail页面的评估，需要知道直接转化到订单的渠道的各自占比情况，再根据占比情况来确定由Detail页面产生的订单QPS所占的比例。因此QPS（Detail）=订单QPS×Detail产生的订单的占比/Detail到订单的转化率。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>第3步，根据主页面和异步请求的比例关系，预估异步Ajax请求。异步Ajax请求如果流到和主页面同样的应用上，可以根据应用监控获取主页面和异步Ajax请求的比例，进而预估异步Ajax请求QPS。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>第4步，根据主页面和非关键链路页面的比例关系，预估非关键链路页面QPS。同样非关键路径页面请求如果流到和主页面一样的应用上，可以根据应用监控获取主页面和非关键链路页面请求的比例，进而预估非关键链路页面QPS。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>第5步，预估爬虫QPS。爬虫也会有一定比例的访问，需要计算出大概的比例。要指出的是，爬虫的数量并不会随着大促活动的变化而变化，所以爬虫QPS的预估一般不能简单地按照比例来进行，可以按照绝对值来进行。从实际观测数据来看，爬虫在大促期间占比会变小，因为其他流量变大了，而爬虫的爬取量一般相对比较稳定。爬虫日常占比较大，大促期间占比较小，是否计算爬虫QPS，要看爬虫爬取高峰和大促峰值是否重合。如果重合，那么需要在峰值之内计算；如果错峰，那么爬虫爬取带来的流量可以忽略。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>最终预估QPS=关键页面QPS+异步Ajax请求QPS+非关键链路页面QPS+爬虫QPS</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在容量评估的演进过程中，总发现实际的大小比评估的大小要小很多，这就是因为在容量评估的过程中存在比较大的误差，这个误差是去除Buffer之后的误差的体现。例如业务预估需要1亿的QPS，为了防止合理的突发流量访问，技术保障目标往往提高30%～40%，它会保障到1.4亿的QPS。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>通过仔细回顾和重新检查发现了以下问题。1）评估误区之一：单渠道全占式评估错误</p>
</blockquote>
</blockquote>
<h2 id="◆-第10章-高性能系统架构模式" tabindex="-1"><a class="header-anchor" href="#◆-第10章-高性能系统架构模式" aria-hidden="true">#</a> ◆ 第10章 高性能系统架构模式</h2>
<blockquote>
<blockquote>
<p>在大型网站架构过程中，存在一些共性的解决方案，这些解决方案在很多场景中得到了体现。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>一个高性能网站一般都会用到缓存架构，缓存意味着高性能，通过空间换时间。近代CPU能力的高速提升，除了与核心架构有关，缓存的作用也功不可没。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>从架构的角度来说，更重要的是体现高水平伸缩能力。单机吞吐量高并不代表能够通过增加机器数来满足高容量的需求，影响水平扩展能力的因素很多，所以有必要介绍一下高水平扩展能力的常见架构和部署模式。</p>
</blockquote>
</blockquote>
<h3 id="◆-10-1-无状态架构" tabindex="-1"><a class="header-anchor" href="#◆-10-1-无状态架构" aria-hidden="true">#</a> ◆ 10.1 无状态架构</h3>
<blockquote>
<blockquote>
<p>10.1 无状态架构</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>第一个问题是用户登录的问题。小型网站通常将用户的登录信息存在Session中，Session是典型的有状态架构，而负载均衡会将用户流量均分在集群的每个负载中，用户的第一次请求分配到集群中的A机器上，后续的请求可能会分流到B机器上，那么Session的信息就会丢失。为了处理负载的随机分配问题，可采用Session复制的机制，Session复制易造成水平扩展能力急剧下降，同时也会因为复制的延迟导致系统出现稳定性问题。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>解决方案一——Session复制</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>Session复制发生在Session写入时，通过同步或者异步的方式进行Session复制</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>[插图]</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>全量Session复制的架构有明显的伸缩性问题，随着集群的扩大，如果同步全量复制不仅会导致集群的吞吐能力受到极大的限制，同时也会造成客户端响应的延迟急剧增加。如果异步复制，会导致在Session复制没有完成之前，请求被分配到另外一个实例上，从而出现可用性问题，在大规模集群的情况下也会造成网络风暴，导致整个机房的容量和高可用性出现问题。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>为了解决全量复制带来的复制成本和故障转移问题，WebLogic应用服务器采用主从复制，当一个实例不可用时，请求会被分配到从实例上。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>解决方案二——Session Sticky</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>负载均衡技术新增了Session Sticky机制，确保用户的访问通过负载均衡设备固定分配到用户第一次登录的应用服务器上，来解决Session复制的问题。但是这个方案最大的问题是故障转移能力缺失，当 Sticky 的机器不可用时，就会造成Session 丢失。为了解决这个问题，类似于WebLogic的解决方案，采用主从复制，即当一台机器不可用时，用另外一台机器进行托管。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>解决方案三——Session集中式存储</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>无论是Session复制还是Session Sticky都带来了很大的伸缩性问题，Session集中式存储是将Session信息缓冲到分布式集中存储中，应用服务器即使出现故障，也不会影响用户的登录，实现无缝的故障转移。同时 Session 集中式存储如 Tair、Redis，本身具有过期时间的自动管理功能，Session过期的问题自然得到解决</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>[插图]</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>Session集中式存储通常有两种实现方式。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>第一种，通过Web容器插件（如Tomcat、Jetty）来实现，Tomcat有tomcat-redis-sessionmanager,Jetty 有 jetty-session-redis。优点是对开发人员透明，开发人员还是跟以前一样使用Session，不需要增加、修改任何代码；缺点是过于依赖容器，容器升级问题比较麻烦，好在容器升级是非常大的事情，不需要经常升级。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>第二种，流行框架的会话管理工具，例如spring-session等，可以理解为替换了Servlet那一套会话管理，不依赖容器，不用改动代码。如果采用spring-session，则使用spring-data-redis连接池。Spring使用范围较广，扩展性也好，缺点是只针对Java语言。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>集中式存储需要考虑高可用的问题，一旦某个节点不可用，会出现严重的问题。如果是Redis，可以采用主从复制架构，Redis有强大的Master和多Slave复制的能力</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>[插图]</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>要实现自动故障转移，可以增加一层代理，在代理和Redis之间配置心跳检查</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>[插图]</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>解决方案四——基于浏览器Cookie的无状态架构</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>Session是有状态的，Session复制的架构会造成扩展能力急剧下降，Cookie的共享方式是，通过浏览器记录，将共享的具体信息存储在 Cookie中，目前很多大型电商网站都采用这种架构。Cookie的最大问题是，大小存在限制，在有限制的情况下，维护Cookie比较烦琐，特别是在优化Cookie大小时，通常会下调已经没有维护的业务，但是修改Cookie是牵一发而动全身的事情，很容易造成大故障。另外要注意Cookie安全问题（要防止被篡改）和过期时间。</p>
</blockquote>
</blockquote>
<h3 id="◆-10-2-基于负载均衡器的水平扩展架构" tabindex="-1"><a class="header-anchor" href="#◆-10-2-基于负载均衡器的水平扩展架构" aria-hidden="true">#</a> ◆ 10.2 基于负载均衡器的水平扩展架构</h3>
<blockquote>
<blockquote>
<p>10.2 基于负载均衡器的水平扩展架构</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>一个应用集群如果采用了无状态架构，应用集群就可以水平扩展了，但是7层负载均衡器如何水平扩展就成了下一个问题。4层负载均衡器的吞吐量和转发速度大大优于7层负载均衡器，4层负载均衡器的高效率转发基于4层转发，包封装层数较少，HTTP 7层封装转发效率较低，但是功能也较为强大，7层转发可以做到基于Cookie的负载均衡。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>7层负载均衡器在大型网站中的作用通常有两个，一个作用是统计QPS，将HTTP的所有访问记录在日志里面，QPS可以基于日志进行，另外一个作用是通过URL的rewrite来实现静态化链接到动态化的重写。考虑效率问题，为了实现7层负载均衡器的水平扩展，通常在7层负载均衡器上架设4层负载均衡设备或者软负载服务器。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>图10-5是F5的4层负载均衡的示意图，F5基于主备（冷备）切换的方式实现高可用，备份F5和主F5进行心跳检测，当发现主F5心跳检测异常，备F5会启动全托管，主备之间的配置完全相同，路由器会路由到备F5。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在很多大型网站中，为了实现4层负载均衡器的高水平扩展能力，通常通过Anycast 和ECMP等价路由技术将LVS组成一个OSPF（Open Shortest PathFirst，开放式最短路径优先）网络集群。OSPF是一个内部网关协议，LVS和交换机间运行OSPF协议，交换机上生成该VIP的等价路由ECMP，从而实现更高能力的水平扩展和伸缩</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>[插图]</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>[插图]</p>
</blockquote>
</blockquote>
<h3 id="◆-10-3-基于dns的负载均衡" tabindex="-1"><a class="header-anchor" href="#◆-10-3-基于dns的负载均衡" aria-hidden="true">#</a> ◆ 10.3 基于DNS的负载均衡</h3>
<blockquote>
<blockquote>
<p>10.3 基于DNS的负载均衡</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在某些场景下，为了降低部署的复杂度，同时在容量需求还不够大的时候，在技术储备还不是很好的情况下，可以采用DNS多A地址轮询来实现4层负载均衡的水平扩展，DNS的负载均衡有如下的问题。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>首先，故障转移相对而言较为困难，可维护性差。DNS的负载均衡基于配置多A地址进行轮询，如果一台服务器失效，会导致将域名解析到该服务器的用户看到服务中断，即使用户按Reload按钮也无济于事。系统管理员也不能随时将一台服务器切出服务进行维护，例如进行操作系统和应用软件升级，需要修改RR-DNS 服务器中的IP地址列表，把该服务器的IP地址从中划掉，然后等待一段时间，直到所有域名服务器将该域名到这台服务器的映射淘汰，所有映射到这台服务器的客户机不再使用该站点为止。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>其次，负载倾斜可能会非常严重，由于DNS是从Local DNS到Root DNS，一直到权威DNS进行递归解析的，每层都有缓存，一旦有缓存，DNS解析请求不会发送给权威DNS进行解析，从而造成RR轮询不均衡。Local DNS根据TTL进行缓存，而Local DNS存在很强的地域性，例如一个大型运营商在某个地区的Local DNS是相同的，由于运营的大小差异，会造成负载严重倾斜，服务器负载严重不均衡。</p>
</blockquote>
</blockquote>
<h3 id="◆-10-4-读写分离架构" tabindex="-1"><a class="header-anchor" href="#◆-10-4-读写分离架构" aria-hidden="true">#</a> ◆ 10.4 读写分离架构</h3>
<blockquote>
<blockquote>
<p>10.4 读写分离架构</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>读写分离架构是从小型系统过渡到大型系统的过程中常用的架构，其根本目的是减少写的压力，让读的性能更好，同时读和写互不影响，也消除了读和写之间的锁等待，减少延迟，从理论上能够将应用访问数据库的性能提升一倍。此架构适合在读一致性要求不高的场景中使用，Master 主库服务器负责包括读在内的事务型数据库操作，这是为了有强一致性的场景，Slave辅库服务器负责无事务需求的读操作。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>路由层的工作一般由中间件来完成，如淘宝的TDDL中间件就被用于完成此项工作。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在数据分布式存储——基于分库分表水平切分的架构中，也可以用读写分离的架构进行性能改善，短期内能够产生较为直观的效果。需要特别注意的是，在读写分离时要考虑程序本身是否做了事务控制，如果没有，会造成读写分离在不同的库中数据同步不及时，造成逻辑错误。例如程序的逻辑是根据读的结果来判断是否可以写，当在备库读时，由于主库的写未及时同步到备库，因此读不到数据，判断为空，后续的写操作也无法继续</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>[插图]</p>
</blockquote>
</blockquote>
<h3 id="◆-10-5-基于数据水平切分的水平扩展架构" tabindex="-1"><a class="header-anchor" href="#◆-10-5-基于数据水平切分的水平扩展架构" aria-hidden="true">#</a> ◆ 10.5 基于数据水平切分的水平扩展架构</h3>
<blockquote>
<blockquote>
<p>10.5 基于数据水平切分的水平扩展架构</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>数据水平切分主要针对写进行水平扩展，关系型数据库的写存在明显的上限，一般来说单库写数据时QPS不超过3000，单表最好不要超过4000万行，否则查询、写的性能将急剧衰减。在强一致性的场景下，通常必须使用数据库进行数据的水平切分，将请求分散到不同的物理库中。对大型电子商务系统来说，有时商品数会达到数十亿，订单数也会达到数十亿，这种规模的数据，任何一个库都存放不下。将数据进行分库、分表存储，可以降低在查询时需要读的数据和索引的页数，同时也降低了索引的层数，从而提高查询速度。通常不建议将表进行垂直切分，垂直切分将字段进行分离，数据库的维护成本大大增加，同时对于开发使用极不方便，这就是像淘宝的TDDL等中间件，基本都是利用水平切分的方式进行数据可扩展的原因。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>基于分库的水平分割：按记录行进行分割，不同的记录通过分库分表路由规则计算分别保存到不同的库中，将数据分离在不同的库中进行存储，每个子表的列数相同。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>基于分表的水平分割：将表进行别名分割，通过路由规则进行计算，分到不同的表中，通常表名在数据库中将以别名的形式存在。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>例如，将数据库分成16个库，每个库16张表，总共256张表。将商品进行分库分表存储，路由中间件主要完成SQL的透明解析、连接池的管理、并发控制和主备切换，对调用端透明，路由规则通常需要由使用方进行定义，可以根据自身的业务进行调整。例如，可以将商品ID的后4位作为分库位，中间4位作为分表位，也可以简单地将整个ID作为分库分表位。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>[插图]</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在分库分表的情况下，会产生跨库查询性能等问题，在这种情况下通常使用搜索引擎构造一个宽表，搜索引擎利用索引倒排的技术，基于行进行负载均衡，请求平均分发到某一行，行中的每个列服务存储不同的索引，一行组成的服务器是索引的全量，最后由merge Server进行合并</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>[插图]</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对于跨库事务，要做到强一致性必须采用分布式事务解决方案。很多时候，我们要尽量避免分布式事务，而采用本地事务解决问题。分布式事务解决方案，也可以通过补偿预案进行补充。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>为了避免跨库访问，分库分表字段的设计要符合以下基本原则：根据业务场景真实访问的多少来进行分库分表字段维度的设计，以多的场景为最基本的分库分表维度。在实战中，通常会用很多维度进行分库分表的设计，例如买家、卖家都有查看订单的需求，通常以买家维度查看为主，同时买家用户体验的要求更高，订单表的维度设计需要以买家维度来进行设计，另外可以通过异步化消息的方式来进行卖家维度分库的设计。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在实战过程中理赔赔付的额度和保单的保额有一定的关系，理赔的额度最高不能超过保单的保额，我们需要从保单表中获取保额的信息。从架构的角度来说，可能会将理赔服务和保单服务进行切分，这就造成了分布式事务，需要全局事务进行保障。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在分库分表字段上，为了避免跨库事务，将理赔单据和保单的分库数据放在一个逻辑库中，在设计时将理赔单据的后4位作为分库分表位。但是如果将理赔和保单的服务分在不同的服务里面部署就会存在跨库事务的问题，理赔时会通过RPC接口调用保单的服务查询保额，此时本地事务无法跨应用生效，为了避免这种情况，可将理赔服务和保单服务进行合并。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>[插图]</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在实战中，分库是逻辑的部署单元，为了节约物理资源，通常会将多个库部署在同一台物理机器上，往往都会交叉部署，笔者曾经碰到评价库和商品库放在一个物理机器上，由于评价和批量查询计算任务而导致相互影响的情况发生，在部署时通常要遵守以下基本原则。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 尽量将重要的业务单独进行物理部署重要的业务单独部署可以避免相互影响，因为一处问题会影响全局。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 尽量将重要的业务和批量化的业务分开部署</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>单一的写和读性能对数据库的影响较小，特别是在索引设置不合理的情况下，会导致整体磁盘性能急剧下降，通常批量读取还会涉及幻读等问题。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 尽量将非重要业务部署在一起非重要业务放在一起，即使出现问题，也不会造成很大的影响。</p>
</blockquote>
</blockquote>
<h3 id="◆-10-6-缓存架构" tabindex="-1"><a class="header-anchor" href="#◆-10-6-缓存架构" aria-hidden="true">#</a> ◆ 10.6 缓存架构</h3>
<blockquote>
<blockquote>
<p>10.6 缓存架构</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>缓存架构是典型的以空间换时间的优化方法。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>缓存的基本属性● 命中率：表示缓存命中的次数占总请求数的百分比，这是缓存设计的重要质量指标之一。● 容量：即缓存介质的容量最大值。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 成本：即开发成本、部署成本、软硬件成本。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 过期时间：缓存一般都有过期时间属性，也可以设置成永不过期。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>缓存的分类1.按照存储位置分类</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 本地缓存</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>将数据缓存在本地，常见的本地缓存开源框架有EhCache、OSCache、iBatis和Hibernate，本地缓存数据已经和这些缓存框架做了很好的集成，通过配置化的方式得到无侵入式的使用。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 分布式缓存</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>常见的分布式缓存框架包括业界比较流行的Memcached,Tair的MDB解决方案对Memcached进行了很多借鉴。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>2.按照存储介质分类</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 内存缓存：将数据缓存在内存里。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 磁盘缓存：Tair 的 LDB 解决方案是将数据缓存在磁盘（SSD）上，磁盘缓存的主要优势在于数据不容易丢失，即使在掉电的情况下也不会丢失，数据安全性比内存缓存要高。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>3.按照缓存的对象分类</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 页面缓存</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对于商品详情页面来说，Web页面缓存的key可以是商品ID，对象是整个商品ID对应的HTML页面。通常缓存整个HTML页面的场景很少，一般的Web页面都含有大量的动态因素，例如商品的价格、库存的数量，这些数据都需要强一致性。通常在这种场景下会使用片段页面缓存，ESI（Eage Server Include）是片段页面缓存的常见解决方案，也是最早出现的CDN缓存HTML 的解决方案，它已经成为一个通用的标准，Varnish 及 Squid 页面缓存解决方案都支持ESI的语法和标准。页面缓存也可以通过CDN进行动态加速。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 对象缓存</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>通常对象缓存是为了减少远程网络开销和网络延迟，或为了减少CPU的计算以提升性能，常见的如DB数据对象缓存和远程服务对象缓存。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>缓存使用常见的问题和误区</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>缓存的特性是通过将计算好的结果缓存在内存中，后续在内存中直接获取，节省网络开销或者CPU时间，缓存带来的最大问题是脏读和数据的不一致。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>缓存带来的问题</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>（1）修改本地缓存对象，导致被其他线程读取，缓存对象被污染，引起故障。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>本地缓存是将数据缓存在本地，由于线程通过引用修改了对象的值，其他线程读取时，读取了已经改变的值，导致故障，所以一定要确保本地缓存使用时，缓存的对象不会被修改，只是读取。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>（2）将缓存在同一时间点失效，导致缓存击穿，引发故障。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>缓存完全被击穿是非常危险的，除非平时访问量很低，否则不要轻易地使缓存全量失效，或者在缓存设计时，要重点考量后端服务的承受能力，做好充分预估后再做这样的设计。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>（3）由于Tair缓存出现热点，导致Tair服务器崩溃而引发大规模故障。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>缓存命中率是检验缓存效果的重要指标，命中率越高效果越好。命中率的高低通常和业务特点有关，本身不易变的业务数据命中率较高。缓存的命中率和其他因素也有关，例如过短的缓存时间，过少的缓存空间，导致被LRU交换出去，同时如果缓存对象设计不合理，对象极其易变，缓存频繁失效，最终也会导致命中率过低。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>（4）不考虑主动缓存失效，导致缓存对象已经被污染，引发故障。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>例如，在物流模板缓存架构升级过程中，缓存设置的过期时间是两天，这意味着两天时间内的物流模板非常可能是旧的数据。当时由于开始卖家设置的模板有问题，在几个小时之后修改了模板的内容，但是迟迟不生效，而在缓存架构设计的过程中，只是简单地认为卖家更新模板的概率很小，至少在一周以上才会进行模板的修改和更新，所以当时没有考虑主动失效方案，后来因为这个问题出了故障。经历这件事情之后，我们在缓存架构的规约中规定，所有的缓存架构方案设计必须实现主动失效的功能。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>（5）缓存的对象过于复杂，因其中某些对象失效过于频繁，导致命中率急剧降低。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>通常这种缓存对象包含了多个子对象，这些对象中只要存在一个易变的对象，就会被失效，导致缓存命中率过低。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>（6）缓存对象被过度拆分，调用端要多次读取才能完成业务的拼接，导致性能损耗。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>通常这种情况发生的原因是缓存对象缺乏聚合度，聚合对象原本是业务需要的，然而需要多次RPC调用才能获取完整的业务对象。这种情况经常发生，网络开销始终是存在的。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>（7）内存型缓存，不考虑内存丢失兜底，可能因为掉电或者内存丢失，导致故障。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>缓存使用场景</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>（1）什么时候可以使用缓存？</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 在容量无法突破的情况下，考虑使用缓存来提升容量，否则尽量不要使用缓存。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 对用户访问延迟有明显改善的情况，同时对优化RT有明显的效果，例如如果不缓存需要2s，缓存之后只需要100ms，可以考虑使用缓存。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 使用缓存之前一定要了解缓存带来的问题，缓存不保障数据的一致性，要能接受这种不一致带来的各种问题，甚至资损问题。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>（2）使用内存型缓存（MDB）还是磁盘型缓存（LDB）</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>MDB是内存型缓存的代表。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>优点：● QPS极大，是LDB的数倍。● 单份缓存，一致性相对较高。● 响应更快。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>缺点：● 不保证数据的安全，在掉电的情况下数据容易丢失。● 默认只有一份数据，没有备份，数据本身的可靠性稍差。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>所以使用MDB时要特别注意，一般需要MDB+DB做双防护，避免数据丢失导致数据无法获取。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>LDB是磁盘缓存的代表。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>优点：● QPS比数据库大很多，比MDB小。● 双份数据，数据安全性较高（不会因为掉电而丢失）。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>缺点：● 两份数据，一致性相对较低，当出现一份成功一份失败时，不会回滚。● QPS稍弱。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对性能有极高要求时通常使用 MDB，而对数据安全、数据可用性要求高时使用LDB。例如，支付宝的Session信息就存放在LDB上，LDB通常都会被当作数据源使用，不需要用DB进行兜底。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>两者都需要考虑的问题：缓存场景下的一致性是无法得到根本保障的，要在强一致性的场景下，尽量弱依赖。例如，当数据库进行主备切换的时候，通过Tair来进行幂等的校验，防止因为主备延迟导致幂等失效。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>缓存使用规范和原则</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>本地缓存需要确保缓存对象不会被更改，或者尽量少使用本地缓存，除非特殊场景，例如秒杀场景，对于QPS有极高的要求。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 内存缓存一定要考虑双防护，后端一定要在DB层进行兜底。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 内存缓存一定要加过期时间，一般以业务上数据变更的时长为准。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 缓存的使用要实现主动失效方案，否则要特别说明。● 所有为突破性能容量上限而设计的缓存架构方案，必须考虑兜底方案。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 在设计缓存时，需要明确预估缓存命中率，过低的命中率导致的性能下降，比缓存数据不一致带来的风险更大。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 避免Tair访问热点，Tair的单服务器设计上限大约在20万QPS，超过容量时Tair将提供服务。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 设计缓存架构时，需要充分预估缓存不一致带来的风险，需要考虑快速发现方案、自动修复方案及兜底方案。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 快速发现：监控上需要设计如何快速发现Tair的标记是否丢失。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 自动修复：在出单时，校验数据库和Tair的标记是否一致，如果不一致，自动补充标记或者去除标记（如卖家退出运费险赠送时），将影响范围减小到最小。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 超预估兜底：例如，业务上要求买家在前台下单页面看到运费险的标记时，一定要出单，所以在出单时，可以设置为如果超过天数不一致或者业务资损超过一定的限额，则禁止出单。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>例如，对于运费险来说，卖家如果参加了运费险赠送活动，买家在购买该卖家的商品时就会出现运费险的标记，这个标记就是放在Tair上的，之所以放在Tair上，是因为考虑到运费险访问量巨大，用DB的方式支撑这么大的容量，需要巨大的投入。在这种情况下，无疑使用Tair是合理的，但是需要在设计时进行以下三个方面的设计。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 设计缓存架构时，需要充分考虑缓存的分层影响，对于一致性要求高的元素和一致性要求不高的元素进行分离设计，充分评估影响面。例如工厂设计产品时，产品的价格是一致性要求比较高的，需要考虑分离设计，将价格进行分离，或者采用乐观锁版本号，如果版本发生变化，则从数据库中获取，如果没有发生变化，则从Tair中获取。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 从Tair中操作数据时，一定要解析resultcode，不要根据异常进行成功与否的判断。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 客户端Tair不会抛出任何异常，即使在连接Tair服务器出现问题时，只会在resultcode中告知是连接超时还是错误。</p>
</blockquote>
</blockquote>
<h3 id="◆-10-7-近端架构" tabindex="-1"><a class="header-anchor" href="#◆-10-7-近端架构" aria-hidden="true">#</a> ◆ 10.7 近端架构</h3>
<blockquote>
<blockquote>
<p>10.7 近端架构</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>近端架构主要解决的是由于 RPC 远程调用带来的网络开销，特别是在收敛比较小的机柜和服务器上，网络的瓶颈更容易出现。另外还要考虑线程池的开销，序列化、反序列化的开销，如果在应用层网络传输没有采用类似epoll的架构，还有内核区到用户区内存拷贝的开销。近端架构通常在性能要求极高的情况下采用，近端部署的逻辑通常在本地执行，逻辑相对比较简单，只有一些本地执行的代码。在实战中，如双11的宝贝详情页面对 QPS 有非常高的要求，对于营销优惠、运费的计算，还有运费险的推荐，可以对远程RPC调用的代码防盗宝贝详情应用进行部署，从而大幅度提高性能。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>近端架构通常作为大促的常规预案，在日常正常流量的情况下，建议不要启用，近端部署的方式会带来维护性问题，也违反了架构的高内聚、低耦合的原则。</p>
</blockquote>
</blockquote>
<h3 id="◆-10-8-异步化架构" tabindex="-1"><a class="header-anchor" href="#◆-10-8-异步化架构" aria-hidden="true">#</a> ◆ 10.8 异步化架构</h3>
<blockquote>
<blockquote>
<p>10.8 异步化架构</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>异步化架构是高扩展能力的常见模式，一般同步调用的架构会占用连接资源，特别是在QPS要求极高的情况下，连接资源占用会引起整个集群的连锁反应，异步化架构通常用在对实时性要求不高的场景。在实战过程中，很多地方可以异步化，比如运费险出单的场景。大家都知道双11从零点开始，下单会出现极高的峰值，如果要在下单的同时出运费险保单，保险核保、出单动作是非常耗时的，保险的核心系统要达到和主交易同样的吞吐能力，是非常难的事情。通过业务分析可知，通常在用户发货之后才会有退货的场景，所以实际上从业务的角度来说，没有必要在下单的时候就产生保单，完全可以在主下单交易完成后再完成出单的动作。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>异步化架构通过事务型消息异步架构来完成：主链路业务处理完成后发布消息到消息中间件，消息中间件推送给消费者端应用，消费者端将异步处理任务给到数据库，再由业务系统扫描取出任务进行相应的业务处理，由于扫描任务表通常通过多线程和线程池处理，线程池的并发数可以调整，从而达到削峰的目的。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>[插图]</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>异步化架构的优点是，不仅能够削峰，而且能够通过异步化从强依赖变成弱依赖，从稳定性的角度看，减少直接依赖可以提高稳定性。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>异步化架构的不足是，带来了维护成本，异步化往往需要后续的任务处理，需要建立任务表和任务扫描，维护的范围有所扩大，问题排查成本也比较高。</p>
</blockquote>
</blockquote>
<h3 id="◆-10-9-排队缓冲架构" tabindex="-1"><a class="header-anchor" href="#◆-10-9-排队缓冲架构" aria-hidden="true">#</a> ◆ 10.9 排队缓冲架构</h3>
<blockquote>
<blockquote>
<p>10.9 排队缓冲架构</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在极高并发的场景下，如下单场景，如果直接将请求发到数据库，会造成数据库的崩溃，通常的做法是将请求放到队列中，进行排队处理，避免大规模冲击到数据库。既然是大型网站就免不了高并发的读写操作，很典型的一个例子就是秒杀，这种高并发的写操作，如果一下子都涌入数据库中，会导致数据库的压力非常大，从而导致客户端的访问延迟增加，即使不挂也容易造成数据库的死锁，遇到这种一拥而入的情况，就必须进行线性化操作。在代码层面上可以用锁机制来串行化，在分布式中可以用“消息队列”来串行化，而且还可以通过逻辑操作对消息队列进行动态的防洪和控洪。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>排队缓冲架构，通常需要计算合理的阈值，超过阈值的部分使用队列进行排队缓冲，即将请求放到缓冲队列中进行串行化处理。一旦进入缓冲队列，用户端的响应时间会变长，注意要设置合理的队列长度，否则会造成用户体验变差。像秒杀这种场景，用户响应时间超过3s，页面可能处于白屏状态或者旋转等待状态。如果每次响应的时间超过10ms，那么队列的长度建议设置成300个，这样用户等待的最长时间不会超过3s</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>[插图]</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>淘宝开源框架Tengine也采用了类似的机制，Tengine可以设置限流阈值，超过阈值的部分不是直接被丢弃，而是放在一个队列里面</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在实战过程中发现了一个问题，当QPS压力测试到一定程度的时候，会一直处于一个稳定的状态，但是RTT会逐渐变长，而且没有任何错误的响应。通过了解原理，原来 Tengine 有个队列，当队列越来越长时，响应时间也会越来越长。</p>
</blockquote>
</blockquote>
<h3 id="◆-10-10-多机房架构" tabindex="-1"><a class="header-anchor" href="#◆-10-10-多机房架构" aria-hidden="true">#</a> ◆ 10.10 多机房架构</h3>
<blockquote>
<blockquote>
<p>10.10 多机房架构</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>多机房架构用于提升机房级别的水平扩展能力，提高吞吐能力。在大型网站的构建过程中，单机房在电力供应、服务器存放空间、设备的连接数、机房整体的吞吐能力等方面都有明显的容量上限，还有可用性方面的问题。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>包括数据库被误删、机器坏死、交换机故障、其他网络故障、中间件故障及安全攻击等，多机房架构起到了十分关键的作用。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>同城架构</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>同城机房的主要作用是通过双冗余能力的建设来提升可用性、提升灾备的能力，同时带来容量的提升。同城机房有热备和冷备之分，冷备只启用一个机房提供服务，当机房出现问题时，再启用另外一个机房。热备同时启用两个机房对外提供服务，淘宝早期也采用冷备的方式，将整个机房进行备份，缺点是成本太高，对于能够承担多少流量，需要不断演练，优点是架构相对简单，一个机房即为一个完整的单元，目前主流的同城部署架构都是热备</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>同城机房主要通过DNS的A地址轮询配置来实现机房级别的负载均衡，当某个机房不可用时，将DNS的A地址进行切换，对故障进行恢复。同城机房的难点不在于应用的故障转移，目前大型互联网的架构都可以做到应用是无状态的，所以在故障转移时，应用可以通过域名的A地址进行无缝切换，切换时间取决于DNS的TTL。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>同城架构的设计包含以下关键技术。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>1.数据强一致性保障</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在同城架构中，机房间的距离较小，不一定是同一个城市。例如也可以认为杭州和上海属于同城机房，上海和杭州的距离大约为200km，理论的网络延迟大约为1ms，如果深圳和杭州的距离是1300km，理论的网络延迟大约为5ms,5ms只是一个RTT的传输，如果一个数据包较大，可能需要多个RTT来回，那么访问数据库时会额外多5～20ms，这种性能是无法接受的。保证高性能是很重要的，这决定架构方案。强一致性保障有两种方案。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>方案一：数据库跨机房部署，互为主备</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>跨机房访问的延迟是可以接受的，为了减小架构的复杂度，确保数据的强一致性，在数据访问上多采用跨机房部署，机房间数据库互为主备，机房间通过专线打通。所以从数据层面来说，数据库访问的吞吐能力并未得到提升，但是存储的机器数量增加了一倍，理论上可以做到水平扩展。当一个机房的数据库出现问题时，将数据路由切换到备库，每个机房的备份，主数据都是完整的全量数据，容灾切换时，原有的连接会受到影响。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>如果服务连接到出问题的数据库，其连接数已经接近连接池的上限，主备切换的作用并不是特别大，主备切换起作用的是新连接会连接到备库。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>还有一个方案是当发现主备切换效果不大时，可以重启服务，但要避免在访问高峰时切换，否则连接池资源可能很快会耗尽，形成雪崩效应。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>总的来说，数据库跨机房部署互为主备的架构，优点是架构方案简单，有一定的故障转移能力，不足是数据层的故障转移能力和灾备能力一般。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>方案二：数据库冷备，主要考虑数据库的容灾</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>数据库冷备通过主机房的专线将数据同步到冷备的数据库，数据延迟要求较低，同城的机房确保都访问同一个数据库。此时应用、服务、缓存都自封闭，数据跨封闭访问。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>2.幂等失效处理</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在主备切换期间有另外一个需要注意的问题，主备切换期间容易造成幂等失效，导致资损。一般的做法是尽量等5分钟左右进行切换，以确保数据同步的完整。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>异地架构</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>异地架构是常见的高水平扩展能力的运维架构模式。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>同城机房将完整的业务数据全部放在一个机房里，包括交易、商品、导购、物流都在一个机房里，如果一个机房全挂，所有的业务都会受到影响。随着业务规模的扩大，一个城市的电力、机房的空间等无法满足需要，这是异地架构改造的动力所在。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>同城机房和异地机房因为机房间的距离差异，架构上有很大的不同。如果机房间存在跨机房调用，会产生极大的性能问题，所以异地架构一般采用单元化部署的方式，尽量将访问切分到一个机房内进行，在容灾切换时，又可以无缝切换到其他机房。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对于金融行业的很多场景，特别是支付场景及其他涉及资金流的场景，处理逻辑都需要强一致性。异地多活架构并不是所有的业务都使用单元化的解决方案，通常只需将用户直接相关的数据进行单元化架构部署即可，其他的如商品、会员等服务，单元化的改造成本很大，将买家和卖家的数据都在一个单元内封闭访问，会带来很多问题。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>淘宝将买家交易进行单元化架构的改造，确保一个用户在商品搜索、查看、购物决策、交易的链路等环节能够全部走通。将所有与这个用户相关的数据都放在一个单元里，基于交易的改造主要问题还是交易的写压力很大，容量上有很大的瓶颈。同时从用户体验角度考虑，交易是非常关键的环节，用户访问性能越好，成交转化的可能性越高。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>[插图]</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>异地架构关键问题和关键技术如下。1.单元数据如何切分</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>单元数据切分一般从用户维度进行，如Facebook的多数据中心，是基于用户维度进行切分的。在电子商务平台上，购物车的数据以用户为维度，这个数据可以封闭在一个单元里进行访问，但是商品、卖家等信息是归属于卖家维度的数据，这些数据封闭在一个单元里进行访问或者直接跨机房访问，跨机房调用会带来很大的延迟问题，这在性能上是无法接受的。这些公共数据在一个单元内访问，需要同步来实现，写在中心机房完成，再通过专线同步到各个单元机房。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>2.用户访问路由</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>按照买家维度进行单元封闭访问时，要确保写和读都在这个单元内，否则如果出现写在两个机房，就会出现问题。所以路由时要确定用户访问的规则，这就是图10-16中统一接入时遵守的规则。这个规则要考虑多种情况，可选的规则包括按照用户的常用收货地址、用户注册地和用户的IP地址进行路由选择，通常会按照用户的收货地址进行确认，即使用户异地出差，路由规则必须和用户常用收货地址所在的城市保持一致，当然这样做可能会牺牲一点用户访问性能。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>3.延迟保障</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>按照买家维度切换和归属单元的方式，可以将用户产生的数据放在一个单元内封闭访问，但是卖家的数据、商品的数据，要做到在一个单元内访问，需要单元的机房和中心机房之间存在数据同步，确保在一个单元内也能获取商品、卖家的信息。数据同步必须确保很小的延迟，用传统的MySQL基于binlog的主备同步，不能确保在很短时间内进行访问。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>4.发布和维护</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>代码发布时必须确保所有的机房内的代码是一致的，并且需要确保多个地方的访问是正常的，所以也需要构造一个高效的发布验证系统。</p>
</blockquote>
</blockquote>
<h3 id="◆-10-11-基于服务的可扩展架构" tabindex="-1"><a class="header-anchor" href="#◆-10-11-基于服务的可扩展架构" aria-hidden="true">#</a> ◆ 10.11 基于服务的可扩展架构</h3>
<blockquote>
<blockquote>
<p>10.11 基于服务的可扩展架构</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>基于服务（SOA）的可扩展架构更多地强调了复用性、组件化、高内聚、松耦合，解耦思想最终落实到架构设计中是业务能力组件化，组件间的交互通过服务接口进行，粗粒度的服务本身就体现了松耦合，解耦不仅包括组件的应用层，还包括数据库和数据层，都能够自成一套，可以独立进行需求、设计、开发、测试和运维的全生命周期管理。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>基于服务的水平伸缩区别于传统的基于soap XML报文的伸缩架构。基于RPC的服务伸缩架构是点对点的，是一种去中心化的架构模式。传统的SOA架构需要基于HTTP，一般HTTP需要经过7层代理服务器进行转发，从性能上来说HTTP的报文无论是基于JSON还是XML，在性能上都有很大的优化空间。在大型互联网的分布式架构中，普遍采用类似Dubbo、HSF框架的架构，可以很好地支持更多报文压缩协议，例如Google的ProtoBuffer协议，基于二进制的Hessian协议。ProtoBuffer协议通过索引表的方式，在交互时大幅压缩传输的内容，而Hessian协议则在压缩上取得了很大的优化。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在大型网站中采用基于服务的架构，目的如下。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>1.业务垂直化，高内聚低耦合，依赖弱耦合</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在业务发展初期，人少、团队规模小、业务简单，业务上的诉求更多的是快速功能迭代、快速上线，在简单的业务背景下，架构的分解也是非常粗粒度的，也是合理的。但是当业务发展到数百人的时候，业务需求复杂度大幅增加，之前的一个功能从头到尾都是几个人在维护，变成很多功能每个团队都需要维护。典型的案例是订单费用计算和物流费用计算，随着业务的发展，物流的费用在很多地方都需要展示，如搜索列表页面、商品详情页面、购物车下单页面、订单列表页面等。在早期的时候，可能这些展示需求都由一个人来维护问题不大，当分散到各个小团队的时候，每个人都必须对物流的逻辑有所了解，但是每个人理解的逻辑和获取的信息不同，可能造成用户看到的费用不一样，用户体验极差。所以需要功能内聚，由专门的团队来维护，提供服务化接口，供展示方调用。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>2.服务水平伸缩，和Web应用解除耦合</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>从伸缩性角度来说，如果功能耦合在一起，特别是Web系统和服务耦合在一起，带来的最大问题是伸缩困难。大家都知道动态Web页面的性能由于页面渲染带来大量的CPU消耗，导致性能很差，Web页面的QPS远比单纯的服务的CPU计算量要大很多，杂糅在一起往往导致整体的性能很难提升。将服务和页面解耦，服务可以水平伸缩，在制定性能优化策略时也可以分而治之找到瓶颈所在。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>3.可维护性提升</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>从高效功能开发角度来说，基于服务的架构最大的好处是将功能之间的强耦合变成松耦合，可维护性更强。基于以前业务二方库的实现，将重要的业务逻辑放在二方库里，当二方库需要升级时，将所有的依赖全部梳理出来，如果有些二方库没有更新，就不能立即发现问题。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>4.去中心化，点对点直连</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>传统的基于SOA的架构基于soap、restful接口，需要经过7层HTTP的转发进行负载均衡，水平伸缩时需要考虑7层负载均衡，部署复杂度相对较高，通过去中心化的SOA框架如Dubbo实现点对点直连，避免了ESB带来的伸缩性问题，水平伸缩能力更好。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>5.传输报文格式优化，性能高</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>不同于XML的报文格式，基于传统的SOA采用XML作为报文中介，传输内容多，接收方报文解析需要消耗不少CPU资源。XML报文有首尾对称的节点名，造成空间浪费明显，相应解析的效率也会降低。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>互联网常用的Duboo中间件，支持Hessian二进制压缩报文，报文更小，解析效率更高，还支持Google的ProtoBuffer协议，基于index的数据字典，通过双边的index对照来翻译出对应的方法名、节点名，从而大幅减少CPU消耗。</p>
</blockquote>
</blockquote>
<h3 id="◆-10-12-日结架构" tabindex="-1"><a class="header-anchor" href="#◆-10-12-日结架构" aria-hidden="true">#</a> ◆ 10.12 日结架构</h3>
<blockquote>
<blockquote>
<p>10.12 日结架构</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>日结架构是在金融类场景下常见的解决方案。在金融场景下一般需要依赖机构的能力，例如互联网保险平台，需要通过机构进行投保的核保和保单的生成，如果通过机构实时地交互，金融系统会承担很大的压力，无论是泛金融的保险场景，还是银行类的转账场景，系统的承压能力非常有限。日结的方式首先在业务上要得到认可，认可中间层先将业务进行处理，然后在T 日日结给机构。特别是在投保的场景下，核保规则可以前置在中间层业务里，风险也由中间层来把控，并且风险实际是可控的。这种场景不适合于像车险、寿险这种复杂的核保规则，车险的核保需要与行业平台进行交互，业务平台无法直接和行业平台进行交互，并且有些数据业务中间平台很难获得，如车险的出险记录、交通违法情况、续保的情况等。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>日结架构要求业务平台具有很强的掌控力，资金安全保障难度大，例如平台计算了错误的价格，如果跟机构交互，机构会对计算的订单金额进行校验，日结架构削弱了资金结算复核能力。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>日结架构的优点：交互解耦、直接依赖变成弱依赖、松耦合，性能和稳定性都有很大的提升。不足：对业务场景要求高，需要业务有很强的掌控力，时效性较差，容易出现资金安全问题。</p>
</blockquote>
</blockquote>
<h3 id="◆-10-13-热点避免架构" tabindex="-1"><a class="header-anchor" href="#◆-10-13-热点避免架构" aria-hidden="true">#</a> ◆ 10.13 热点避免架构</h3>
<blockquote>
<blockquote>
<p>10.13 热点避免架构</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>所谓的热点通常指同一个数据被多次访问和更新，而且访问量巨大，超过了单点服务器的容量上限，由于访问集中在一个单点上，导致访问负载无法均衡。常见的分布式架构是指访问被均匀地分散到不同的机器上，每台服务器承担的访问容量大体比较均衡。在以下情况下，会明显出现热点效应。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>1.电商爆品的描述场景</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>尽管大多数架构会采用集中式缓存架构来进行高容量访问的承载，但是由于缓冲的key是商品ID，一个商品缓冲会集中到一台服务器上，热点商品信息访问很可能超过单点缓冲服务器的容量上限。例如Tair的单点访问容量上限是20万QPS，一个数据库分库的访问上限可能只有3000～5000 QPS。在这种情况下，一个秒杀商品的库存信息更新，对应更新数据库的一条记录，而且在更新时为了防止并发，很可能要加行锁，无论是悲观锁还是乐观锁，如果访问量大，会出现数据库访问超时。可以说电商的秒杀系统架构是高性能访问最难的问题，特别是像淘宝这种量级的网站，不仅要提供高性能的访问，还要提供有效防止库存超卖的措施，中间某个环节出问题，非常可能出现库存超卖。同时要考虑稳定性问题，库存对应的数据库如果要和其他业务部署，最好将库存数据库进行部署隔离，避免由于库存系统的问题，导致整个业务不可用。在实践过程中，要完全避免库存超卖很难做到，但是确实可以将库存超卖的概率变得极小，秒杀系统的具体设计可以参考阿里官网发布的双11系列丛书。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>2.新闻热点场景</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>近几年随着娱乐产业的急剧发展，某些明星热点新闻被微博等媒体曝光之后，由于互联网的病毒式传播能力，导致在一个瞬间，同一条新闻的访问量急剧上升，出现热点访问的问题。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>3.金融类的热点账户场景</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在金融领域，热点账户的问题是非常常见的，在活动期间可能存在大量的用户同时向一个账户转账，这个账户对应数据库中的一条记录，需要频繁地对账户的余额进行更新，分库分表时大多数按用户维度进行切分，而热点账户在同一个数据库实例上，从而造成大量的锁竞争和锁等待，导致QPS过低。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>热点问题的解决方案基本是通过空间来换时间的，无论是金融类的热点账户、微博类型的访问量巨大的大 V，还是热点新闻类型的业务，都是通过空间的分散来解决的。热点大部分都会出现在存储上，缓存架构和数据库架构尤其明显。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对于热点新闻类型的业务，笔者通常将这些新闻缓存到集中存储的服务器上，由于一条热点新闻通常会Hash到同一个服务器上，而一台服务器的处理能力是有限的，此时需要考虑如何对该热点进行分散处理。本地缓存是最常见的做法，也就是将热点的数据提前缓存到本地服务器，将数据从集中的缓存服务器上，比较均匀地分散到本地的数十台服务器或者数千台服务器上，从而有效地分散压力。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>本地缓存架构要防止本地缓存全量失效的情况</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>本地缓存的key规则发生变更，导致全部的请求压到后端的热点存储上</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>[插图]</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>本地缓存架构的优点是，有效避免了远程缓存的单点服务器的热点访问压力，由于是基于内存的操作，存取速度快，有效地缓解了数据库的压力。缺点是本地缓存维护困难，很容易出现本地缓存穿透到远程集中式缓存或者数据库的问题，例如发布时忘记做灰度或者分批发布，导致本地缓存全部失效穿透。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>本地缓存架构要考虑诸多的后续维护问题，尤其要注意以下几点。（1）要防止本地缓存元素key的构造规则发生变化和升级，升级时要考虑灰度切流或者提前预热。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>（2）避免由于本地缓存穿透导致热点集中崩溃，最终导致严重的故障。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>（3）要防止本地缓存对象被修改，导致对象被污染。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>（4）要特别注意灰度发布，不能一次性暴力重启集群中的全部机器，要考虑本地缓存的预热方案。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在金融场景下，热点账户的问题通常可以通过业务处理方式的变更来解决，例如前面提到的异步化架构，通过分析业务场景将实时性要求不高的场景进行异步化，再通过异步记录的扫描逐个消化，将同步带来的集中冲击改成控制固定并发的压力，这种情况通常在业务上想办法，通过业务缓存来避免大量瞬间的操作发生，具体方案如下。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>方案一：日结批量入账</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在T＋1日，将热点账号的操作经过平滑削峰处理进行批量平滑入账，类似于异步化架构，这种架构通过业务上的缓存方式来实现。该方案的优点是，账户热度低、系统压力小，缺点是实时性差。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>方案二：缓存汇总入账</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>汇总入账是金融系统经常采用的业务分散的策略，通过时间换空间来减缓集中访问的压力，银行在T日的时间点完成所有成功交易汇总统计，通过交易凭证计算出总账，然后一次更新到结算账户，同时补充账务明细，用于明细对账。这实际上是异步化架构的一种具体业务形式的实现。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>优点：数据库压力小，只要在T日汇总计算出结算金额，再与流水记录进行核对，就可以将汇总金额更新到指定账户。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>缺点：结算金额不能实时更新，在T＋1日才能看到结算金额，结算账户的资金并未实时结算更新，T日的交易款要到T+1日才能结算到账。同时用户体验会比较差，用户发生了转账行为，但是当时总金额并没有发生变化。如果转账时账户是可以用的，告知用户转账成功，在实际转账时却发生失败，可能很多用户无法理解。</p>
</blockquote>
</blockquote>
<h3 id="◆-11-1-大促保障概述" tabindex="-1"><a class="header-anchor" href="#◆-11-1-大促保障概述" aria-hidden="true">#</a> ◆ 11.1 大促保障概述</h3>
<blockquote>
<blockquote>
<p>大促保障概述</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>运营的三个重要工作是拉新、促活和转化。像大促运营做预热工作，就是典型的先将用户圈定，在大促当天通过多种运营活动进行召回，如定点发优惠券、给用户发送营销短信等。大促当天转化率会有很大的提升，同时也意味着流量和并发会有所爆发，所以在大促环境下，对系统的容量要求很高。通常大促性能保障工作，跟业务或者运营的玩法直接相关，要做好保障工作，必须对运营的每个细节进行详细的了解，需要找到能配合运营的关键点、关键链路，将关键链路保障好。大促的保障工作是一个系统化的工程，包括项目管理、组织保障、风险保障、容量保障等具体的工作</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>性能优化在很多时候要注重投入和产出的关系</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>大促保障并不只是性能相关的课题，还有风险保障体系和资金安全保障体系</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>大促保障整体流程</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>从技术的角度看，首先整个项目需要有一个虚拟的团队负责，特别是涉及整个公司级别的大促保障工作，需要每个事业部出一个负责人，由负责人推进整体项目。负责人需要每周发送项目周报，包含整体封网时间、发布冻结时间、关键问题的收集、全链路压力测试计划等。架构师主要负责大促需求的关键架构设计。开发工程师主要完成项目需求，做好关键链路的依赖梳理、风险的识别、风险的修复及最终的风险预案的产出。负责性能的工程师必须配合全链路压力测试，准备好压力测试脚本，以及问题排查脚本，提前对大促关键链路做好细致而完备的监控，既要有总量的监控，也要有细节的监控，涉及资金安全，必须提前部署核对脚本、止血应急熔断开关。运维工程师需要协同开发工程师进行容量的评估和机器的采购。安全工程师主要负责应用的安全、恶意攻击的安全防护工作，特别是常见的3层攻击如SYN Flood攻击、DDoS攻击、7层CC攻击、越权攻击、SQL注入攻击和XSS攻击的防护工作。中间件工程师需要从工程师那里收集容量信息。公共服务提供方的工程师需要收集各种容量报备信息，并且做各种预案。数据工程师需要对数据库的容量进行整体评估，管理团队需要从组织上进行保障，制定制度，把握整体风险。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在大促的准备过程中，首先会成立大促项目，确立整体总负责人，总体负责人对整个项目负责，一般分项负责人包括业务负责人、大促应用负责人、运维负责人、数据库负责人、安全负责人、大促项目管控负责人、大促压力测试负责人、大促性能保障负责人、中间件负责人、各个业务的负责人，业务方负责人通常是业务运营代表。大促负责人负责项目进度的跟踪和问题处理，并且定期发送报告，报告进展。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>业务目标获取和分解，沟通业务玩法，同时确定业务数据化目标</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>业务评估主要关注业务量和业务玩法，特别需要关注各种渠道的流量分配，以及业务的客单价预估，需要提醒的是不同渠道、不同商品的客单价不同。业务玩法主要是为系统的峰值容量评估提供依据，例如秒杀的玩法，往往在10分钟之内就能够消耗完，就不能按照一天的访问量通过二八原则（峰值是平均值的4倍）进行预估。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>容量评估和验证，根据业务玩法分解，进行容量换算。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>容量换算通常从粗到细，各个负责人根据初步的容量换算指标，细化成系统的容量指标。再度量单服务器的峰值QPS，并进行扩容采购。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对目前系统的单机容量进行度量，再将目标QPS和单机容量进行除法操作，换算成需要增加的机器数，一般网络工程师、数据库工程师会对容量进行评估。线上扩容之后，通过容量验证来发现影响系统水平伸缩能力的瓶颈。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在压力测试过程中，要抽出部分时间进行监控点的梳理，特别是大促的监控大盘，通过一个大盘，可以看到以终为始的关键指标的情况。大盘一般分为业务大盘和技术大盘，技术大盘主要关注关键系统资源的使用情况，关注关键链路的异常数量是否有增加，一旦异常数量有增加或者关键系统资源开始超负荷（CPU、Load、Memory 等），那么要从备用机器池中进行紧急扩容，或者启动应急预案，让异常数量降下去。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>最后，针对容量验证的结果，对系统的水平伸缩能力进行升级。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>有些问题是代码层面的、架构层面的，有些是设备容量相关的，需要进行设备升级。如果机柜的交换机带宽过小，那么需要升级设备，如果4层负载均衡采用F5硬负载部署，并且F5带宽和处理能力达到上限，那么也需要升级设备。升级完成之后，需要重新对容量进行验证。</p>
</blockquote>
</blockquote>
<h3 id="◆-11-2-大促保障体系详解" tabindex="-1"><a class="header-anchor" href="#◆-11-2-大促保障体系详解" aria-hidden="true">#</a> ◆ 11.2 大促保障体系详解</h3>
<blockquote>
<blockquote>
<p>大促保障体系详解</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>容量保障体系</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>1.业务指标</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>为了做大促，业务上要提供目标，例如销量、用户数，还有转化率。要定一个目标，同时要给出每个细分的数据，例如上聚划算，聚划算会引流到哪个页面，大概预计多长时间内达到目标，什么时间点发购物券，预计多长时间内被领完，领完大概多长时间内消费完，都需要预估。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>业务指标要有以下3点信息，以便做容量评估和风险保障，同时能够识别重点保障链路。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>1）总目标量</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>总业务目标主要做总体目标换算使用，在电子商务类型的大促下，通常业务总目标量的指标是GMV（交易额）、订单数，过程指标包含转化率、客单价、用户数、流量，以终为始的总体目标可以逐步换算到上游系统，例如订单数的设定目标是1000万单，换算成交易下单 QPS=1000w/（24×3600）×峰值权重（一般是二八原则），再往上游推，可以推算出从宝贝详情页面直接下单的QPS，购物车下单的QPS，所以总目标量可以做关键链路的QPS换算使用。仅有总目标量还不够，大促容量的整体工作需要分解，通过分解再对第一轮评估做校核，容量的评估一般需要由业务的细分玩法决定。例如业务的玩法是在某个集中的时间点进行优惠券的发放，那么在这个时间点的容量评估和按照以终为始的指标进行换算的评估要有所差异。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>2）细分目标</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在进行大促合作分工的过程中，通常每个团队会将总体目标拿回去各自进行评估，然后汇总给大促项目组和机器采购人员。在各自分工的领域，需要再进行细分，确定细分到哪个端（移动端、PC端），哪个渠道（聚划算渠道、外部引流渠道），哪些产品是重点销售的，哪些产品是爆品，目的是更加清楚地了解各个渠道不同的容量需求。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>3）运营玩法</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>业务玩法对容量评估起非常关键的作用。业务玩法是运营的方式，通常有预热型、秒杀型、全天营销型、分批营销型。预热型表示容量峰值可能需求较低，它提前将容量需求进行释放。在618期间，用户通常喜欢提前将计划购买的商品放入购物车，购物车的容量在预热期间提前释放；秒杀型通常是针对爆品而言的，秒杀型的商品通常库存和价格都非常低，用户行为会在一瞬间释放出极高的容量需求；全天营销型可以根据历史的峰值进行换算；分批营销型一般将优惠券分批发送，商家和平台在大促期间根据业务目标是否达到决定是否通过发优惠券来刺激消费。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>2.系统容量评估和换算</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>由于大促的营销活动很多都是新业务，因此主要按照二八原则进行容量评估，不同特点的业务，峰值的倍数会有所不同，如秒杀型的玩法，需要根据预计时间内完成的销售情况进行调整，同时需要参考历史的情况。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>容量评估需要评估全链路系统的容量，包括应用、服务、存储（DB）、中间件、CDN、网络、交换机、路由器、负载均衡设备等。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>CDN评估可以先根据链路的容量情况，再根据链路平均有多少图片访问，一个页面访问的QPS对应多少张图片或者静态资源的访问来进行。空间上，可以根据平均的图片大小计算出图片总体占用的空间，并将这个数据提交给CDN提供商。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>3.单机压力测试</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>单机压力测试主要通过测试单机的极限吞吐量和容量，以及现有的机器情况，为采购提供扩容的依据。单机压力测试通常有下面两种方式。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 线上引流压力测试：将线上流量通过4层或者7层负载均衡设备调整机器的权重导入一台机器，直到机器的容量达到上限。要特别注意SOA服务的压力测试，引流时需要注意时刻观察服务的情况，因为服务通常被很多业务调用，例如交易、商品、会员服务，不能让这些公共服务将单机的容量压到极限，一旦过载，会造成大故障。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 压力测试脚本干预压力测试：通常在大促的新页面或者有新需求时，由于业务并没有对外开放，需要人造流量，将流量导入线上，通过干预的方式，设定一定量的并发，通过混合场景压力测试度量出线上单机的容量。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>4.机器预算</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>根据单机压力测试结果可以评估出机器预算的汇总中间件、数据库、网络等，同时要评估CDN的容量。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>机器采购预算如表11-1所示。表11-1[插图]</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>5.容量验证</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>容量验证是逐步的过程，扩容完成之后，各个应用负责人会单独组织应用集群的压力测试，验证集群是否能满足大促的需求，后面要整体做全链路压力测试。全链路压力测试是完全模拟大促当天的情况，让系统的容量全部达到极限，验证整个机房的容量上限，此时中间件、网络、公共服务的容量达到极限，由于资源之间存在互相依赖的关系，应用集群的压力测试只能验证部分容量，有些公共服务、设备的容量瓶颈并没有被探测出来，只有全链路的模拟才能确保公共服务的容量能够满足真实的需求。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对于同步调用，大型网站一般采取的容量验证方式有以下两种。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 机房内部压力测试：施压机器在机房内部，直接压力测试集群的VIP，通过VIP入口压力测试到对应的集群，机房内部压力测试适合于单机房压力测试，双机房压力测试通常会造成流量不均</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>压力测试机器的成本还是比较高的，一般一台物理机器的压力测试需要300个并发。施压时会出现各种问题，例如文件描述符的限制问题、施压机将机柜网络打满影响压力测试效果的问题、在多活机房压力测试时DNS不均衡的问题。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● CDN 服务器压力测试：全链路的压力测试需要极高的容量，对于机房内部压力测试有很多的干扰和问题，比如针对双机房的压力测试，由于DNS的缓存问题，会造成严重的流量不均，而CDN压力测试通过众多的CDN节点，DNS的解析结果也比较均衡，它主要模拟用户分散的特性，让流量场景更加接近用户的真实访问。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对于异步调用，要验证任务的处理能力，通常采用蓄洪压力测试的方式进行压力测试。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 蓄洪压力测试：针对异步化的流程进行压力测试，主要针对异步消息和消费能力进行验证，同时针对异步化之后的任务表扫描、处理进行验证。将消息堆积到一定的场景下，要造异步化消息的流量，并且打标，积压到一定阶段，来看消费方的消费、处理情况是否达到预期。在实战过程中，很多业务场景对异步处理时间有一定要求，例如保险出单要求在当天完成，而业务可能要求保单的生效在第二天完成，此时任务处理事件如果比较慢，会导致业务问题。另外在保险场景下，如果保单24小时没有生成，机构可能会拒绝出单，造成用户投诉等问题。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>全链路压力测试是全方位、真实模拟测试集群瓶颈和系统水平扩展能力的常见方式。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>从读链路到写链路，从模拟写到隔离压力测试写链路的过程，现在对于大多数场景来说，读远大于写，这和交易相关，交易系统的业务特色是用户先观看，再到决策的过程。所以写链路的瓶颈随着交易量变大而改变，压力测试通常也是从读链路到写链路过渡，随着网站规模的变大而做相应的策略调整。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>链路的梳理是关注点分离、分工进行梳理的过程，应用工程师可以从应用的视角进行链路的梳理，而网络工程师的视角是从网络入口到机柜的服务器，CDN工程师的视角是从Edge Server到回源站的链路，收集图片、JS、CSS等静态资源的访问量，反馈给CDN服务提供商，并且要求提供商参与大促的准备。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>运维保障是指从机房入口到机柜的机器的全链路保障，流量从入口到核心路由器，到核心交换机，再到机柜，运维主要负责网络设备、机柜的更新和安全保障工作。在大促期间为了防止恶意攻击，要做很多安全上的保障，常见的包括3层攻击和7层攻击防护，7层攻击防护主要是应用服务器（如Nginx）的防护策略。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>1.机房容量保障</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>从机房的维度来说，电量大小和所能容纳服务器的数量、机柜的数量都是有限的，在大促准备的时候，需要考虑机房是否能够扩容，如果机房容量不够，需要考虑同城机房。同城机房的建造需要花大量时间，包括机器和各种设备的采购、机房的选址、架构的部署和搭建、业务测试和回归测试、灰度验证等。机房级别的改造和升级，需要提前至少半年到一年。同城多活的机房架构，一般采用双机房热备的方式，机房间可以互为备份，流量比例可以任意切换。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>2.网络容量保障</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在初期，为了节约成本，设备通常会采用千兆网络，核心交换机和机柜的上联交换机都是千兆设备。当业务发展比较快的时候，网络容量非常容易出现瓶颈，特别是图片回源服务器是非常占用带宽的，而且在网络部署规划上，由于初期业务量小，会简单地将大流量的应用和小流量的应用进行混合机柜部署，这些应用会大量占用带宽，小流量应用是和大流量的带宽进行共享的，从而导致非常容易出现网络瓶颈。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>针对网络保障，总结如下。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>（1）梳理网络部署：提前对网络的部署进行梳理，重点对关键链路的机柜、服务器带宽进行梳理，为后续的升级提供最基本的素材。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>（2）隔离部署优化：针对大小流量系统进行隔离部署，关键链路和非关键链路隔离部署，关键链路之间也需要做适当的隔离。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>（3）升级网络带宽：在预算允许的情况下进行带宽的升级，如果预算有限，可以考虑将关键链路所在的机柜带宽进行升级。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>中间件保障</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>中间件包括RPC中间件、缓存中间件、消息中间件、分库分表中间件、存储解决方案和同步中间件。中间件通常需要做到较高的伸缩性，只要增加机器就可以解决容量的问题，但是中间件和存储同样存在容量上限。</p>
</blockquote>
</blockquote>
<h3 id="◆-11-3-大促容量峰值保障策略" tabindex="-1"><a class="header-anchor" href="#◆-11-3-大促容量峰值保障策略" aria-hidden="true">#</a> ◆ 11.3 大促容量峰值保障策略</h3>
<blockquote>
<blockquote>
<p>11.3 大促容量峰值保障策略</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在大促过程中，系统需要4大能力，这4大能力是峰值保障的基本点</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>[插图]</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 容量评估能力：比较精确的容量评估能力是非常关键的，如果能够预估每个关键链路和关键业务的容量需求，那么可以根据预估的容量通过扩容来进行容量的线性增加。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 容量验证能力：机器扩容之后，需要对集群进行吞吐能力验证。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 水平伸缩能力：水平伸缩指的是通过线性增加机器来提升系统的吞吐能力，大促时瞬间流量大，对系统的峰值处理能力有很高的要求。大促保障很重要的一个工作是保障峰值的处理能力，确保系统能够轻松应对瞬间的峰值。从单个机房来说，要从流量流向的链路逐层分析，从2层、3层的路由器、交换机、机柜、服务器，4层的软负载均衡LVS （F5），再到7层的应用层，都需要具备水平伸缩能力。从机房级别来说，架构和部署上要既支持同城双机房，又支持异地多活多机房，这些架构是水平伸缩能力的关键。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 应急预案能力：在发生问题时，能够通过提前准备好的应急预案解决问题。一般是通过梳理和容量验证之后，发现可疑的风险点，经过预案的开发沉淀出应急预案。预案分为常规预案和应急预案</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>围绕这4大能力，大促峰值保障策略总结如下。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 做好容量规划，清晰掌握容量需求：通过业务目标到系统容量的换算，计算出大促的容量需求，并提前进行采购扩容，同时为风险梳理提供数据参考。在风险梳理时，可以对现有的峰值能力和目标峰值能力进行比较，能力差距大的峰值可以列为风险。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 度量每个关键链路的现有峰值能力：对每个关键链路的现有容量通过压力测试进行度量，清晰地了解现有的峰值处理能力，便于及时改进和升级。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 梳理每个容量风险：根据容量规划计算出明显的容量瓶颈，包括设备的网络带宽和存储空间，特别需要重点关注单点、单数据库的风险。笔者多次经历过单数据库引起的容量瓶颈造成短时间不可用，针对单点容量瓶颈需要提前进行规划和改进，不要等到容量验证后才开始改造。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 对公共依赖进行上报：对公共依赖的容量需求进行报备，以免被限流。大促期间公共服务如交易、店铺、会员，还有中间件的容量需求要上报，各个服务的提供方会根据这些容量进行保障，如果没有报备，可能会进入限流池。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 提前做水平伸缩能力的改造：一般重大的架构改造需要很长时间，水平伸缩能力需要提前建设，识别到容量风险就立即行动，特别是关键链路，越早做改造，大促保障工作就越轻松。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 通过压力测试进行容量的验证：通过多轮、分层的压力测试来检验集群的峰值吞吐能力是否符合大促的容量需求，同时对不满足需求的系统进行整改和升级。压力测试工作应该分层次进行，提早进行单集群压力测试、单场景压力测试，再进行混合场景压力测试，最后进行全链路压力测试。经过分层压力测试，可以提前发现系统的伸缩性能力问题。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 做好链路的分析和监控，快速发现瓶颈：监控是发现问题和分析问题的基础，一个好的监控系统不仅可以更快地发现问题，也能更好地分析问题。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 对关键链路做好预案：对容量的风险做各种预案，包括技术预案、业务预案，提前做好详尽的预案，并且一定要对预案进行演练。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 流量控制兜底应对突发流量：流量控制是峰值保障最通用的方案，确保系统不会因为突发流量崩溃。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 备用机器，应对容量预估误差：容量评估和度量可能存在各种误差，可以通过预留一部分机器（10%）来解决问题，它不像流量控制会对业务有影响。</p>
</blockquote>
</blockquote>
<h3 id="◆-11-4-大促风险保障策略" tabindex="-1"><a class="header-anchor" href="#◆-11-4-大促风险保障策略" aria-hidden="true">#</a> ◆ 11.4 大促风险保障策略</h3>
<blockquote>
<blockquote>
<p>大促风险保障策略</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>大促保障是保3样东西，保峰值、保关键业务、保关键链路</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>以前做过的大促和最近梳理风险过程中遇到的问题，再加上依赖故障假设原则，风险分类总结如下。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>1.单点、单链路风险</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>单点和单链路风险等级最高，特别是单点的QPS不太大时，风险更大。通常单点稳定性提升，需要使用多冗余策略，同时与分布式风险分散策略搭配使用，通过自动冗余路由，在单链路出现问题时，通过另外一个链路自动启用或者应急启用来防止单点不可用问题。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>常见的单点和单链路风险如下。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● DB单库风险</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>○ DB单库风险主要来自容量不足造成的连接池满，一旦在大促期间出现问题，由于流量持续处在高位，会出现较大的故障。○ DB单库风险还来自单库服务器出现问题时，风险无法分散导致全部崩溃。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 单机房风险○ 单机房风险通常出现在一些特殊情况下，例如单机房的某个硬件设备坏死，包括交换机老化、路由器老化、网络线路坏死、机房特殊断电、机房电量供应不足等，都会出现单机房整体故障，此时可以通过双机房冗余策略来提升稳定性。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 热点链路风险○ 通常一个分布式系统具有一定的抗不可用能力，但是当出现热点时，热点会被分配到一个单点上，此时就会造成故障。针对热点的策略，或者通过近端策略来减少热点的压力，或者通过业务上缓存架构的重新设计来减少热点的问题。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>2.旁路风险</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>旁路风险指的是非关键业务对关键业务的影响，因为非关键业务和关键业务可能会共用资源，而非关键业务直接影响关键链路。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>3.容量风险</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>业务需要的容量和实际容量存在的差距，叫作容量风险。容量风险一部分通过梳理获得，一部分通过全链路压力测试进行检验。预测容量风险需要获取每一个依赖的容量上限。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 网络设备的容量上限</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>○ 万兆网卡打满，可导致整个大促容量无法支撑。○ 源图片服务器和应用混在一起，由于图片服务器占用了大量的交换机带宽资源，导致在同一机柜中的机器受到影响。○ F5最大带宽上限为10GB。○ 安全攻击防护设备存在明显带宽上限（例如80GB）。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 连接上限○ Oracle的连接数上限为3000，设定后如果要修改，需要重启服务器，单库的QPS上限为2000～4000。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 存储上限○ 单表数据量上限为2000万～4000万。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>○ 搜索引擎存储上限与服务器的行列数相关，行数代表QPS，行数越多QPS越高，列数表示索引的数量。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 公共服务上限○ 618期间，一些公共服务都会有限流，没有报备的服务非常可能在限流之列。○ Tair单机的QPS大约在10万～20万，热点的QPS不超过5万。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>4.雪崩风险随着访问量的增加和持续，一个单点故障会加重资源的消耗，导致所有的请求都受到影响。笔者曾经遇到一个场景，由于Dubbo有自动的三次重试，当系统出现问题时会不断地重试，从应用端进行压力测试只有200个并发，但是服务提供方却有600个并发，导致服务完全不可用。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>另外一个风险和超时时间设置相关，超时时间太长，会造成线程资源不能快速被释放，从而导致所有的连接都受到影响。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>5.非相关共享风险</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>共享有很多含义，其实任何链路之间都存在某种共享，例如一个应用中所有的CPU资源和内存资源都是共享的，一个机柜内所有的网络带宽都是共享的。共享风险不是指这些风险，而是指完全不同的关键链路由于共享某些资源，让看似不相关的资源产生风险。最典型的是HSF连接池共享风险和数据库连接池风险。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>6.级联冗余风险</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>级联冗余风险通常指的是，在双链路情况下，由于主链路出现问题，另外一个链路无法承受大流量冲击，从而造成链路不可用。通常发生在“集中式缓存（如 Tair）+DB”的经典双链路搭配时，Tair由于容量远大于DB，当Tair被击穿或者Tair稍微抖动时部分击穿，DB承受不住流量冲击，导致链路不可用，DB 此时变成强依赖，从而导致级联冗余风险。级联冗余风险通常的应对策略是“熔断非优先调用+优先保障”，在流量非常大时，打到DB实际上是不可行的，因此对于多个链路，要保障DB给那些更加需要的地方、业务优先级更高的地方。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>7.强弱倒置风险</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>例如购物时有个获取卖家黑名单的逻辑，由于开发人员没有正确处理异常，在卖家黑名单服务出现问题时，导致整个购物车页面不可用。这种强弱倒置的风险，在日常保障和大促保障时都需要关注，在某次平台风险梳理的过程中，笔者发现当调用类目的服务出现问题时，整个搜索页面无法打开，从而导致用户购买链路受到影响，所以针对强弱倒置风险，进行了风险应对。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>风险保障策略</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>1.全链路压力测试策略</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>全链路压力测试是发现问题最直接的手段，通过链路的全覆盖，能够发现大部分的中间件、应用、服务、软件、硬件的容量瓶颈，风险也更加一目了然。全链路压力测试是以每个系统业务需要的峰值吞吐量为基本目标的，没达到目标则需要改进。全链路压力测试最大的问题是链路的覆盖是否完全，要真正做到全链路，需要对链路上所有业务的流量进行盘点和梳理，这是非常浩大的工程。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>测</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>2.减少直接依赖策略</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>常见的稳定性保障策略都是减少直接依赖的策略。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>1）强依赖变弱依赖策略</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>2）近端策略</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>3）异步化策略</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>4）熔断降级策略</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>5）多冗余策略</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>6）风险分散策略</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>7）限流过载保护策略</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>11.4.5 分组隔离策略</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>通常分组隔离包含以下两种隔离方法。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 客户端隔离：主要避免性能差的服务影响性能好的服务，以及避免互相影响。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 服务端隔离：服务端隔离一般采用分组的方式，对服务部署进行分组，分别提供给重要的业务和非关键业务，这样也起到了一定的隔离作用。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>业务降级策略业务降级是常用的大促保障策略，在梳理链路时，如果发现某个链路有风险，对用户体验影响比较小，可以和业务沟通，在大促期间对此业务不进行服务的提供。例如在执行保障策略时，将碎屏险的激活验机流程在618期间关闭，用户当天只是进行购买行为，激活放在618峰值之后，以便在618期间保障购买流程。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>监控发现策略监控的目的是快速发现和定位问题。由于大促期间，很多新的业务需求会上线，针对新的业务，日常监控往往覆盖不足，同时很多应急策略和方案依赖于监控。针对监控，首先还是要梳理重要的监控项，从业务监控大盘到应用监控大盘，再到系统监控大盘，应急方案是根据监控的结果来执行的，通常监控发现策略不仅要将关键的监控加上，必要时还要将数据的抓取放在监控发现策略里。</p>
</blockquote>
</blockquote>
<h3 id="◆-11-5-大促资金安全保障策略" tabindex="-1"><a class="header-anchor" href="#◆-11-5-大促资金安全保障策略" aria-hidden="true">#</a> ◆ 11.5 大促资金安全保障策略</h3>
<blockquote>
<blockquote>
<p>11.5.1 常见的资金安全防护策略</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>通常资金安全问题容易发生在并发控制、幂等控制出现问题的时候，其后果是客户少付或多付。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>笔者曾经历的一次资损，本来产品的售价是与份数相关的，开发人员理解成价格不与份数相关，用户买10份的价格和1份的价格是相同的。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>并发控制是资金安全问题无法完全避免的根本原因，只要是人设计的系统，都会有资损的风险，特别是涉及非常复杂的系统之间的交互时。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在实战过程中，上游系统和下游系统，涉及金额单位的问题，上游系统默认的单位是元，下游是资金处理系统，默认的单位是分，此时会有一百倍资损问题发生。又例如，在代扣场景下，一般会将代扣的任务记录在任务表里，在代扣日到来之前进行资金代扣的处理。代扣任务的生成，同样要符合幂等原则，即确保每月的代扣任务记录只有一条，这一条记录通常通过幂等字段进行控制，在生成任务记录时，一般会有业务单据号和类型两个字段，这两个字段作为联合业务主键，确保代扣只处理一次。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>常见的资金安全防护分为事前、事中、事后3个阶段，实行立体式防护</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>大促资金安全防护大促期间有不少新增的链路，这些链路的资金安全防护要特别注意，同时由于访问量会在瞬间产生，大量的资金处理在一瞬间开始爆发，要能够很快发现资金安全问题，这对核对的实时性有很高的要求。（1）有新增链路：大促期间有不少需求是新增的，所以要考虑新增链路原有的防护策略是否能覆盖，包括熔断止血、核对覆盖及幂等和并发控制的设计均需要进行全面的梳理和备案。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>（2）实时性要求高：对资损发现的问题有实时性的要求，因为大促涉及资金量大，同时考虑到有新增链路，所以对核对能力有很高的要求。</p>
</blockquote>
</blockquote>
<h3 id="◆-12-1-webp性能优化案例背景" tabindex="-1"><a class="header-anchor" href="#◆-12-1-webp性能优化案例背景" aria-hidden="true">#</a> ◆ 12.1 WebP性能优化案例背景</h3>
<blockquote>
<blockquote>
<p>WebP性能优化案例背景</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>[插图]</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>Google的WebP推出了新的图片压缩算法WebP，很多网站都开始使用WebP作为图片的压缩格式。这种压缩算法压缩出来的图片体积更小，但是解压缩时需要占用更多的CPU资源，是典型的以时间换空间的算法。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>（1）有一张图片A，用JPG压缩之后的体积是90KB，用WebP压缩之后的体积是60KB，使用WebP之后，体积缩小了1/3。（2）当时的网络RTT是200ms，不要疑惑，确实是200ms，因为这个RTT是跨了大洋的RTT。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>（3）当时的拥塞窗口初始值是4，那么第一次请求可以携带的数据量是1460×4=5840B。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>那么这1/3体积的缩小能带来什么助益呢？假设当时的网络情况是理论最优的，没有任何抖动，拥塞窗口的大小为32，用当时的场景来进行推导</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>通过这样的表格对比，发现60KB的WebP图片在这个场景下只需要4次网络传输，加上链接时间，总耗时是1000ms，而90KB的图片在这个场景下则需要5次网络传输，加上链接时间，总耗时是1200ms。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>假设拥塞窗口的平均大小是16，再来推导一遍</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在拥塞窗口小的情况下，90KB的JPG图片比60KB的WebP图片多耗时400ms。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>WebP改造使L-D转化率下降</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>通过一番改造之后，WebP上线了，但遗憾的是Apache Bench test中WebP那部分的L-D转化率居然下降了</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>直到几个月后的一天，笔者无意中发现WebP的TTFB居然高达847ms，而JPG的TTFB只有51ms</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>这是什么概念呢？CDN边缘节点的缓存服务器接收请求之后，加载本地磁盘中的WebP图片，然后返回图片数据，第一个返回的字节到达客户端的时间为847ms，按照计算机当前的发展水平，这是不符合常理的，可以解释的原因只有一个，即那台服务器上并没有这张WebP图片。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>笔者开始不断地请求这张图片，清了浏览器缓存之后再请求，不管怎么请求，TTFB 始终超过800ms，这说明并不是恰巧请求到正好没有WebP的某台机器上，于是请Akamai的接口人确认一下，结果对方的回复是确实没有为WebP这种图片类型进行过缓存</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>如果通过数据分析的方式来进行问题诊断，那么可以立即找到问题所在并将其解决。接下来将阐述如何根据数据分析来诊断这个问题。</p>
</blockquote>
</blockquote>
<h3 id="◆-12-2-性能优化中的数据分析原理与方法" tabindex="-1"><a class="header-anchor" href="#◆-12-2-性能优化中的数据分析原理与方法" aria-hidden="true">#</a> ◆ 12.2 性能优化中的数据分析原理与方法</h3>
<blockquote>
<blockquote>
<p>12.2 性能优化中的数据分析原理与方法</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>数据分析简单来说就是使用统计学的相关方法对所收集数据进行分析，以寻找数据中的规律，最后形成结论。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>[插图]</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>数据分析之杜邦分析</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>数据分析中常见的分析技巧有对比分析、分组分析、结构分析、平均分析、交叉分析、综合评价分析、杜邦分析、矩阵关联分析等</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>[插图]</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>我们先来简单地了解一下什么是杜邦分析，如图12-5所示。[插图]</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在杜邦分析中，有如下几个术语。● 指标：衡量目标的单位。● 数据表：每个指标都来自对数据的统计，所以指标定义完成后必须获得指标所需要的数据表。● 子指标：指标一般都由子指标构成，当一个指标无法再分解时就没有子指标了。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 父子指标函数关系：父指标和子指标都存在显示或隐式的函数关系，比如上例中显示的函数关系有如下两种。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>○ 权益净利率=资产净利率×权益系数</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>○ 净利=销售收入-全部成本+其他利润-所得税</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>这样一个由指标、子指标及父子指标函数关系组成的树形结构，称为指标树。指标树有什么作用呢？</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>1）便于绩效管理明确业务指标及指标分解，便于团队作战（这是管理问题，不是数据分析问题）。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>2）缩小问题范围（1）指标向下追溯：当父指标发生波动时，可以自动向下追溯，以查看父指标的波动是由哪个子指标引发的。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>（2）指标拆解：可以对任何一个波动的指标进行多维分析，以缩小问题范围。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>杜邦分析是一个典型的用在财务上的分析，但是它也可以用在性能领域</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>杜邦分析能够帮助我们找到影响性能指标的子指标，在精准定位到是哪个指标的问题之后，我们还需要进行另外一种分析，即多维分析。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>12.2.3 数据分析之多维分析</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>传统的多维分析将观察事物的角度分成了多个维度</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>这里简单地介绍一个维度和度量的概念。● 维度：观察数据的角度，比如在业务指标上，观察“最近7天交易金额”这个指标，可以从多个维度观察，比如人群标签维度、地区维度、商品品类维度等。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 度量：度量某个具体的数据值，比如对最近7天交易金额进行求和操作就是一个度量。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>select sum（最近7天销售收入）from table_aa group by省份，产品类型在这条语句中，省份和产品类型就是维度，sum（销售收入）就是度量。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>这里的度量好像跟指标有些相似，它们的区别是：指标是面向业务的，度量是面向数据的。指标一般都是由度量和维度组成的，比如“华东区最近7日交易额”这个指标里面有地区维度，也有交易额汇总的度量。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>当销售额这个指标发生变化时，需要对指标进行拆解，而拆解的方法就是查看是哪个省份的哪个产品类型的销售额在时间维度上发生了什么样的变化：select sum（销售收入），产品类型，日期from table_aa group by产品类型日期查完之后，将数据通过可视化的效果展示出来</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>得到了这份数据，可以知道大概是4号出了问题，但是不知道是哪个城市出了问题，于是加上城市维度</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>接下来通过一些可视化工具来对其进行可视化展现，以便更快地发现问题</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>发现只有少数城市4号销售额没有下跌，大部分城市的销售额都下跌了。这好像也是说得通的，因为5月4号大家开始上班了，没有时间购物了。但是常州在4号的销售额没有下跌，这是为什么呢？这要根据品类进行拆解，看看是不是有什么异常，比如其他品类的销售额下跌了，但是有一个品类的销售额上升了，我们就要详细调查为什么这个品类的销售额突然上升。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>这就是通过多维分析的方式来缩小问题的范围的方法，它本质上适用于各行各业</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>以上简要地讲述了数据分析的相关概念和简单方法，总结如下。（1）定义指标：如杜邦分析案例中的财务指标，会有很多个。（2）确定指标之间的关联关系：指标有父子关系，父指标受到多个子指标的影响。（3）采集数据：这里要进行埋点、采集，确保数据质量。（4）维度建模：为每个指标建立维度模型，同时加工数据，映射成这个维度模型。（5）指标追溯：通过每个指标的维度模型中的数据计算确定发生问题的子指标。（6）指标多维分析：对子指标进行多维分析，找到发生问题的维度，缩小问题的范围，进而确定问题所在。</p>
</blockquote>
</blockquote>
<h3 id="◆-12-3-通过数据分析来诊断webp的性能问题" tabindex="-1"><a class="header-anchor" href="#◆-12-3-通过数据分析来诊断webp的性能问题" aria-hidden="true">#</a> ◆ 12.3 通过数据分析来诊断WebP的性能问题</h3>
<blockquote>
<blockquote>
<p>12.3 通过数据分析来诊断WebP的性能问题</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在理解了杜邦分析之后，下面来数一数电商中经常出现的类似的顶层指标有哪些。（1）GMV：表示网站的交易额。（2）L-D转化率：表示用户从List页面跳转到Detail页面的比例。（3）D-O转化率：表示用户从Detail页面跳到下单成功页面的比例。（4）客单价：每个客人的订单平均金额。（5）网站UV：表示特定时间段内有多少消费者来到该网站。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>一个网站的GMV路径很多，典型的有如图12-11所示的3条。[插图]</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>为了简化问题，只讨论主搜索这个转化漏斗，各指标之间的关系如下：GMV=网站UV ×L-D转化率×D-O转化率×客单价</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>这个函数关系是显而易见的，但是公式中的每个因子又是由哪些子因子组成的呢？这里以L-D转化率来重点阐述一下。是什么导致一个用户有兴趣从List页面跳转到Detail页面进行商品详情的查看呢？可能有下面这几种因素。（1）搜索正确率：用户搜一个iPhone 7，结果出来一堆iPhone 7的手机壳，这可能就不是用户想要的内容，用户自然就不会进入Detail页面。（2）价格：一个用户的消费能力是有级别的，对方想买一个50～100元的手机壳，但是系统给他推荐了10元的，自然他进Detail页面的可能性也降低了。（3）网页打开性能：面对一个需要5秒打开的搜索页面和一个需要3秒打开的页面，用户进入Detail页面的概率也是大大不同的。这里页面加载性能又分成如下两块。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>① HTML页面的下载和渲染速度。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>② 图片的性能。如果 List 页面的图片打开速度特别快，且图片精美，那么也能影响 L-D转化率。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>（4）营销活动：如果在List页面看到某些商品有优惠，那么用单击Detail页面的概率就增加了。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>现在问题来了，之前在GMV和子指标之间有明确的函数关系，但是L-D转化率指标和其下的子指标存在什么函数关系呢？假设：L-D转化率=F（搜索正确性指标，价格指标，性能指标，营销活动指标）</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>我们知道，营销活动如何设定跟平台和卖家都有很大的关系，如果一个卖家出血本做营销活动，L-D转化率一般都升高得比较明显，但是如果营销的优惠幅度下降，那么L-D转化率也会跟着下降。在这个场景中，平台和卖家投入的资金会非常影响L-D。这是人为因素，这一人为因素将直接决定L-D转化率指标。只有人为因素产生了规律性，这个F函数才可能稳定下来。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>基于指标树自动诊断WebP的性能问题</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>这里不观察子指标和父指标的函数关系，而是来观察当父指标波动时，机器是否可以快速诊断出是哪个子指标出现了问题。所以有必要搞清楚，父指标由哪些子指标构成。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>接下来分析一下图片性能指标的子指标有哪些。（1）图片大小。（2）图片质量。（3）用户下载图片的RTT。（4）边缘节点的缓存命中率。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>根据上述分析，就可以构建指标树了，如图12-12所示。这棵指标树是一个局部的展现，当GMV下降时：（1）通过程序自动向下追溯，可以得知L-D转化率下降导致GMV下降。（2）再往下追溯就会发现，图片性能下降导致了L-D转化率的下降。（3）再往下追溯又发现，原来是由于节点缓存命中率的下降导致了图片性能的下降。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>[插图]</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>通过指标树向下追溯缩小了问题的范围。当问题缩小到边缘节点缓存命中率导致L-D转化率下降时，可以开展多维分析，再次缩小问题的范围。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在边缘节点的缓存命中率上，需要搜集数据，这份数据应该如图12-13所示，其中RTT可以简化理解为创建链接的时间，这两个时间相对其他时间来说较为接近。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>[插图]</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>应该通过什么样的分析技巧来发现这份数据隐藏的问题呢？多维分析的思路如下。[插图]</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>分析结果如图12-14所示（这个分析结果的可视化对数据做了一些处理，以便更好地理解）。[插图]</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>通过这样的分析，应该马上就可以找出大小接近的图片，但是 WebP 图片的 TTFB高达800ms，远高于 JPG 图片。上述分析过程，可以通过程序自动完成，自然人要做的就是定义好指标树，以及准备好对应的数据，其他分析都可以通过机器来完成。机器每天发诊断报告，供技术、运营、产品人员参考。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>当然，在实际生产环境中，可能还有其他业务需求上线，影响整体的GMV，从而导致向下追溯的时候很难判断到底是什么因素影响了GMV。这个时候，可以从第二层级来看指标波动的情况。比如GMV没变，但是UV增加了，L-D下降了，这时候肯定有需求导致UV增加，也有新增的内容导致L-D下降，于是程序就从L-D往下追溯，不能认为GMV没变就没出问题。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>通过类似杜邦分析的指标树拆解及多维分析，甚至可以求得性能指标对整体GMV的影响，关键在于当指标树的某个层次无法人为求得F函数的时候，要通过机器学习的方式得到父子指标之间的关系。如此一来，整体的指标函数就会变得完整，甚至可以事前预估性能优化的效果，以判断性能优化对GMV的影响。</p>
</blockquote>
</blockquote>
<h3 id="◆-12-4-案例-通过数据分析进行olap分析和rt优化" tabindex="-1"><a class="header-anchor" href="#◆-12-4-案例-通过数据分析进行olap分析和rt优化" aria-hidden="true">#</a> ◆ 12.4 案例：通过数据分析进行OLAP分析和RT优化</h3>
<blockquote>
<blockquote>
<p>12.4 案例：通过数据分析进行OLAP分析和RT优化</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在线分析系统有如下两个特点。（1）SQL无规则：查询方式由用户在页面上拖曳的方式来决定，产生了五花八门的SQL。（2）SQL变化快：如果用户的分析行为发生变化或者产品发布新功能，查询的SQL就会发生变化。这种系统的并发量会比在线事务系统低很多，它面临的主要挑战是，在各种各样的分析需求下，如何保障系统的响应时间，不能让用户每次做分析时都等几十秒。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在线分析系统响应指标基线的定义</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>1.在线分析系统的响应时间过长问题</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>用户在查看报表或者进行数据分析的时候，每次都要等很长时间，有时候RT高达90s，有时候RT是1s。用户在做数据分析时不得不等待系统的返回，而这个时间片段又很难让用户转而去做其他事情，这对用户来说会导致工作效率下降。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>那么到底多长时间是一个比较合适的时间，这里有Akamai的一份调研数据。在2006年，一个来自Akamai的研究认为：通常4s左右的平均加载时间，可能是用户等待页面加载的最大时间。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>这里有一个计算模型，里面包含了很多因子，比如行业水准、终端、网络、地域、转化率等，统一建模后得出了一个综合的结果。如果分领域，不同领域的值应该是不一样的。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>2.指标基线定义的方法和原则1）当前水准的判断</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>（1）跨行业参考：即使互联网用户的等待容忍时间是4s（2006年），也不代表在OLAP领域的等待容忍时间也是4s。所以在这里定义的这个指标具有不确定性，因为无法确定是否可以达到这个目标。（2）本行业指标基线建模：行业水准、终端、网络、地域、转化率等一系列因素，可以精准地计算出何种基线对产品是最有帮助的。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>2）即使500ms 的查询时间是最优的，但是目前还达不到，所以指标基线的定义应该遵循以下原则：（1）基线定义得太高，遥不可及会打击团队的信心和士气。（2）基线定义得太低，毫不费力会让团队成长速度下降。（3）合理的基线设定原则是全力以赴后可以达到，比全力以赴更努力可以超越既定基线。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在项目启动后，第一时间要确定两件事。● 指标：查询在3s内的比例。● 目标值：95%。为了达到这个目标，接下来要对这个应用进行分析，简要诊断框架可以分成两个部分，一个是应用架构，另一个是系统流程。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>首先要做的就是分析应用的构成，该应用包含的模块如图12-15所示。[插图]图12-15● OLAP Server：接收用户请求，并决定将请求发送到哪里。● Cache：用来缓存用户查询过的数据。● MR:SQL on Hadoop，在HDFS上执行Map-Reduce任务来进行数据查询。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● queryengine:SQL on MPP是一个基于列存的分布式查询引擎，目前SQL onMPP还有一些功能上的缺失，遇到这部分请求时，还是需要路由回SQL onHadoop。</p>
</blockquote>
</blockquote>
</div></template>



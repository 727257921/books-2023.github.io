<template><div><h1 id="数据结构与算法图解" tabindex="-1"><a class="header-anchor" href="#数据结构与算法图解" aria-hidden="true">#</a> 数据结构与算法图解</h1>
<p>杰伊·温格罗</p>
<h2 id="第1章-基础数据结构" tabindex="-1"><a class="header-anchor" href="#第1章-基础数据结构" aria-hidden="true">#</a> 第1章 基础数据结构</h2>
<h3 id="◆-1-1-基础数据结构-数组" tabindex="-1"><a class="header-anchor" href="#◆-1-1-基础数据结构-数组" aria-hidden="true">#</a> ◆ 1.1 基础数据结构：数组</h3>
<p>一般数据结构都有以下4种操作（或者说用法）。
❏ 读取：查看数据结构中某一位置上的数据。对于数组来说，这意味着查看某个索引所指的数据值。例如，查看索引2上有什么食品，就是一种读取。
❏ 查找：从数据结构中找出某个数据值的所在。对于数组来说，这意味着检查其是否包含某个值，如果包含，那么还得给出其索引。例如，检查&quot;dates&quot;是否存在于食品清单之中，给出其对应的索引，就是一种查找。
❏ 插入：给数据结构增加一个数据值。对于数组来说，这意味着多加一个格子并填入一个值。例如，往购物清单中多加一项&quot;figs&quot;，就是一种插入。
❏ 删除：从数据结构中移走一个数据值。对于数组来说，这意味着把数组中的某个数据项移走。例如，把购物清单中的&quot;bananas&quot;移走，就是一种删除。</p>
<p>操作的速度，并不按时间计算，而是按步数计算。</p>
<p>操作的速度，也常被称为时间复杂度。在本书中，我们会提到速度、时间复杂度、效率、性能，但它们其实指的都是步数。</p>
<p>读取，即查看数组中某个索引所指的数据值。这只要一步就够了，因为计算机本身就有跳到任一索引位置的能力。</p>
<p>计算机的内存可以被看成一堆格子。下图是一片网格，其中有些格子有数据，有些则是空白。
<img src="/img/common/image-20210928100438327.png" alt="image-20210928100438327" style="zoom:50%;" /></p>
<p>当程序声明一个数组时，它会先划分出一些连续的空格子以备使用。换句话说，如果你想创建一个包含5个元素的数组，计算机就会找出5个排成一行的空格子，将其当成数组。</p>
<h5 id="_1-读取" tabindex="-1"><a class="header-anchor" href="#_1-读取" aria-hidden="true">#</a> （1）读取：</h5>
<p>计算机之所以在读取数组中某个索引所指的值时，能直接跳到那个位置上，是因为它具备以下条件。(1) 计算机可以一步就跳到任意一个内存地址上。（就好比，要是你知道大街123号在哪儿，那么就可以直奔过去。）(2) 数组本身会记有第一个格子的内存地址，因此，计算机知道这个数组的开头在哪里。(3) 数组的索引从0算起。
当我们叫计算机读取索引3的值时，它会做以下演算。(1) 该数组的索引从0算起，其开头的内存地址为1010。(2) 索引3在索引0后的第3个格子上。(3) 于是索引3的内存地址为1013，因为1010 + 3=1013。当计算机一步跳到1013时，我们就能获取到&quot;dates&quot;这个值了。
如果我们问的不是“索引3有什么值”，而是“&quot;dates&quot;在不在数组里”，那么这就需要进行查找操作了。
（2）查找：
对于数组来说，查找就是检查它是否包含某个值，如果包含，还得给出其索引。那么，我们就试试在数组中查找&quot;dates&quot;要用多少步。想要查找数组中是否存在某个值，计算机会先从索引0开始，检查其值，如果不匹配，则继续下一个索引，以此类推，直至找到为止。</p>
<p>这种逐个格子去检查的做法，就是最基本的查找方法——线性查找。
如果我们要找的值刚好在数组的最后一个格子里（如本例的elderberries），那么计算机从头到尾检查每个格子，会在最后才找到。同样，如果我们要找的值并不存在于数组中，那么计算机也还是得查遍每个格子，才能确定这个值不在数组中。</p>
<p>一个5格的数组，其线性查找的步数最大值是5，而对于一个500格的数组，则是500。以此类推，一个N格的数组，其线性查找的最多步数是N（N可以是任何自然数）。可见，无论是多长的数组，查找都比读取要慢，因为读取永远都只需要一步，而查找却可能需要多步。
（3）插入：</p>
<p>假设我们想要在购物清单的末尾插入&quot;figs&quot;。那么只需一步。因为之前说过了，计算机知道数组开头的内存地址，也知道数组包含多少个元素，所以可以算出要插入的内存地址，然后一步跳到那里插入就行了。但在数组开头或中间插入，就另当别论了。这种情况下，我们需要移动其他元素以腾出空间，于是得花费额外的步数。一个含有N个元素的数组，其插入数据的最坏情况会花费N + 1步。即插入在数组开头，导致N次移动，加上一次插入。
<img src="/img/common/image-20210928101330320.png" alt="image-20210928101330320" style="zoom:50%;" />
（4）删除：
删除的最坏情况就是删掉数组的第一个元素。因为数组不允许空元素，当索引0空出，那么剩下的所有元素都要往左移去填空。对于含有5个元素的数组，删除第一个元素需要1步，左移剩余的元素需要4步。而对于500个元素的数组，删除第一个元素需要1步，左移剩余的元素需要499步。可以推出，对于含有N个元素的数组，删除操作最多需要N步。</p>
<h3 id="◆-1-2-集合-一条规则决定性能" tabindex="-1"><a class="header-anchor" href="#◆-1-2-集合-一条规则决定性能" aria-hidden="true">#</a> ◆ 1.2 集合：一条规则决定性能</h3>
<p>集合。它是一种不允许元素重复的数据结构。</p>
<p>集合就是一个带有“不允许重复”这种简单限制的数组。而该限制也导致它在4种基本操作中有1种与数组性能不同。</p>
<p>集合的读取跟数组的读取完全一样，计算机只要一步就能获取指定索引上的值。</p>
<p>对于集合，计算机得先确定要插入的值不存在于其中——因为这就是集合：不允许重复值。于是每次插入都要先来一次1查找。</p>
<p>（1）在N个元素的集合中进行插入的最好情况需要N + 1步——N步去确认被插入的值不在集合中，加上最后插入的1步。</p>
<p>（2）最坏的情况则是在集合的开头插入，这时计算机得检查N个格子以保证集合不包含那个值，然后用N步来把所有值右移，最后再用1步来插入新值。总共2N + 1步。</p>
<p>这是否意味着因为它的插入比一般的数组慢，所以就不要用了呢？当然不是。在需要保证数据不重复的场景中，集合是非常重要的。但如果没有这种需求，那么选择插入比集合快的数组会更好一些。具体哪种数据结构更合适，当然要根据你的实际应用场景而定。</p>
<h3 id="◆-1-3-总结" tabindex="-1"><a class="header-anchor" href="#◆-1-3-总结" aria-hidden="true">#</a> ◆ 1.3 总结</h3>
<p>理解数据结构的性能，关键在于分析操作所需的步数。采取哪种数据结构将决定你的程序是能够承受住压力，还是崩溃。</p>
<h2 id="第2章-基础算法" tabindex="-1"><a class="header-anchor" href="#第2章-基础算法" aria-hidden="true">#</a> 第2章 基础算法</h2>
<h3 id="◆-2-1-有序数组" tabindex="-1"><a class="header-anchor" href="#◆-2-1-有序数组" aria-hidden="true">#</a> ◆ 2.1 有序数组</h3>
<img src="/img/common/image-20210928102312644.png" alt="image-20210928102312644" style="zoom:50%;" />
<p>假设要把75插入到有序数组中，就要做一个比较查找，先比较和3的大小，然后适是和17的大小，依次比较，最后找到合适的位置，然后把80和之后的数据，一个个的往后移，最后腾出来位置，放75。所以性能相对常规数组，多了一个查找确定位置的过程。
往有序数组中插入新值，需要先做一次查找以确定插入的位置。这是它跟常规数组的关键区别（在性能方面）之一。</p>
<p>虽然插入的性能比不上常规数组，但在查找方面，有序数组却有着特殊优势。</p>
<h3 id="◆-2-2-查找有序数组" tabindex="-1"><a class="header-anchor" href="#◆-2-2-查找有序数组" aria-hidden="true">#</a> ◆ 2.2 查找有序数组</h3>
<p>常规数组的查找方式：从左至右，逐个格子检查，直至找到。这种方式称为线性查找。</p>
<p>有序数组相比常规数组的一大优势就是它可以使用另一种查找算法。此种算法名为二分查找，它比线性查找要快得多。</p>
<h3 id="◆-2-3-二分查找" tabindex="-1"><a class="header-anchor" href="#◆-2-3-二分查找" aria-hidden="true">#</a> ◆ 2.3 二分查找</h3>
<p>有序数组相比常规数组的一大优势就是它除了可以用线性查找，还可以用二分查找。常规数组因为无序，所以不可能运用二分查找。
有序数组的线性查找大多数情况下都会快于常规数组。除非要找的值是最后那个，或者比最后的值还大，那就只能一直查到最后了。</p>
<h3 id="◆-2-4-二分查找与线性查找" tabindex="-1"><a class="header-anchor" href="#◆-2-4-二分查找与线性查找" aria-hidden="true">#</a> ◆ 2.4 二分查找与线性查找</h3>
<p>如果数组变得更大，比如说10000个元素，那么线性查找最多会有10000步，而二分查找最多只有14步。再增大到1000000个元素，则线性查找最多有1000000步，二分查找最多只有20步。</p>
<p>有序数组并不是所有操作都比常规数组要快。它的插入就相对要慢。衡量起来，虽然插入是慢了一些，但查找却快了许多。还是那句话，你得根据应用场景来判断哪种更合适。
在3个元素的数组上线性查找，最多要3步，7个元素就最多要7步，100个元素就最多要100步，即元素有多少，最多步数就是多少。数组长度翻倍，线性查找的最多步数就会翻倍，而二分查找则只是增加1步。
<img src="/img/common/image-20210928103002355.png" alt="image-20210928103002355" style="zoom:25%;" /></p>
<h3 id="◆-2-5-总结" tabindex="-1"><a class="header-anchor" href="#◆-2-5-总结" aria-hidden="true">#</a> ◆ 2.5 总结</h3>
<p>世界上并没有哪种适用于所有场景的数据结构或者算法。你不能因为有序数组能使用二分查找就永远只用有序数组。在经常插入而很少查找的情况下，显然插入迅速的常规数组会是更好的选择。</p>
<h2 id="第3章-大o记法" tabindex="-1"><a class="header-anchor" href="#第3章-大o记法" aria-hidden="true">#</a> 第3章 大O记法</h2>
<p>以线性查找为例，它的步数等于数组的元素数量。如果数组有22个元素，线性查找就需要22步；如果数组有400个元素，线性查找就需要400步。量化线性查找效率的更准确的方式应该是：对于具有N个元素的数组，线性查找最多需要N步。</p>
<h3 id="◆-3-1-大o-数步数" tabindex="-1"><a class="header-anchor" href="#◆-3-1-大o-数步数" aria-hidden="true">#</a> ◆ 3.1 大O：数步数</h3>
<p>数组不论多大，读取都只需1步。用大O记法来表示，就是：O(1)</p>
<p>O(1)意味着一种算法无论面对多大的数据量，其步数总是相同的。就像无论数组有多大，读取元素都只要1步。这1步在旧机器上也许要花20分钟，而用现代的硬件却只要1纳秒。但这两种情况下，读取数组都是1步。</p>
<p>也属于O(1)的操作还包括数组末尾的插入与删除。之前已证明，无论数组有多大，这两种操作都只需1步，所以它们的效率都是O(1)。</p>
<p>对于N个元素的数组，线性查找需要花N步。用大O记法来表示，即为：O(N)</p>
<h3 id="◆-3-2-常数时间与线性时间" tabindex="-1"><a class="header-anchor" href="#◆-3-2-常数时间与线性时间" aria-hidden="true">#</a> ◆ 3.2 常数时间与线性时间</h3>
<p>O(N)算法所需的步数等于数据量，意思是当数组增加一个元素时，O(N)算法就要增加1步。而O(1)算法无论面对多大的数组，其步数都不变。</p>
<p>O(N)也被称为线性时间。</p>
<p>O(1)也被称为常数时间</p>
<p>O(1)就是用来表示所有数据增长但步数不变的算法。</p>
<h3 id="◆-3-3-同一算法-不同场景" tabindex="-1"><a class="header-anchor" href="#◆-3-3-同一算法-不同场景" aria-hidden="true">#</a> ◆ 3.3 同一算法，不同场景</h3>
<p>虽然大 O可以用来表示给定算法的最好和最坏的情景，但若无特别说明，大 O记法一般都是指最坏情况。</p>
<h3 id="◆-3-4-第三种算法" tabindex="-1"><a class="header-anchor" href="#◆-3-4-第三种算法" aria-hidden="true">#</a> ◆ 3.4 第三种算法</h3>
<p>二分查找的时间复杂度介于O(1)和O(N)之间</p>
<p>二分查找的大O记法是：O(logN)</p>
<p>O log N”。归于此类的算法，它们的时间复杂度都叫作对数时间。</p>
<p>O(log N)意味着该算法当数据量翻倍时，步数加1。</p>
<h3 id="◆-3-6-解释o-log-n" tabindex="-1"><a class="header-anchor" href="#◆-3-6-解释o-log-n" aria-hidden="true">#</a> ◆ 3.6 解释O(log N)</h3>
<p>当我们说O(log N)时，其实指的是O(log2N)，不过为了方便就省略了2而已。</p>
<p>O(logN)算法的步数等于二分数据直至元素剩余1个的次数。
<img src="@source/docs/theme-reco/img/common/image-20210928104217655.png" alt="image-20210928104217655">
<img src="/img/common/image-20210928104323792.png" alt="image-20210928104323792" style="zoom:50%;" /></p>
<h2 id="第4章-运用大o来给代码提速" tabindex="-1"><a class="header-anchor" href="#第4章-运用大o来给代码提速" aria-hidden="true">#</a> 第4章 运用大O来给代码提速</h2>
<p>大O记法能客观地衡量各种算法的时间复杂度，是比较算法的利器。我们也试过用它来对比二分查找和线性查找的步数差异，发现二分查找的步数为O(log N)，比线性查找的O(N)快得多。
如果你通过大O发现自己的算法比其他的要慢，你就应该退一步，好好想想怎样优化它，才能使它变成更快的那种大O。虽然并不总有提升空间，但在确定编码之前多加考虑还是好的。</p>
<h3 id="◆-4-1-冒泡排序" tabindex="-1"><a class="header-anchor" href="#◆-4-1-冒泡排序" aria-hidden="true">#</a> ◆ 4.1 冒泡排序：</h3>
<p>​	如何将一个无序的数字数组整理成升序？</p>
<p>​	起初我们会学习一些“简单排序”，它们很好懂，但效率不如其他排序算法。冒泡排序是一种很基本的排序算法，步骤如下。</p>
<p>​	冒泡排序的执行步骤可分为两种。❏ 比较：比较两个数看哪个更大。❏ 交换：交换两个数的位置以使它们按顺序排列。</p>
<img src="/img/common/image-20210928104634258.png" alt="image-20210928104634258" style="zoom: 33%;" />
<img src="/img/common/image-20210928104809878.png" alt="image-20210928104809878" style="zoom: 33%;" />
<h3 id="◆-4-4-冒泡排序的效率" tabindex="-1"><a class="header-anchor" href="#◆-4-4-冒泡排序的效率" aria-hidden="true">#</a> ◆ 4.4 冒泡排序的效率</h3>
<p>假设5个元素的数组，第一轮对4个元素进行了4次比较，第二轮做了3次比较，第三轮2次，第4轮1次，所以最后就是4+3+2+1=10次比较，推广到n个元素就是：(N -1) + (N -2) + (N -3) + … + 1次比较。分析过比较之后，再来看看交换。如果数组不只是随机打乱，而是完全反序，在这种最坏的情况下，每次比较过后都得进行一次交换。因此10次比较加10次交换，总共20步。现在把两种步骤放在一起来看。一个含有10个元素的数组，需要：9 + 8 + 7 + 6 + 5 + 4 + 3 + 2 + 1=45次比较，以及45次交换，共90步。
<img src="/img/common/image-20210928105921447.png" alt="image-20210928105921447" style="zoom:50%;" />
你会发现随着N的增长，步数大约增长为N的2次方。因此描述冒泡排序效率的大O记法，是O(N<sup>2</sup>次方)。</p>
<img src="/img/common/image-20210928105817469.png" alt="image-20210928105817469" style="zoom: 50%;" />
<img src="/img/common/image-20210928105947600.png" alt="image-20210928105947600" style="zoom:50%;" />
<h3 id="◆-4-5-二次问题" tabindex="-1"><a class="header-anchor" href="#◆-4-5-二次问题" aria-hidden="true">#</a> ◆ 4.5 二次问题</h3>
<p>假设你正在写一个JavaScript应用，它要检查数组中是否有重复值。首先想到的做法可能是类似下面的嵌套for循环。
<img src="/img/common/image-20210928110256050.png" alt="image-20210928110256050" style="zoom:50%;" /></p>
<p>嵌套循环算法的效率就是O(N<sup>2</sup>)。一旦看到嵌套循环，你就应该马上想到O(N<sup>2</sup>)。很明显效率很低，所以要优化。该函数只有一种步骤，就是比较。它重复地比较i和j所指的值，看它们是否相等，以判断数组有没有重复值。最坏的情况就是没有重复，这将使我们跑遍内外两层循环，比较完所有i、j组合，才返回false。虽然hasDuplicateValue是我们目前唯一想到的解决方法，但在确定采用之前，应意识到它的O(N2)意味着低效。当遇到低效的算法时，我们都应该花些时间思考下有没有更快的做法。特别是当数据量巨大的时候，优化不足的应用甚至可能会突然挂掉。尽管这可能已经是最佳方案，但你还是要确认一下。</p>
<h3 id="◆-4-6-线性解决" tabindex="-1"><a class="header-anchor" href="#◆-4-6-线性解决" aria-hidden="true">#</a> ◆ 4.6 线性解决</h3>
<p>线性解决以下是hasDuplicateValue的另一种实现，它没有嵌套循环。看看它是否会比之前的更加高效。此实现只有一个循环，并将迭代过程中遇到的数字用数组existingNumbers记录下来。其记录方法很有趣：每发现一个新的数字，就以其为索引找出existingNumbers中对应的格子，将其赋值为1。举个例子，如果参数array为[3,5,8]，那么循环结束时，existingNumbers就会变成以下这样。里面那些1的位置为索引3、5、8，因为array包含的这些数字已被发现。
<img src="/img/common/image-20210928111230457.png" alt="image-20210928111230457" style="zoom:50%;" />
<img src="/img/common/image-20210928110829159.png" alt="image-20210928110829159" style="zoom:50%;" />
此算法的主要步骤也是比较。读取existingNumbers上某索引的值，并与undefined比较，最坏的情况就是无重复，因为你得跑完整个循环才能发现。N个元素就要 N次比较。因为这里只有一个循环，数组有多少个元素，它就要迭代多少次。因此其大O记法是O(N)。
毫无疑问，熟悉大O记法能使我们发现低效的代码，有助于我们挑选出更快的算法。然而，偶尔也会有两种算法的大O相同，但实际上二者快慢不一的情况。下一章我们就来学习当大O记法太过粗略的时候，如何识别两种算法的效率高低。</p>
<h2 id="第5章-用或不用大o来优化代码" tabindex="-1"><a class="header-anchor" href="#第5章-用或不用大o来优化代码" aria-hidden="true">#</a> 第5章 用或不用大O来优化代码</h2>
<p>大O是一种能够比较算法效率，并告诉我们在特定环境下应采用何种算法的伟大工具。但我们不能完全依赖于它。因为有时候即使两种算法的大O记法完全一样，但实际上其中一个比另一个要快得多。</p>
<h3 id="◆-5-1-选择排序" tabindex="-1"><a class="header-anchor" href="#◆-5-1-选择排序" aria-hidden="true">#</a> ◆ 5.1 选择排序</h3>
<p>选择排序的步骤如下：</p>
<p>(1) 从左至右检查数组的每个格子，找出值最小的那个。在此过程中，我们会用一个变量来记住检查过的数字的最小值（事实上记住的是索引，但为了看起来方便，下图就直接写出数值）。如果一个格子中的数字比记录的最小值还要小，就把变量改成该格子的索引，如图所示。</p>
<img src="/img/common/image-20210928112724088.png" alt="image-20210928112724088" style="zoom:50%;" />
<p>(2) 知道哪个格子的值最小之后，将该格与本次检查的起点交换。第1次检查的起点是索引0，第2次是索引1，以此类推。下图展示的是第一次检查后的交换动作。</p>
<img src="/img/common/image-20210928112751129.png" alt="image-20210928112751129" style="zoom:50%;" />
<p>(3) 重复第(1) (2)步，直至数组排好序。</p>
<h3 id="◆-5-2-选择排序实战" tabindex="-1"><a class="header-anchor" href="#◆-5-2-选择排序实战" aria-hidden="true">#</a> ◆ 5.2 选择排序实战</h3>
<img src="/img/common/image-20210928112916005.png" alt="image-20210928112916005" style="zoom:50%;" />
<img src="/img/common/image-20210928112938632.png" alt="image-20210928112938632" style="zoom:50%;" />
<p>​	<img src="/img/common/image-20210928112959983.png" alt="image-20210928112959983" style="zoom:50%;" /></p>
<img src="/img/common/image-20210928113037039.png" alt="image-20210928113037039" style="zoom:50%;" />
<img src="/img/common/image-20210928113232621.png" alt="image-20210928113232621" style="zoom:50%;" />
<h3 id="◆-5-4-选择排序的效率" tabindex="-1"><a class="header-anchor" href="#◆-5-4-选择排序的效率" aria-hidden="true">#</a> ◆ 5.4 选择排序的效率</h3>
<p>​		选择排序的步骤可分为两类：比较和交换，也就是在每轮检查中把未排序的值跟该轮已遇到的最小值做比较，以及将最小值与该轮起点的值交换以使其位置正确。推广开来，若有N个元素，就会有 (N -1) + (N -2) + (N -3) + … + 1次比较。但每轮的交换最多只有1次。如果该轮的最小值已在正确位置，就无须交换，否则要做1次交换。相比之下，冒泡排序在最坏情况（完全逆序）时，每次比较过后都要进行1次交换。选择排序的步数大概只有冒泡排序的一半，即选择排序比冒泡排序快一倍。</p>
<img src="/img/common/image-20210928113727191.png" alt="image-20210928113727191" style="zoom:50%;" />
<img src="/img/common/image-20210928113926869.png" alt="image-20210928113926869" style="zoom:50%;" />
<h3 id="◆-5-5-忽略常数" tabindex="-1"><a class="header-anchor" href="#◆-5-5-忽略常数" aria-hidden="true">#</a> ◆ 5.5 忽略常数</h3>
<p>选择排序的大O记法为O(N<sup>2</sup>)，跟冒泡排序一样。这是因为大O记法的一条重要规则我们至今还没提到：大O记法忽略常数。</p>
<p>大O记法不包含一般数字，除非是指数。</p>
<p>严格来说本应为O(N<sup>2</sup>/ 2)，最终得写成O(N<sup>2</sup>)。类似地，O(2N)要写成O(N);O(N/2)也写成O(N)；就算是比O(N)慢100倍的O(100N)，也要写成O(N),速度相差100倍的两种算法，它们的大O记法却一样，这或许会让人觉得大O没什么意义。就像同为 O(N)的选择排序和冒泡排序，其实前者比后者快1倍，要在二者之中挑选，无疑是用选择排序。</p>
<h3 id="◆-5-6-大o的作用" tabindex="-1"><a class="header-anchor" href="#◆-5-6-大o的作用" aria-hidden="true">#</a> ◆ 5.6 大O的作用</h3>
<img src="/img/common/image-20210928115110983.png" alt="image-20210928115110983" style="zoom:50%;" />
<p>尽管不能比较冒泡排序和选择排序，大O还是很重要的，因为它能够区分不同算法的长期增长率。当数据量达到一定程度时，O(N)的算法就会永远快过 O(N<sup>2</sup>)，无论这个 O(N)实际上是O(2N)还是O(100N)。即使是O(100N)，这个临界点也是存在的。</p>
<p>大O记法只表明，对于不同分类，存在一临界点，在这一点之后，一类算法会快于另一类，并永远保持下去。至于这个点在哪里，大O并不关心。因此，不需要写成O(100N)，归类到O(N)就好了。</p>
<p>同样地，在数据量增大到某个点时，O(log N)便会永远超越O(N)，即使该O(log N)算法的实际步数为O(2log N)。</p>
<p>所以大 O是极为有用的工具，当两种算法落在不同的大 O类别时，你就很自然地知道应该选择哪种。因为在大数据的情况下，必然存在一临界点使这两种算法的速度永远区分开来。</p>
<h2 id="第6章-插入排序" tabindex="-1"><a class="header-anchor" href="#第6章-插入排序" aria-hidden="true">#</a> 第6章 插入排序</h2>
<h3 id="◆-6-1-插入排序" tabindex="-1"><a class="header-anchor" href="#◆-6-1-插入排序" aria-hidden="true">#</a> ◆ 6.1 插入排序</h3>
<p>冒泡排序和选择排序。虽然它们的效率都是O(N<sup>2</sup>)，但其实选择排序比冒泡排序快一倍。现在来学第三种排序算法——插入排序。
步骤：
(1) 在第一轮里，暂时将索引1（第2格）的值移走，并用一个临时变量来保存它。这使得该索引处留下一个空隙，因为它不包含值。
<img src="/img/common/image-20210928115445903.png" alt="image-20210928115445903" style="zoom: 33%;" />
在之后的轮回，我们会移走后面索引的值。
(2) 接着便是平移阶段，我们会拿空隙左侧的每一个值与临时变量的值进行比较。
<img src="/img/common/image-20210928115513976.png" alt="image-20210928115513976" style="zoom:33%;" />
随着值右移，空隙会左移。如果遇到比临时变量小的值，或者空隙已经到了数组的最左端，就结束平移阶段。
(3) 将临时移走的值插入当前空隙。
<img src="/img/common/image-20210928115625980.png" alt="image-20210928115625980" style="zoom:33%;" />
(4) 重复第(1)至(3)步，直至数组完全有序。</p>
<h3 id="◆-6-4-插入排序的效率" tabindex="-1"><a class="header-anchor" href="#◆-6-4-插入排序的效率" aria-hidden="true">#</a> ◆ 6.4 插入排序的效率</h3>
<p>插入排序包含4种步骤：移除、比较、平移和插入。要分析插入算法的效率，就得把每种步骤都统计一遍。首先看看比较。每次拿temp_value跟空隙左侧的值比大小就是比较。在数组完全逆序的最坏情况下，我们每一轮都要将temp_value左侧的所有值与temp_value比较。因为那些值全都大于temp_value，所以每一轮都要等到空隙移到最左端才能结束。
对于有5个元素的数组，最多需要：1 + 2 + 3 + 4=10次比较。对于有10个元素的数组，最多需要：1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9=45次比较。（对于有20个元素的数组，最多需要190次比较，以此类推。）由此可发现一个规律：对于有N个元素的数组，大约需要N<sup>2</sup>/ 2次比较（10<sup>2</sup>/ 2是50,20<sup>2</sup>/ 2是200）。
我们每次将值右移一格，就是平移操作。当数组完全逆序时，有多少次比较就要多少次平移，因为每次比较的结果都会使你将值右移。把最坏情况下的比较步数和平移步数相加。N<sup>2</sup>/ 2次比较+ N<sup>2</sup>/ 2次平移=N<sup>2</sup>步
temp_value的移除跟插入在每一轮里都会各发生一次。因为总是有N -1轮，所以可以得出结论：有N -1次移除和N -1次插入。把它们都相加。N<sup>2</sup>比较和平移的合计+ N -1次移除+ N -1次插入=N<sup>2</sup>+ 2N -2步
而大O只保留最高阶的N。换句话说，如果有个算法需要N4+ N3+ N2+ N步，我们就只会关注其中的N4，即以O(N4)来表示。为什么呢？因为随着N的变大，N4的增长越来越抛离其他阶。当N为1000时，N4就比N3大了1000倍。因此，我们只关心最高阶的N。所以在插入排序的例子中，O(N2+ N)还得进一步简化成O(N2)。
所以在最坏的情况里，插入排序的时间复杂度跟冒泡排序、选择排序一样，都是O(N2)。虽然冒泡排序和选择排序都是O(N<sup>2</sup>)，但选择排序实际上是N<sup>2</sup>/2步，比N<sup>2</sup>步的冒泡排序更快。乍一看，你可能会觉得插入排序跟冒泡排序一样，因为它们都是O(N<sup>2</sup>)，其实插入排序是N<sup>2</sup>+ 2N -2步。
总结：目前来说：实际冒泡排序是N<sup>2</sup>，选择排序是N<sup>2</sup>/2，插入排序是N<sup>2</sup>+ 2N -2
确实，在最坏情况里，选择排序比插入排序快。但是我们还应该考虑平均情况。最坏情况是所有数据都要比较和平移；最好情况是每轮一次比较、零次平移；对于平均情况，总的来看，是比较和平移一半的数据。如果说插入排序的最坏情况需要N2步，那么平均情况就是N2/ 2步。尽管最终大O都会写成O(N2)。
<img src="/img/common/image-20210928132733293.png" alt="image-20210928132733293" style="zoom:50%;" /></p>
<blockquote>
<blockquote></blockquote>
</blockquote>
<h3 id="◆-6-5-平均情况" tabindex="-1"><a class="header-anchor" href="#◆-6-5-平均情况" aria-hidden="true">#</a> ◆ 6.5 平均情况</h3>
<p>插入排序的性能在不同场景中差异很大。最坏、平均、最好情况，分别需要 N<sup>2</sup>、N<sup>2</sup>/ 2、N步。
选择排序是无论何种情况，最坏、平均、最好，都要N<sup>2</sup>/2步。因为这个算法没有提早结束某一轮的机制，不管遇到什么，每一轮都得比较所选索引右边的所有值。
那么哪种算法更好？选择排序还是插入排序？答案是：看情况。对于平均情况（数组里的值随机分布），它们性能相近。如果你确信数组是大致有序的，那么插入排序比较好。如果是大致逆序，则选择排序更快。如果你无法确定数据是什么样，那就算是平均情况了，两种都可以。</p>
<h3 id="◆-6-7-总结" tabindex="-1"><a class="header-anchor" href="#◆-6-7-总结" aria-hidden="true">#</a> ◆ 6.7 总结</h3>
<p>懂得区分最好、平均、最坏情况，是为当前场景选择最优算法以及给现有算法调优以适应环境变化的关键。记住，虽然为最坏情况做好准备十分重要，但大部分时间我们面对的是平均情况。</p>
<h2 id="第7章-查找迅速的散列表" tabindex="-1"><a class="header-anchor" href="#第7章-查找迅速的散列表" aria-hidden="true">#</a> 第7章 查找迅速的散列表</h2>
<p>试想你在写一个快餐店的点单程序，准备实现一个展示各种食物及相应价格的菜单。你可能会用数组来做（当然这没问题）。menu=[ [&quot;french fries&quot;, 0.75], [&quot;hamburger&quot;, 2.5], [&quot;hot dog&quot;, 1.5], [&quot;soda&quot;,0.6] ]该数组由一些子数组构成，每个子数组包含两个元素。第一个元素是表示食物名称的字符串，第二个元素是该食物的价格。就如第2章学到的，在无序的数组里查找某种食物的价格，得用线性查找，需要O(N)步。有序数组则可以用二分查找，只需要O(log N)步。尽管O(log N)也不错，但我们可以做得更好。事实上，可以好很多。到了本章结尾，你会掌握一种名为散列表的数据结构，只用O(1)步就能找出数据。</p>
<h3 id="◆-7-1-探索散列表" tabindex="-1"><a class="header-anchor" href="#◆-7-1-探索散列表" aria-hidden="true">#</a> ◆ 7.1 探索散列表</h3>
<p>大多数编程语言都自带散列表这种能够快速读取的数据结构。但在不同的语言中，它有不同的名字，除了散列表，还有散列、映射、散列映射、字典、关联数组。</p>
<h3 id="◆-7-2-用散列函数来做散列" tabindex="-1"><a class="header-anchor" href="#◆-7-2-用散列函数来做散列" aria-hidden="true">#</a> ◆ 7.2 用散列函数来做散列</h3>
<p>一个散列函数需满足以下条件才有效：每次对同一字符串调用该散列函数，返回的都应是同一数字串。如果每次都返回不一样的结果，那就无效。散列表由一对对的数据组成。一对数据里，一个叫作键，另一个叫作值。键和值应该具有某种意义上的关系。</p>
<h3 id="◆-7-3-一个好玩又赚钱的同义词典" tabindex="-1"><a class="header-anchor" href="#◆-7-3-一个好玩又赚钱的同义词典" aria-hidden="true">#</a> ◆ 7.3 一个好玩又赚钱的同义词典</h3>
<p>散列表可以看成是一行能够存储数据的格子，就像数组那样。每个格子都有对应的编号，散列表是如何存储数据的？首先，计算机用散列函数对键进行计算。为了方便演示，这里我们依然使用之前提及的那个乘法函数。假设要取数据：thesaurus['bad'],
收到命令后，计算机就会进行如下两步简单的操作。(1) 计算这个键的散列值：BAD=2×1×4=8。(2) 由于结果是8，因此去到第8格并返回其中的值。在本例中，该值为&quot;evil&quot;。
<img src="/img/common/image-20210928141206781.png" alt="image-20210928141206781" style="zoom:50%;" /></p>
<p>这就是为什么从散列表里读取数据只需要 O(1)了，因为其过程所花的时间是恒定的。它总是先计算出键的散列值，然后根据散列值跳到对应的格子去。</p>
<p>如果是用数组，那么就得一个格子一个格子地去找，直至找到为止。无序数组需要O(N)，有序数组需要O(log N)。但用散列表的话，我们就能够以食物作为键来做O(1)的查找。这就是散列表的好处。</p>
<h3 id="◆-7-4-处理冲突" tabindex="-1"><a class="header-anchor" href="#◆-7-4-处理冲突" aria-hidden="true">#</a> ◆ 7.4 处理冲突</h3>
<p>往已被占用的格子里放东西，会造成冲突。幸好，我们有解决办法。一种经典的做法就是分离链接。当冲突发生时，我们不是将值放到格子里，而是放到该格子所关联的数组里。现在仔细观察该散列表的冲突位置。每个子数组包含两个元素，第一个是被检索的词，后一个是其相应的同义词。在子数组中查找就是线性查找了，数组中每个元素都要走一遍。</p>
<p>若散列表的格子含有数组，因为要在这些数组上做线性查找，所以步数会多于1。如果数据都刚好存在同一个格子里，那么查找就相当于在数组上进行。因此散列表的最坏情况就是O(N)。</p>
<p>为了避免这种情况，散列表的设计应该尽量减少冲突，以便查找都能以O(1)完成。</p>
<h3 id="◆-7-5-找到平衡" tabindex="-1"><a class="header-anchor" href="#◆-7-5-找到平衡" aria-hidden="true">#</a> ◆ 7.5 找到平衡</h3>
<p>散列表的效率取决于以下因素。❏ 要存多少数据。❏ 有多少可用的格子。❏ 用什么样的散列函数。
这就是使用散列表时所需要权衡的：既要避免冲突，又要节约空间。要想解决这个问题，可参考计算机科学家研究出的黄金法则：每增加7个元素，就增加10个格子。</p>
<p>如果要保存14个元素，那就得准备20个格子，以此类推。数据量与格子数的比值称为负载因子。把这个术语代入刚才的理论，就是：理想的负载因子是0.7（7个元素 / 10个格子）。</p>
<p>如果你一开始就将7个元素放进散列表，那么计算机应该会创建出一个含有10个格子的散列表。随着你添加元素，计算机也会添加更多的格子来扩展这个散列表，并改变散列函数，使新数据能均匀地分布到新的格子里去。</p>
<p>幸运的是，一般编程语言都自带散列表的管理机制，它会帮你决定散列表的大小、散列函数的逻辑以及扩展的时机。</p>
<h3 id="◆-7-6-一个实例" tabindex="-1"><a class="header-anchor" href="#◆-7-6-一个实例" aria-hidden="true">#</a> ◆ 7.6 一个实例</h3>
<p>使用散列表来提高算法速度：
我们学习了基于数组的集合——一种能保证元素不重复的数组。每次往其中插入新元素时，都要先做一次线性查找来确定该元素是否已存在（如果是无序数组）。如果要在一个大集合上进行多次插入，效率将会下降得很快，因为每次插入都需要O(N)。我们都可以把散列表当成集合来用。把数组作为集合的话，数据是直接放到格子里的。用散列表时，则是将数据作为键，值可以为任何形式，例如数字1，或者布尔值true也行。
散列表确实非常适用于检查数据的存在性。
之前的查找数组中是否有重复的值，使用类似的逻辑，但换成散列表（在Javascript里叫作对象），就可以处理字符串了。这种方法也是O(N)，其中的existingValues不是数组而是散列表，用字符串作为键（索引）是没有问题的。
<img src="/img/common/image-20210928145304030.png" alt="image-20210928145304030" style="zoom:50%;" /></p>
<h2 id="第8章-用栈和队列来构造灵巧的代码" tabindex="-1"><a class="header-anchor" href="#第8章-用栈和队列来构造灵巧的代码" aria-hidden="true">#</a> 第8章 用栈和队列来构造灵巧的代码</h2>
<p>两种新的数据结构：栈和队列。事实上它们并不是全新的东西，只不过是多加了一些约束条件的数组而已。</p>
<p>栈和队列都是处理临时数据的灵活工具。在操作系统、打印任务、数据遍历等各种需要临时容器才能构造出美妙算法的场景，它们都大有作为。</p>
<h3 id="◆-8-1-栈" tabindex="-1"><a class="header-anchor" href="#◆-8-1-栈" aria-hidden="true">#</a> ◆ 8.1 栈</h3>
<p>栈存储数据的方式跟数组一样，都是将元素排成一行。只不过它还有以下3条约束。
❏ 只能在末尾插入数据。
❏ 只能读取末尾的数据。
❏ 只能移除末尾的数据。</p>
<p>绝大部分计算机科学家都把栈的末尾称为栈顶，把栈的开头称为栈底。
压栈和出栈可被形容为LIFO（last in, first out）后进先出。解释起来就是最后入栈的元素，会最先出栈。</p>
<h3 id="◆-8-2-栈实战" tabindex="-1"><a class="header-anchor" href="#◆-8-2-栈实战" aria-hidden="true">#</a> ◆ 8.2 栈实战</h3>
<p>栈很少用于需要长期保留数据的场景，却常用于各种处理临时数据的算法。</p>
<p>写一个初级的JavaScript分析器——一种用来检查JavaScript代码的语法是否正确的工具。因为JavaScript的语法规则很多，所以它可以做得很复杂。简单起见，我们就只专注于检查括号的闭合情况吧，包括圆括号、方括号、花括号，
先分析一下括号的语法错误会有哪些情况。分类就是以下3种：
1.首先是有左括号没有右括号的情况。<code v-pre>（var x=2;</code>
2.没有左括号但有右括号的情况。<code v-pre>var x=2;)</code>
3.右括号类型与其前面最近的左括号不匹配，例如：<code v-pre>(var x=[1,2,3)]</code>;
那么怎样才能实现一种能检查一行代码里括号写得对不对的算法呢？用栈就好办了。先准备一个空栈，然后从左至右读取代码的每一个字符，并执行以下规则。
(1) 如果读到的字符不是任一种括号（圆括号、方括号、花括号），就忽略它，继续下一个。
(2) 如果读到左括号，就将其压入栈中，意味着后面需要有对应的右括号来做闭合。
(3) 如果读到右括号，就查看栈顶的元素，并做如下分析。
■ 如果栈里没有任何元素，也就是遇到了右括号但没有左括号，即第2类语法错误。
■ 如果栈里有数据，但与刚才读到的右括号类型不匹配，那就是第3类语法错误。
■ 如果栈顶元素是匹配的左括号，则表示它已经闭合。那么就可以将其弹出，因为已经不需要再记住它了。
(4) 如果一行代码读完，栈里还留有数据，那就表示存在左括号，没有右括号与之匹配，即第1类语法错误。</p>
<blockquote>
<blockquote></blockquote>
</blockquote>
<p>**当数据的处理顺序要与接收顺序相反时（LIFO），用栈就对了。**像文字处理器的“撤销”动作，或网络应用程序的函数调用（递归的核心），你应该都会需要栈来实现。</p>
<h3 id="◆-8-3-队列" tabindex="-1"><a class="header-anchor" href="#◆-8-3-队列" aria-hidden="true">#</a> ◆ 8.3 队列</h3>
<p>队列对于临时数据的处理也十分有趣，它跟栈一样都是有约束条件的数组。</p>
<p>“FIFO”（first in, first out）先进先出，</p>
<p>与栈类似，队列也有3个限制（但内容不同）。
❏ 只能在末尾插入数据（这跟栈一样）。
❏ 只能读取开头的数据（这跟栈相反）。
❏ 只能移除开头的数据（这也跟栈相反）。</p>
<h3 id="◆-8-4-队列实战" tabindex="-1"><a class="header-anchor" href="#◆-8-4-队列实战" aria-hidden="true">#</a> ◆ 8.4 队列实战</h3>
<p>队列应用广泛，从打印机的作业设置，到网络应用程序的后台任务，都有队列的存在。</p>
<h3 id="◆-8-5-总结" tabindex="-1"><a class="header-anchor" href="#◆-8-5-总结" aria-hidden="true">#</a> ◆ 8.5 总结</h3>
<p>掌握了栈和队列，就解锁出了下一个目标：学习基于栈的递归。递归也是其他高级算法的基础</p>
<h2 id="第9章-递归" tabindex="-1"><a class="header-anchor" href="#第9章-递归" aria-hidden="true">#</a> 第9章 递归</h2>
<p>函数调用自身，就叫作递归。无限递归用处不大，甚至还挺危险，但是有限的递归很强大。</p>
<h3 id="◆-9-1-用递归代替循环" tabindex="-1"><a class="header-anchor" href="#◆-9-1-用递归代替循环" aria-hidden="true">#</a> ◆ 9.1 用递归代替循环</h3>
<p>几乎所有循环都能够转换成递归。但能用不代表该用。递归的强项在于巧妙地解决问题，但在上面的例子中，它并不比普通的循环更加优雅、高效。我们很快就会看到能让递归发挥威力的场景，但在那之前，还是先理清递归的运作方式。</p>
<h3 id="◆-9-2-基准情形" tabindex="-1"><a class="header-anchor" href="#◆-9-2-基准情形" aria-hidden="true">#</a> ◆ 9.2 基准情形</h3>
<p>在递归领域（真有这么一个地方），不再递归的情形称为基准情形。</p>
<h3 id="◆-9-3-阅读递归代码" tabindex="-1"><a class="header-anchor" href="#◆-9-3-阅读递归代码" aria-hidden="true">#</a> ◆ 9.3 阅读递归代码</h3>
<p>递归是需要时间和练习才能适应的，到那时候，你会掌握两种技巧：阅读递归代码和编写递归代码。阅读递归代码相对简单一点，所以就先从这里入手吧。</p>
<h3 id="◆-9-4-计算机眼中的递归" tabindex="-1"><a class="header-anchor" href="#◆-9-4-计算机眼中的递归" aria-hidden="true">#</a> ◆ 9.4 计算机眼中的递归</h3>
<p>计算机调用factorial(3)，并在该方法返回前，调用了factorial(2)，而在factorial(2)返回前，又调用了factorial(1)。从技术上来说，当计算机执行factorial(1)时，它其实还在factorial(2)之中，而factorial(2)又正在factorial(3)之中。计算机是用栈来记录每个调用中的函数。这个栈就叫作调用栈。</p>
<p>让我们以factorial为例来观察调用栈如何运作。</p>
<p>起初计算机调用的是factorial(3)。然而，在该方法完成之前，它又调用了factorial(2)。为了记住自己还在factorial(3)中，计算机将此事压入调用栈中。
<img src="/img/common/image-20210928152545311.png" alt="image-20210928152545311" style="zoom:50%;" /></p>
<p>接着计算机开始处理factorial(2)。该factorial(2)会调用factorial(1)。不过在进入factorial(1)前，计算机得记住自己还在factorial(2)中，于是，它将此事也压入调用栈中。
<img src="/img/common/image-20210928152603522.png" alt="image-20210928152603522" style="zoom:50%;" /></p>
<p>然后计算机执行factorial(1)。因为1已经是基准情形了，所以它可以返回，不用再调用factorial。</p>
<p>尽管factorial(1)结束了，但调用栈内仍存在数据，意味着整件事还没完，计算机还处于其他函数当中。你应该还记得，栈的规定是只有栈顶元素（即最后的元素）才能被看到。所以，计算机接下来就去检查了调用栈的栈顶，发现那是factorial(2)。</p>
<p>由于factorial(2)是调用栈的最后一项，因此代表最近调用并且最应该先完成的是factorial(2)。于是计算机将factorial(2)从调用栈弹出。
<img src="/img/common/image-20210928152735324.png" alt="image-20210928152735324" style="zoom:50%;" /></p>
<p>无限递归（如本章开头的例子）的程序会一直将同一方法加到调用栈上，直到计算机的内存空间不足，最终导致栈溢出的错误。</p>
<h3 id="◆-9-5-递归实战" tabindex="-1"><a class="header-anchor" href="#◆-9-5-递归实战" aria-hidden="true">#</a> ◆ 9.5 递归实战</h3>
<p>递归可以自然地用于实现那些需要重复自身的算法。在这些情况下，递归可以增强代码的可读性</p>
<p>比如说遍历文件系统。假设你现在要写一个脚本，它用于对一个目录下的所有文件进行某种操作。这里的“所有文件”，不仅指的是该目录中的文件，还包括其子目录的文件，以及子目录里的子目录的文件，</p>
<p>改用递归并不会改变算法的大O。但是，在下一章你会看到，递归可以作为算法的核心组件，影响算法的速度。</p>
<h3 id="◆-9-6-总结" tabindex="-1"><a class="header-anchor" href="#◆-9-6-总结" aria-hidden="true">#</a> ◆ 9.6 总结</h3>
<p>递归十分适用于那些无法预估计算深度的问题</p>
<p>掌握递归，你就解锁了一批高效但更为高深的算法。它们都离不开递归的原理</p>
<h2 id="◆-第10章-飞快的递归算法" tabindex="-1"><a class="header-anchor" href="#◆-第10章-飞快的递归算法" aria-hidden="true">#</a> ◆ 第10章 飞快的递归算法</h2>
<p>前几章我们学会了一些排序算法，包括冒泡排序、选择排序和插入排序。但在现实中，数组排序不是通过它们来做的。为了免去大家重复编写排序算法的烦恼，大多数编程语言都自带用于数组排序的函数，其中很多采用的都是快速排序。</p>
<p>虽然它已经实现好了，但我们还是想研究一下它的原理，因为其运用递归来给算法提速的做法极具推广意义。</p>
<p>快速排序真的很快。尽管在最坏情况（数组逆序）下它跟插入排序、选择排序的效率差不多，但在日常多见的平均情况中，它的确表现优异。</p>
<p>快速排序依赖于一个名为分区的概念</p>
<h3 id="◆-10-1-分区" tabindex="-1"><a class="header-anchor" href="#◆-10-1-分区" aria-hidden="true">#</a> ◆ 10.1 分区</h3>
<p>此处的分区指的是从数组随机选取一个值，以其为轴，将比它小的值放到它左边，比它大的值放到它右边。分区的算法实现起来很简单</p>
<p>接着就可以分区了，步骤如下
(1) 左指针逐个格子向右移动，当遇到大于或等于轴的值时，就停下来。
(2) 右指针逐个格子向左移动，当遇到小于或等于轴的值时，就停下来。
(3) 将两指针所指的值交换位置。
(4) 重复上述步骤，直至两指针重合，或左指针移到右指针的右边。
(5) 将轴与左指针所指的值交换位置。</p>
<p>当分区完成时，在轴左侧的那些值肯定比轴要小，在轴右侧的那些值肯定比轴要大。因此，轴的位置也就确定了，虽然其他值的位置还没有完全确定。</p>
<h3 id="◆-10-2-快速排序" tabindex="-1"><a class="header-anchor" href="#◆-10-2-快速排序" aria-hidden="true">#</a> ◆ 10.2 快速排序</h3>
<p>快速排序严重依赖于分区。它的运作方式如下所示。
(1) 把数组分区。使轴到正确的位置上去。
(2) 对轴左右的两个子数组递归地重复第1、2步，也就是说，两个子数组都各自分区，并形成各自的轴以及由轴分隔的更小的子数组。然后也对这些子数组分区，以此类推。
(3) 当分出的子数组长度为0或1时，即达到基准情形，无须进一步操作。</p>
<h3 id="◆-10-3-快速排序的效率" tabindex="-1"><a class="header-anchor" href="#◆-10-3-快速排序的效率" aria-hidden="true">#</a> ◆ 10.3 快速排序的效率</h3>
<p>为了搞清楚快速排序的效率，我们先从分区开始。分解来看，你会发现它包含两种步骤。
❏ 比较：每个值都要与轴做比较。
❏ 交换：在适当时候将左右指针所指的两个值交换位置。
一次分区至少有N次比较，即数组的每个值都要与轴做比较。因为每次分区时，左右指针都会从两端开始靠近，直到相遇。交换的次数则取决于数据的排列情况。一次分区里，交换最少会有1次，最多会有N/ 2次，因为即使所有元素都需要交换，我们也只是将左半部分与右半部分进行交换</p>
<p>对于随机排列的数据，粗略来算就是N / 2的一半，即N / 4次交换。于是，N次比较加上N / 4次交换，共1.25N步。最后根据大O记法的规则，忽略常数项，得出分区操作的时间为O(N)。</p>
<p>在上面一个数组含8个元素的例子中，快速排序花了大约21步，也很接近8×log8（等于24）。这种时间复杂度的算法我们还是第一次遇到，用大O记法来表达的话，它是O(N log N)算法。</p>
<p>因为等分发生了log N次，而每次都要对总共N个元素做分区，所以总步数为N×logN。之前我们看到的很多算法，最佳情况都发生在元素有序的时候。但在快速排序里，最佳情况应该是每次分区后轴都刚好落在子数组的中间。</p>
<h3 id="◆-10-4-最坏情况" tabindex="-1"><a class="header-anchor" href="#◆-10-4-最坏情况" aria-hidden="true">#</a> ◆ 10.4 最坏情况</h3>
<p>虽然快速排序在最好情况和最坏情况都没能超越插入排序，但在最常遇见的平均情况，前者的O(N log N)比后者的O(N2)好得多，所以总体来说，快速排序优于插入排序。
<img src="/img/common/image-20210928155145278.png" alt="image-20210928155145278" style="zoom:50%;" /></p>
<p>由于快速排序在平均情况下表现优异，于是很多编程语言自带的排序函数都采用它来实现。因此一般你不需要自己写快速排序。但你可能需要学会写快速选择——它是一种类似快速排序的实用算法。
<img src="/img/common/image-20210928155226021.png" alt="image-20210928155226021" style="zoom:50%;" /></p>
<h3 id="◆-10-5-快速选择" tabindex="-1"><a class="header-anchor" href="#◆-10-5-快速选择" aria-hidden="true">#</a> ◆ 10.5 快速选择</h3>
<p>快速选择需要对数组分区，这跟快速排序类似，或者你可以把它想象成是快速排序和二分查找的结合。</p>
<p>分区的作用就是把轴排到正确的格子上。快速选择就利用了这一点。</p>
<p>快速选择的优势就在于它不需要把整个数组都排序就可以找到正确位置的值。
<img src="/img/common/image-20210928154835716.png" alt="image-20210928154835716" style="zoom:50%;" /></p>
<p>如果像快速排序那样，每次分区后还是要处理原数组那么多的数据，就会导致O(Nlog N)的步数。但快速选择不同，下一次的分区操作只需在上一次分出的一半区域上进行，即值可能存在的那一半。</p>
<h3 id="◆-10-6-总结" tabindex="-1"><a class="header-anchor" href="#◆-10-6-总结" aria-hidden="true">#</a> ◆ 10.6 总结</h3>
<p>由于运用了递归，快速排序和快速选择可以将棘手的问题解决得既巧妙又高效。</p>
<p>其实能递归的不只有算法，还有数据结构。后面几章将要接触的链表、二叉树以及图，就利用了自身递归的特性，给我们提供了迅速的数据操作方式。</p>
<h2 id="第11章-基于结点的数据结构" tabindex="-1"><a class="header-anchor" href="#第11章-基于结点的数据结构" aria-hidden="true">#</a> 第11章 基于结点的数据结构</h2>
<p>基于结点的数据结构拥有独特的存取方式，因此在某些时候具有性能上的优势。</p>
<p>链表，它是最简单的一种基于结点的数据结构</p>
<h3 id="◆-11-1-链表" tabindex="-1"><a class="header-anchor" href="#◆-11-1-链表" aria-hidden="true">#</a> ◆ 11.1 链表</h3>
<p>像数组一样，链表也用来表示一系列的元素。事实上，能用数组来做的事情，一般也可以用链表来做。然而，链表的实现跟数组是不一样的，在不同场景它们会有不同的性能表现。
计算机的内存就像一大堆格子，每格都可以用来保存比特形式的数据。当要创建数组时，程序会在内存中找出一组连续的空格子，给它们起个名字，以便你的应用存放数据。</p>
<p>计算机能够直接跳到数组的某一索引上。如果代码要求它读取索引4的值，那么计算机只需一步就可以完成任务。重申一次，之所以能够这样，是因为程序事先知道了数组开头所在的内存地址——例如地址是1000——当它想去索引4时，便会自动跳到1004处。</p>
<p>与数组不同的是，组成链表的格子不是连续的。它们可以分布在内存的各个地方。这种不相邻的格子，就叫作结点。</p>
<p>那么问题来了，计算机怎么知道这些分散的结点里，哪些属于这个链表，哪些属于其他链表呢？这就是链表的关键了：每个结点除了保存数据，它还保存着链表里的下一结点的内存地址。</p>
<p>这份用来指示下一结点的内存地址的额外数据，被称为链。
<img src="/img/common/image-20210928160139103.png" alt="image-20210928160139103" style="zoom:50%;" /></p>
<p>我们的链表包含4项数据：&quot;a&quot;、&quot;b&quot;、&quot;c&quot;和&quot;d&quot;。因为每个结点都需要2个格子，头一格用作数据存储，后一格用作指向下一结点的链（最后一个结点的链是null，因为它是终点），所以整体占用了8个格子。</p>
<p>若想使用链表，你只需知道第一个结点在内存的什么位置。因为每个结点都有指向下一结点的链，所以只要有给定的第一个结点，就可以用结点1的链找到结点2，再用结点2的链找到结点3……如此遍历链表的剩余部分。</p>
<p>链表相对于数组的一个好处就是，它可以将数据分散到内存各处，无须事先寻找连续的空格子。
实现一个链表：
我们用Ruby来写一个链表，最终实现包含两个类：Node和LinkedList。先是Node。
<img src="/img/common/image-20210928160309208.png" alt="image-20210928160309208" style="zoom: 33%;" />
Node类有两个属性：data表示结点所保存的数据，next_node表示指向下一结点的链，使用方法如下。
<img src="/img/common/image-20210928160358034.png" alt="image-20210928160358034" style="zoom:33%;" />
以上代码创建了4个连起来的结点，它们分别保存着&quot;once&quot;、&quot;upon&quot;、&quot;a&quot;和&quot;time&quot;4项数据。虽然只用Node也可以创建出链表，但我们的程序无法由此轻易地得知哪个结点是链表的开端。因此我们还得创建一个LinkedList类。下面是一个最基本的LinkedList的写法。
<img src="/img/common/image-20210928160507196.png" alt="image-20210928160507196" style="zoom:33%;" />
有了这个类，我们就可以用以下代码让程序知道链表的起始位置了。
<img src="/img/common/image-20210928160542378.png" alt="image-20210928160542378" style="zoom:33%;" />
LinkedList的作用就是一个指针，它指向链表的第一个结点。既然知道了链表是什么，那么接下来做个它跟数组的性能对比，观察它们在读取、查找、插入和删除上有何优劣。</p>
<h3 id="◆-11-3-读取" tabindex="-1"><a class="header-anchor" href="#◆-11-3-读取" aria-hidden="true">#</a> ◆ 11.3 读取</h3>
<p>假设程序要读取链表中索引2的值，计算机不可能在一步之内完成，因为无法一下子算出它在内存的哪个位置。毕竟，链表的结点可以分布在内存的任何地方。程序知道的只有第1个结点的内存地址，要找到索引2的结点（即第3个），程序必须先读取索引0的链，然后顺着该链去找索引1。接着再读取索引1的链，去找索引2，这才能读取到索引2里的值。</p>
<p>读取链表中某个索引值的最坏情况，应该是读取最后一个索引。这种情况下，因为计算机得从第一个结点开始，沿着链一直读到最后一个结点，于是需要N步。由于大O记法默认采用最坏情况，所以我们说读取链表的时间复杂度为O(N)。这跟读取数组的O(1)相比，的确是一大劣势。</p>
<h3 id="◆-11-4-查找" tabindex="-1"><a class="header-anchor" href="#◆-11-4-查找" aria-hidden="true">#</a> ◆ 11.4 查找</h3>
<p>链表的查找效率跟数组一样。记住，所谓查找就是从列表中找出某个特定值所在的索引。对于数组和链表来说，它们都是从第一格开始逐个格子地找，直至找到。如果是最坏情况，即所找的值在列表末尾，或完全不在列表里，那就要花O(N)步。</p>
<h3 id="◆-11-5-插入" tabindex="-1"><a class="header-anchor" href="#◆-11-5-插入" aria-hidden="true">#</a> ◆ 11.5 插入</h3>
<p>在某些情况下，链表的插入跟数组相比，有着明显的优势。回想插入数组的最坏情况：当插入位置为索引0时，因为需要先将插入位置右侧的数据都右移一格，所以会导致 O(N)的时间复杂度。然而，若是往链表的表头进行插入，则只需一步，即O(1)。</p>
<p>与数组相比，链表在前端插入数据更为便捷。</p>
<p>链表的插入效率为O(N)，与数组一样，因为如果是插入到链表的中间或者后面位置，就需要先查找链表，需要O(N),然后再插入需要1，最后就是O(N)+1,也就是O(N)</p>
<p>有趣的是，通过以上分析，你会发现链表的最坏情况和最好情况与数组刚好相反。在链表开头插入很方便，在数组开头插入却很麻烦；在数组的末尾插入是最好情况，在链表的末尾插入却是最坏情况。</p>
<h3 id="◆-11-6-删除" tabindex="-1"><a class="header-anchor" href="#◆-11-6-删除" aria-hidden="true">#</a> ◆ 11.6 删除</h3>
<p>从效率上来看，删除跟插入是相似的。如果删除的是链表的第一个结点，那就只要1步：将链表的first_node设置成当前的第二个结点。</p>
<p>再回想删除数组的第一个元素时，得把剩余的所有元素左移一格，需要O(N)的时间复杂度。</p>
<p>删除链表的最后一个结点，其实际的删除动作只需1步——令倒数第二的结点的链指向null。然而，要找出倒数第二的结点，得花 N步，因为我们依然只能从第一个结点顺着链往下一个个地找。</p>
<p>对比了各种情况下数组和链表删除操作的效率。注意它跟插入效率的表格几乎一模一样。</p>
<p>过一番分析，链表与数组的性能对比如下所示。
<img src="/img/common/image-20210928162611890.png" alt="image-20210928162611890" style="zoom:50%;" /></p>
<p>尽管两者的查找、插入、删除的效率看起来差不多，但在读取方面，数组比链表要快得多。既然如此，那为什么还要用链表呢？</p>
<h3 id="◆-11-7-链表实战" tabindex="-1"><a class="header-anchor" href="#◆-11-7-链表实战" aria-hidden="true">#</a> ◆ 11.7 链表实战</h3>
<p>高效地遍历单个列表并删除其中多个元素，是链表的亮点之一。假设我们正在写一个整理电子邮件地址的应用，它会删掉列表中无效格式的地址。具体算法是，每次读取一个地址，然后用正则表达式（一种用于识别数据格式的特定模式）来校验其有效性。如果发现该地址无效，就将它从列表中移除。</p>
<p>不管这个列表是数组还是链表，要检查每个元素的话，都得花N步。然而，当要删除邮件地址时，它们的效率却不同，下面我们来验证一下。</p>
<p>用数组的话，每次删除邮件地址，我们就要另外再花 O(N)步去左移后面的数据，以填补删除所产生的空隙。而且还必须完成这些平移才能执行下一次邮件地址的检查。所以如果存在需要删除的无效地址，那么除了遍历邮件地址的 N步，还得加上 N步乘以无效地址数。</p>
<p>假设每10个地址就有1个是无效的。如果列表包含1000个地址，那么无效的就应该会有100个。于是我们的算法就要花1000步来读取，再加上删除所带来的大约100000步的操作（100个无效地址×N）。</p>
<p>但要是链表的话，每次删除只需1步就好，因为只需改动结点中链的指向，然后就可以继续检查下一邮件地址了。按这种算法去处理1000个邮件地址，只需要1100步（1000步读取和100步删除）。</p>
<h3 id="◆-11-8-双向链表" tabindex="-1"><a class="header-anchor" href="#◆-11-8-双向链表" aria-hidden="true">#</a> ◆ 11.8 双向链表</h3>
<p>链表的另一个引人注目的应用，就是作为队列的底层数据结构</p>
<p>当时我们用数组作为队列的底层，并解释说队列只是有约束条件的数组。其实，改用链表来做队列的底层也可以，同样地，只要使该链表的元素只在末尾插入，并在开头删除就好了。那么用链表来代替数组有什么好处呢？</p>
<p>队列插入数据只能在末尾。如上文所述，在数组的末尾插入是极快的，时间复杂度为O(1)。链表则要O(N)。所以在插入方面，选择数组比链表更好。但到了删除的话，就是链表更快了，因为它只要O(1)，而数组是O(N)。基于以上分析，似乎用数组还是链表都无所谓。因为它们总有一种操作是 O(1)，另一种是O(N)：数组的插入是O(1)，删除是O(N)；链表则反过来，分别是O(N)和O(1)。然而，要是采用双向链表这一链表的变种，就能使队列的插入和删除都为O(1)。</p>
<p>双向链表跟链表差不多，只是它每个结点都含有两个链——一个指向下一结点，另一个指向前一结点。此外，它还能直接访问第一个和最后一个结点。</p>
<p>由于双向链表总会记住第一个和最后一个结点，因此能够一步（以O(1)的时间）访问到它们。更进一步地，在末尾插入数据也可以一步完成
<img src="@source/docs/theme-reco/img/common/image-20210928163222321.png" alt="image-20210928163222321"></p>
<p>因为双向链表能直接访问前端和末端的结点，所以在两端插入的效率都为O(1)，在两端删除的效率也为 O(1)。由于在末尾插入和在开头删除都能在 O(1)的时间内完成，因此拿双向链表作为队列的底层数据结构就最好不过了。</p>
<h2 id="◆-第12章-让一切操作都更快的二叉树" tabindex="-1"><a class="header-anchor" href="#◆-第12章-让一切操作都更快的二叉树" aria-hidden="true">#</a> ◆ 第12章 让一切操作都更快的二叉树</h2>
<p>当数组有序时，运用二分查找就能以O(log N)的时间复杂度找出任意值的所在位置。</p>
<p>有序数组的插入和删除是缓慢的。往有序数组中插入一个值前，你得将所有大于它的元素右移一格。从有序数组中删除一个值后，你得将所有大于它的元素左移一格。最坏情况下（插入或删除发生在数组开头）这会需要N步，平均情况则是N / 2步。不管怎样，都是O(N)的效率，而O(N)算是挺慢的。</p>
<p>散列表能以O(1)的效率进行查找、插入和删除，但它又有另一明显的不足：不保持顺序。</p>
<p>既要保持顺序，又要快速查找、插入和删除，看来有序数组和散列表都不行。</p>
<h3 id="◆-12-1-二叉树" tabindex="-1"><a class="header-anchor" href="#◆-12-1-二叉树" aria-hidden="true">#</a> ◆ 12.1 二叉树</h3>
<p>一个普通的链表里，每一个结点会包含一个连接自身和另一结点的链。树也是基于结点的数据结构，但树里面的每个结点，可以含有多个链分别指向其他多个结点。</p>
<p>二叉树是一种遵守以下规则的树。
❏ 每个结点的子结点数量可为0、1、2。
❏ 如果有两个子结点，则其中一个子结点的值必须小于父结点，另一个子结点的值必须大于父结点。
<img src="@source/docs/theme-reco/img/common/image-20210928164325642.png" alt="image-20210928164325642"></p>
<p>注意，小于父结点的子结点用左箭头来表示，大于父结点的子结点则用右箭头来表示。</p>
<p><img src="@source/docs/theme-reco/img/common/image-20210928163849663.png" alt="image-20210928163849663">
谈论树的时候，我们会用到以下术语。
❏ 最上面的那一结点（此例中的“j”）被称为根。是的，图中的根位于树的顶端，请自行意会。
❏ 此例中，“j”是“m”和“b”的父结点，反过来，“m”和“b”是“j”的子结点。“m”又是“q”和“z”的父结点，“q”和“z”是“m”的子结点。
❏ 树可以分层。此例中的树有3层。</p>
<h3 id="◆-12-2-查找" tabindex="-1"><a class="header-anchor" href="#◆-12-2-查找" aria-hidden="true">#</a> ◆ 12.2 查找</h3>
<p>二叉树的查找算法先从根结点开始。
(1) 检视该结点的值。
(2) 如果正是所要找的值，太好了！
(3) 如果要找的值小于当前结点的值，则在该结点的左子树查找。
(4) 如果要找的值大于当前结点的值，则在该结点的右子树查找。
<img src="/img/common/image-20210928165855352.png" alt="image-20210928165855352" style="zoom: 33%;" /></p>
<p>推广开来，我们会说二叉树查找的时间复杂度是O(log N)。因为每行进一步，我们就把剩余的结点排除了一半（不过很快就能看到，只在最好情况下，即理想的平衡二叉树才有这样的效率）。再与二分查找比较，它也是每次尝试会排除一半可能性的O(log N)算法，可见二叉树查找跟有序数组的二分查找拥有同样的效率。</p>
<p>要说二叉树哪里比有序数组更亮眼，那应该是插入操作。</p>
<h3 id="◆-12-3-插入" tabindex="-1"><a class="header-anchor" href="#◆-12-3-插入" aria-hidden="true">#</a> ◆ 12.3 插入</h3>
<img src="/img/common/image-20210928170401800.png" alt="image-20210928170401800" style="zoom: 33%;" />
在这个例子里，插入花了5步，包括4步查找和1步插入。插入这1步总是发生在查找之后，所以总共log N + 1步。按照忽略常数的大O来说，就是O(log N)步。
有序数组的插入则是O(N)，因为该过程中除了查找，还得移动大量的元素来给新元素腾出空间。
有序数组查找需要O(log N)，插入需要O(N)，而二叉树都是只要O(logN)。当你估计应用会发生许多数据改动时，这一比较将有助你做出正确选择。
<p>二叉树插入的Python实现，它跟search一样都是递归的
<img src="/img/common/image-20210928170838315.png" alt="image-20210928170838315" style="zoom:33%;" /></p>
<p>只有用随意打乱的数据创建出来的树才有可能是比较平衡的。要是插入的都是已排序的数据，那么这棵树就失衡了，它用起来也会比较低效。比如说，按顺序插入1、2、3、4、5的话，得出的树就会是这样。从中查找5，效率会是O(N)。</p>
<img src="/img/common/image-20210928170900302.png" alt="image-20210928170900302" style="zoom:33%;" />
<p>但要是按3、2、4、1、5的顺序来插入的话，得出的树就是平衡的。
<img src="/img/common/image-20210928170945450.png" alt="image-20210928170945450" style="zoom:33%;" /></p>
<p>假若你要用有序数组里的数据来创建二叉树，最好先把数据洗乱。</p>
<p>在完全失衡的最坏情况下，二叉树的查找需要O(N)。在理想平衡的最好情况下，则是O(log N)。在数据随机插入的一般情况下，因为树也大致平衡，所以查询效率也大约是O(log N)。</p>
<h3 id="◆-12-4-删除" tabindex="-1"><a class="header-anchor" href="#◆-12-4-删除" aria-hidden="true">#</a> ◆ 12.4 删除</h3>
<p>删除是二叉树的各种操作中最麻烦的一个</p>
<p>删除操作遵循以下规则。
❏ 如果要删除的结点没有子结点，那直接删掉它就好。
❏ 如果要删除的结点有一个子结点，那删掉它之后，还要将子结点填到被删除结点的位置上。</p>
<p>❏ 如果要删除的结点有两个子结点，则将该结点替换成其后继结点。一个结点的后继结点，就是所有比被删除结点大的子结点中，最小的那个。</p>
<p>（1）那计算机是怎么找出后继结点的呢？这是有算法可循的。跳到被删除结点的右子结点，然后一路只往左子结点上跳，直到没有左子结点为止，则所停留的结点就是被删除节点的后继结点。
<img src="/img/common/image-20210928171346318.png" alt="image-20210928171346318" style="zoom:33%;" />
（2）来看一个更复杂的删除，这次我们删除根结点。
<img src="/img/common/image-20210928171506571.png" alt="image-20210928171506571" style="zoom:33%;" /></p>
<p>现在需要找后继结点来填补根的位置。首先，访问右子结点，然后一路往左下方向移步，直至没有左子结点的结点上。</p>
<p>（3）然而，还有一种情况我们没遇到过，那就是后继结点带有右子结点。让我们回到根被删除之前的状态，并且给52加上一个右子结点。</p>
<p>如果后继结点带有右子结点，则在后继结点填补被删除结点以后，用此右子结点替代后继结点的父节点的左子结点。</p>
<p>以下为二叉树的删除算法的所有规则。
❏ 如果要删除的结点没有子结点，那直接删掉它就好。
❏ 如果要删除的结点有一个子结点，那删掉它之后，还要将子结点填到被删除结点的位置上。
❏ 如果要删除的结点有两个子结点，则将该结点替换成其后继结点。一个结点的后继结点，就是所有比被删除结点大的子结点中，最小的那个。
​		■ 如果后继结点带有右子结点，则在后继结点填补被删除结点以后，用此右子结点替代后继结点的父节点的左子结点。</p>
<p>跟查找和插入一样，平均情况下二叉树的删除效率也是O(log N)。因为删除包括一次查找，以及少量额外的步骤去处理悬空的子结点。有序数组的删除则由于需要左移元素去填补被删除元素产生的空隙，最终导致O(N)的时间复杂度。
以下是用Python写的二叉树递归式删除算法。为了易于理解，安插了一些注释进去。
<img src="/img/common/image-20210928172047713.png" alt="image-20210928172047713" style="zoom: 33%;" />
<img src="/img/common/image-20210928172121220.png" alt="image-20210928172121220" style="zoom:33%;" />
<img src="/img/common/image-20210928172142083.png" alt="image-20210928172142083" style="zoom:33%;" /></p>
<h3 id="◆-12-5-二叉树实战" tabindex="-1"><a class="header-anchor" href="#◆-12-5-二叉树实战" aria-hidden="true">#</a> ◆ 12.5 二叉树实战</h3>
<p>二叉树在查找、插入和删除上引以为傲的O(log N)效率，使其成为了存储和修改有序数据的一大利器。它尤其适用于需要经常改动的数据，虽然在查找上它跟有序数组不相伯仲，但在插入和删除方面，它迅速得多。</p>
<p>比如说你正在做一个书目维护的应用，它需要具备以下功能。
​		❏ 该应用可以将书名依照字母序打印。
​		❏ 该应用可以持续更新书目。
​		❏ 该应用可以让用户从书目中搜索书名。</p>
<p>如果你预期该书目不常变动的话，那么用有序数组作为存储结构是可以的。但这个应用偏偏要经常实时更新数据。要是其中包含上百万册图书，那还是用二叉树来保存比较好。
<img src="@source/docs/theme-reco/img/common/image-20210928172741169.png" alt="image-20210928172741169"></p>
<p>访问数据结构中所有元素的过程，叫作遍历数据结构。</p>
<p>为了使书名以字母序打印，我们得确保遍历也是以字母序进行。虽然有多种方法可以遍历树，但对于这个要求字母序打印的应用，我们采用中序遍历。
递归是实施中序遍历的有力工具。我们将创建一个名为traverse的递归函数，它可以在任一结点上调用。然后执行以下步骤。
​		(1) 如果此结点有左子结点，则在左子结点上调用自身（traverse）。
​		(2) 访问此结点（对于书目应用来说，就是打印结点的值）。
​		(3) 如果此结点有右子结点，则在右子结点上调用自身（traverse）。
若当前结点没有子结点，则意味着该递归算法到达了基准情形，这时我们无须再调用traverse，只需打印结点中的书名就行了。
<img src="@source/docs/theme-reco/img/common/image-20210928173010807.png" alt="image-20210928173010807">
<img src="@source/docs/theme-reco/img/common/image-20210928173041783.png" alt="image-20210928173041783"></p>
<h3 id="◆-12-6-总结" tabindex="-1"><a class="header-anchor" href="#◆-12-6-总结" aria-hidden="true">#</a> ◆ 12.6 总结</h3>
<p>二叉树是一种强大的基于结点的数据结构，它既能维持元素的顺序，又能快速地查找、插入和删除。</p>
<p>值得一提的是，树形的数据结构除了二叉树以外还有很多种，包括堆、B树、红黑树、2-3-4树等。它们也各有自己适用的场景。</p>
<p>另一种基于结点的数据结构——图。图是社交网络和地图软件等复杂应用的核心组成部分，强大且灵活。</p>
<h2 id="◆-第13章-连接万物的图" tabindex="-1"><a class="header-anchor" href="#◆-第13章-连接万物的图" aria-hidden="true">#</a> ◆ 第13章 连接万物的图</h2>
<p>假设我们正在打造一个像Facebook那样的社交网络。在该应用里，大家可以加别人为“朋友”。这种朋友关系是相互的，如果Alice是Bob的朋友，那么Bob也会是Alice的朋友。这些关系数据要怎么管理才好呢？</p>
<p>一种简单的方法是，以二维数组来保存每一对关系。</p>
<p>由于数据以这种结构存储，若想查找Alice的朋友就得检查数据库中的所有关系，需要O(N)的时间复杂度。</p>
<p>使用图这种数据结构的话，我们可以在O(1)时间内找出Alice的所有朋友。</p>
<h3 id="◆-13-1-图" tabindex="-1"><a class="header-anchor" href="#◆-13-1-图" aria-hidden="true">#</a> ◆ 13.1 图</h3>
<p>图是一种善于处理关系型数据的数据结构，使用它可以很轻松地表示数据之间是如何关联的。</p>
<p>每个人都是一个结点，人与人之间的朋友关系则以线段表示。按照图的术语来说，每个结点都是一个顶点，每条线段都是一条边。当两个顶点通过一条边联系在一起时，我们会说这两个顶点是相邻的。</p>
<p>图的实现形式有很多，最简单的方法之一就是用散列表，因为从散列表里查找一个键所对应的值只需要1步，所以查找Alice的朋友能以O(1)的时间复杂度完成。
<img src="/img/common/image-20210928173409182.png" alt="image-20210928173409182" style="zoom:50%;" /></p>
<p>跟Facebook不同，Twitter里面的关系不是相互的。Alice可以关注Bob，但Bob不一定要关注Alice。让我们构造一个新的图来表示谁关注了谁。
<img src="/img/common/image-20210928173522960.png" alt="image-20210928173522960" style="zoom:50%;" />
图中箭头表示了关系的方向。Alice关注了Bob和Cynthia，但没有人关注Alice。Bob和Cynthia互相关注。
<img src="/img/common/image-20210928173606958.png" alt="image-20210928173606958" style="zoom:33%;" />
Twitter中的关系是单向的，我们也在图中用箭头表示了其方向，因此它的图是有向图。Facebook中的关系则是相互的，我们只画成了普通的线段，它的图是无向图。</p>
<h3 id="◆-13-2-广度优先搜索" tabindex="-1"><a class="header-anchor" href="#◆-13-2-广度优先搜索" aria-hidden="true">#</a> ◆ 13.2 广度优先搜索</h3>
<p>LinkedIn也是一个流行的社交网络，其专注于职业社交。LinkedIn的一个有名的功能就是，你除了能够看到自己直接添加的联系人，还可以发掘你的二度、三度联系人。</p>
<p>Alice能直接联系到Bob, Bob能直接联系到Cynthia。但Alice无法直接联系到Cynthia。由于她们之间的联系要经过Bob，因此Cynthia是Alice的二度联系人。</p>
<p>图有两种经典的遍历方式：广度优先搜索和深度优先搜索。</p>
<p>广度优先搜索算法需要用队列（参见第8章）来记录后续要处理哪些顶点。该队列最初只含有起步的顶点（对本例来说，就是Alice）。</p>
<p>接着按照以下3步去做。
(1) 找出当前顶点的所有邻接点。如果有哪个是没访问过的，就把它标为“已访问”，并且将它入队。（尽管该顶点并未作为“当前顶点”被访问过。）
(2) 如果当前顶点没有未访问的邻接点，且队列不为空，那就再从队列中移出一个顶点作为当前顶点。
(3) 如果当前顶点没有未访问的邻接点，且队列里也没有其他顶点，那么算法完成。</p>
<p>将算法的步骤分为两类之后，我们可以看出图的广度优先搜索的效率。
❏ 让顶点出队，将其设为当前顶点。
❏ 访问每个顶点的邻接点。这样看来，每个顶点都会有一次出队的经历。以大O记法表示，就是O(V)，意思是有V个顶点，就有V次出队。</p>
<p>既然要处理N个顶点，不应该表示为O(N)吗？不是的，因为在此算法（以及很多其他图的算法）中，除了处理顶点本身，还得处理边，因为广度优先搜索有O(V)次出队，还有O(E)次访问，所以我们说它的效率为O(V +E)。</p>
<h3 id="◆-13-3-图数据库" tabindex="-1"><a class="header-anchor" href="#◆-13-3-图数据库" aria-hidden="true">#</a> ◆ 13.3 图数据库</h3>
<p>因为图擅长处理关系信息，所以有些数据库就以图的形式来存储数据。传统的关系型数据库（以行和列的形式保存数据的数据库）也能存储这类信息，但是需要多个表，查找起来很麻烦，复杂度很高。</p>
<p>后端为图数据库时，一旦在数据库中定位到Cindy，那么只需一步就能查到她任一朋友的信息。因为数据库中的每个顶点已经包含了该用户的所有信息，所以你只需遍历那些连接Cindy与朋友的边即可。</p>
<p>用图数据库的话，有N个朋友就需要O(N)步去获取他们的数据。与关系型数据库的O(M log N)相比，确实是极大的效率提升。</p>
<p>Neo4j是开源的图数据库中比较受欢迎的一个。</p>
<p>其他开源的图数据库还有ArangoDB和Apache Giraph。</p>
<h3 id="◆-13-4-加权图" tabindex="-1"><a class="header-anchor" href="#◆-13-4-加权图" aria-hidden="true">#</a> ◆ 13.4 加权图</h3>
<p>还有一种图叫作加权图。它跟普通的图类似，但边上带有信息。</p>
<p>包含了美国几个主要城市的简陋地图，就是一个加权图。
<img src="/img/common/image-20210928175241584.png" alt="image-20210928175241584" style="zoom: 50%;" /></p>
<p>此图中，每条边上都有一个数字，它表示那条边所连接的两个城市相距多少英里。例如，Chicago和New York City之间的距离为714英里。加权图可以是有方向的。以下图为例，尽管从Dallas飞到Toronto只要138美元，但从Toronto飞到Dallas要216美元。</p>
<p>我们可以借助加权图来解决最短路径问题。</p>
<p>假设我目前身在Atlanta，想飞去El Paso。不幸的是，现在没有直达航班。然而，我也可以在其他城市转机过去。例如，先从Atlanta到Denver，再从Denver到ElPaso。这会花费300美元。但再看仔细一点，你会发现从Atlanta沿Denver、Chicago再到El Paso会更加便宜。虽然多转一次，但只需花280美元。这就是一种最短路径问题：如何以最低的价钱从Atlanta飞往El Paso。
<img src="/img/common/image-20210928175419601.png" alt="image-20210928175419601" style="zoom:50%;" />
<img src="/img/common/image-20210928175442876.png" alt="image-20210928175442876" style="zoom:50%;" /></p>
<h3 id="◆-13-5-dijkstra算法" tabindex="-1"><a class="header-anchor" href="#◆-13-5-dijkstra算法" aria-hidden="true">#</a> ◆ 13.5 Dijkstra算法</h3>
<p>解决最短路径问题的算法有好几种，其中一种有趣的算法是由Edsger Dijkstra（念为“dike' struh”）于1959年发现的。该算法也很自然地被称为Dijkstra算法。</p>
<p>Dijkstra算法的规则如下（别担心，之后我们跟着例子运行一遍就会更明白了）。(1) 以起步的顶点为当前顶点。(2) 检查当前顶点的所有邻接点，计算起点到所有已知顶点的权重，并记录下来。(3) 从未访问过（未曾作为当前顶点）的邻接点中，选取一个起点能到达的总权重最小的顶点，作为下一个当前顶点。(4) 重复前3步，直至图中所有顶点都被访问过。
注意：有需要的话需要详细看书，里面有完整流程</p>
<h3 id="◆-13-6-总结" tabindex="-1"><a class="header-anchor" href="#◆-13-6-总结" aria-hidden="true">#</a> ◆ 13.6 总结</h3>
<p>图是处理关系型数据的强大工具，它除了能让代码跑得更快，还能帮忙解决一些复杂的问题。</p>
<p>以时间和算法的步数来衡量代码的性能。性能的衡量方法不止这些。在某些情况下，还有比速度更重要的东西，比如我们可能更关心一种数据结构或算法会消耗多少内存。</p>
<h2 id="◆-第14章-对付空间限制" tabindex="-1"><a class="header-anchor" href="#◆-第14章-对付空间限制" aria-hidden="true">#</a> ◆ 第14章 对付空间限制</h2>
<p>当内存有限时，空间复杂度便会成为选择算法的一个重要的参考因素。比如说，在给小内存的小型设备写程序时，或是处理一些会迅速占满大内存的大数据时都会考虑空间复杂度。既省时又省内存的算法当然是最理想的。但有些情况下我们却只能二者选其一，这时要想做出正确选择，就得仔细分析了。</p>
<h3 id="◆-14-1-描述空间复杂度的大o记法" tabindex="-1"><a class="header-anchor" href="#◆-14-1-描述空间复杂度的大o记法" aria-hidden="true">#</a> ◆ 14.1 描述空间复杂度的大O记法</h3>
<p>O记法来描述一个算法的速度：当所处理的数据有 N个元素时，该算法所需的步数相对于元素数量是多少。例如，O(N)算法就是处理 N个元素需要 N步的算法。O(N2)算法就是处理N个元素需要N2步的算法。</p>
<p>大O来描述一个算法需要多少空间：当所处理的数据有N个元素时,该算法还需额外消耗多少元素大小的内存空间。</p>
<p>分析该函数的话，你会发现它接收一个 N元素的数组，就会产生另一个新的 N元素数组。因此，我们会说这个makeUpperCase函数的空间效率是O(N)。</p>
<p>这第二个版本里，我们没有创建任何新的变量或新的数组，也确实没有消耗额外的内存空间。我们只是变动了原array里的每个字符串，将它们逐一换成大写。最后返回这个修改过的array。因为该函数并不消耗额外的内存空间，所以我们把它的空间复杂度描述为O(1)。记住，时间复杂度的O(1)意味着一个算法无论处理多少数据，其速度恒定。相似地，空间复杂度的O(1)则意味着一个算法无论处理多少数据，其消耗的内存恒定。</p>
<p>空间复杂度是根据额外需要的内存空间（也叫辅助空间）来算的，也就是说原本的数据不纳入计算。尽管在第二个版本里我们有array这一入参，占用了N个元素的空间，但除此之外它并没有消耗额外的内存，所以它是O(1)。</p>
<h3 id="◆-14-2-时间和空间之间的权衡" tabindex="-1"><a class="header-anchor" href="#◆-14-2-时间和空间之间的权衡" aria-hidden="true">#</a> ◆ 14.2 时间和空间之间的权衡</h3>
<p>如果你想要程序跑得超级快，而且你的内存十分充足，那么用第二版会比较好。但如果你不看重速度，而且你的程序是跑在需要谨慎使用内存的嵌入式系统上，那你应该选择第一版。所有技术讨论都是这样的，当需要做出取舍时，你应从全局看待问题。</p>
<h3 id="◆-14-3-写在最后的话" tabindex="-1"><a class="header-anchor" href="#◆-14-3-写在最后的话" aria-hidden="true">#</a> ◆ 14.3 写在最后的话</h3>
<p>我希望你能通过本书明白一个道理：很多看似复杂、深奥的事物，其实都是由你所掌握的简单概念构筑而成的。不要因为某些资料没解释到位，就以为它很困难而被吓退，你一定能找到更详尽的解释资料。</p>
</div></template>



<template><div><h1 id="构建高性能web站点" tabindex="-1"><a class="header-anchor" href="#构建高性能web站点" aria-hidden="true">#</a> 构建高性能Web站点</h1>
<p>郭欣著</p>
<h2 id="第1章-绪论" tabindex="-1"><a class="header-anchor" href="#第1章-绪论" aria-hidden="true">#</a> 第1章 绪论</h2>
<h3 id="◆-1-1-等待的真相" tabindex="-1"><a class="header-anchor" href="#◆-1-1-等待的真相" aria-hidden="true">#</a> ◆ 1.1 等待的真相</h3>
<blockquote>
<blockquote>
<p>（1）等待的时间都包括哪些？</p>
<p>● 数据在网络上传输的时间</p>
<p>● 站点服务器处理请求并生成回应数据的时间</p>
<p>● 浏览器本地计算和渲染的时间</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>数据在网络上传输的时间总的来说包括两部分，即浏览器端主机发出的请求数据经过网络到达服务器的时间，以及服务器的回应数据经过网络回到浏览器端主机的时间。这两部分时间都可以视为某一大小的数据从某主机开始发送一直到另一端主机全部接收所消耗的总时间，我们称它为响应时间，它的决定因素主要包括发送的数据量和网络带宽。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>站点服务器处理请求并生成回应数据的时间主要消耗在服务器端，包括非常多的环节，我们一般用另一个指标来衡量这部分时间，即每秒处理请求数，也称吞吐率，注意这里的吞吐率不是指单位时间处理的数据量，而是请求数。影响服务器吞吐率的因素非常多，比如服务器的并发策略、I/O模型、I/O性能、CPU核数等，当然也包括应用程序本身的逻辑复杂度等。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>浏览器本地计算和渲染的时间自然消耗在浏览器端，它依赖的因素包括浏览器采用的并发策略、样式渲染方式、脚本解释器的性能、页面大小、页面组件的数量、页面组件缓存状况、页面组件域名分布以及域名DNS解析等，并且其中一些因素随着各厂商浏览器版本的不同而略有变化。</p>
</blockquote>
</blockquote>
<h3 id="◆-1-2-瓶颈在哪里" tabindex="-1"><a class="header-anchor" href="#◆-1-2-瓶颈在哪里" aria-hidden="true">#</a> ◆ 1.2 瓶颈在哪里</h3>
<blockquote>
<blockquote>
<p>系统性能的瓶颈，是指影响性能的关键因素，这个关键因素随着系统的运行又会发生不断的变化或迁移</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>中医是一门关于生命的哲学，也是中国人智慧的结晶，它的光芒在于独到的思辨能力和系统性的分析方法，它认为世间万物都在不停地变化，并赋予它们阴阳状态，包括天地、季节、天气、心理、生理等，而患者的病理也在随之变化，所以，中医会对同一位患者在不同季节进行不同的诊断，找到不同的病因。</p>
</blockquote>
</blockquote>
<h3 id="◆-1-3-增加带宽" tabindex="-1"><a class="header-anchor" href="#◆-1-3-增加带宽" aria-hidden="true">#</a> ◆ 1.3 增加带宽</h3>
<blockquote>
<blockquote>
<p>当Web站点的网页或组件的下载速度变慢时，一些架构师可能想到的最省事的办法就是增加服务器带宽，因为他们认为是服务器带宽不够用了，对于一些以提供下载服务为主的站点来说也许是这样的，但是对于其他服务的站点，你知道站点当前究竟使用了多少带宽吗？这些带宽都用到哪里了呢？如何计算站点现在和可预见未来使用的带宽？带宽增加后下载速度就可以加快吗？使用独享带宽和共享带宽的本质区别是什么？如何节省带宽？还有，你可能会忍无可忍地问，究竟什么是带宽？</p>
</blockquote>
</blockquote>
<h3 id="◆-1-4-减少网页中的http请求" tabindex="-1"><a class="header-anchor" href="#◆-1-4-减少网页中的http请求" aria-hidden="true">#</a> ◆ 1.4 减少网页中的HTTP请求</h3>
<blockquote>
<blockquote>
<p>Web站点中几乎任何一个网页都包含了多个组件，每个组件都需要下载、计算或渲染，毫无疑问，这些行为都会消耗时间。那么如果可以让网页减少这些行为，应该就可以加快网页的展示速度，这是毫无疑问的，但是往往我们需要在优雅的网页表现和性能之间权衡取舍，这也许是美和快之间的博弈，找到最优的均衡点至关重要，我们为此做了很多尝试和努力：</p>
<p>● 设计更加简单的网页，使其包含较少的图片和脚本，但是这可能牺牲了美观和用户交互。</p>
<p>● 将多个图片合并为一个文件，利用CSS背景图片的偏移技术呈现在网页中，避免了多个图片的下载。</p>
<p>● 合并JavaScript脚本或者CSS样式表。</p>
<p>● 充分利用HTTP中的浏览器端Cache策略，减少重复下载。</p>
</blockquote>
</blockquote>
<h3 id="◆-1-5-加快服务器脚本计算速度" tabindex="-1"><a class="header-anchor" href="#◆-1-5-加快服务器脚本计算速度" aria-hidden="true">#</a> ◆ 1.5 加快服务器脚本计算速度</h3>
<blockquote>
<blockquote>
<p>用这些脚本语言编写的程序文件需要通过相应的脚本解释器进行解释后生成中间代码，然后依托在解释器的运行环境中运行。所以生成中间代码的这部分时间又成为大家为获取性能提升而瞄准的一个目标，对于一些拥有较强商业支持的脚本语言，比如ASP．NET和JSP，均有内置的优化方案，比如解释器对某个脚本程序第一次解释的时候，将中间代码缓存起来，以供下次直接使用。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对于开源类的脚本语言，也有很多第三方组件来提供此类功能，比如PHP的APC组件等。</p>
</blockquote>
</blockquote>
<h3 id="◆-1-7-使用数据缓存" tabindex="-1"><a class="header-anchor" href="#◆-1-7-使用数据缓存" aria-hidden="true">#</a> ◆ 1.7 使用数据缓存</h3>
<blockquote>
<blockquote>
<p>我们意识到在自己的站点中，某些动态内容的计算时间其实主要消耗在一些烦人的特殊数据上，这些数据或者更新过于频繁，或者消耗大量的I/O等待时间，比如对关系数据库中某字段的频繁更新和读取，这时我们为了提高缓存的灵活性和命中率，以及性能的要求，便开始考虑数据缓存。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>更加细粒度的数据缓存避免了过期时大量相关网页的整体更新，比如很多动态内容都包含了一段公用的数据，如果我们将整个页面全部缓存，那么假如这段数据频繁更新导致频繁过期，无疑会使得所有网页都要频繁地重建缓存，这对网页的其他部分内容似乎很不公平。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>将数据缓存存储在哪里呢？这需要考虑多方面的因素。速度是一方面，如果无法提供高速的读写访问，那么这部分数据缓存可能不久便成为新的系统瓶颈。另外，数据缓存的共享也至关重要，如同一主机上不同进程间的共享、网络上不同主机间的共享等，一旦设计不当，将对站点未来的规模扩展带来致命的威胁。</p>
</blockquote>
</blockquote>
<h3 id="◆-1-9-更换web服务器软件" tabindex="-1"><a class="header-anchor" href="#◆-1-9-更换web服务器软件" aria-hidden="true">#</a> ◆ 1.9 更换Web服务器软件</h3>
<blockquote>
<blockquote>
<p>开始影响全球经济的开源软件，不可否认给我们的生活带来了更多丰富的体验和选择，但是更多的选择也代表着更多的结局，不论结局是好是坏，我们都需要为此承担责任。在Web服务器软件的选择问题上，很多架构师依然困惑，大量的压力测试对比数据蛊惑着激进的开发者和运维工程师，人们只关注所谓的并发量冠军，却忽视了更加本质性的东西，甚至不了解眼前测试数据的潜在前提。社会总是这样的，象牙塔式的精英教育和残酷的淘汰机制断送了无数人才的未来。而这一次，错误的选择将要付出沉痛的代价。有人拿着所谓的测试数据说Apache已经过时，你相信吗？也许下此结论为时尚早，尽管放弃它的人比比皆是，但是它的成功不是空穴来风，毕竟它已经活了很久了。</p>
</blockquote>
</blockquote>
<h3 id="◆-1-10-页面组件分离" tabindex="-1"><a class="header-anchor" href="#◆-1-10-页面组件分离" aria-hidden="true">#</a> ◆ 1.10 页面组件分离</h3>
<blockquote>
<blockquote>
<p>在Web站点中，网页和各种各样的组件是否也需要“分班”呢？显然它们的下载量和对服务器的能力要求不尽相同，如果由同一台物理服务器或者同一种并发策略的Web服务器软件来统一提供服务，那势必造成计算资源的浪费以及并发策略的低效。所以，分离带来的好处是显而易见的，那就是可以根据不同组件的需求，比如下载量、文件大小、对服务器各种资源的需求等，有针对性地采用不同的并发策略，并且提供最佳的物理资源。</p>
</blockquote>
</blockquote>
<h3 id="◆-1-11-合理部署服务器" tabindex="-1"><a class="header-anchor" href="#◆-1-11-合理部署服务器" aria-hidden="true">#</a> ◆ 1.11 合理部署服务器</h3>
<blockquote>
<blockquote>
<p>在基于IP寻址的互联网中，IP地址相近的主机之间通信，数据经过少数的路由器即可到达，比如同一局域网内通信或者接入同一个城市交换节点的局域网之间通信，在这种情况下数据到达时间相对较短。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>而如果通信的两端主机位于不同运营商的互联网中，那么数据必须流经两个互联网运营商的顶级交换节点和骨干线路，在这个过程中可想而知数据要经过更多次的存储转发，而且各互联网顶级交换节点之间又存在出口带宽的限制，如果互联网之间数据通信量比较大的话，那么这个顶级交换节点，也就是“出口”，将会是瓶颈所在，就像连接两座城市之间的高速公路，当大量汽车需要频繁地往返于两座城市时，高速公路出现车流缓慢，那么汽车从一个城市到另一个城市的总体时间加长了。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>我们当然希望Web站点的用户和服务器位于同一个互联网运营商的网络内</p>
</blockquote>
</blockquote>
<h3 id="◆-1-12-使用负载均衡" tabindex="-1"><a class="header-anchor" href="#◆-1-12-使用负载均衡" aria-hidden="true">#</a> ◆ 1.12 使用负载均衡</h3>
<blockquote>
<blockquote>
<p>通过各种不同的方法来实现Web负载均衡，可能是简单的HTTP重定向，或者是基于DNS的轮询解析，或者通过反向代理服务器来实现负载均衡调度，还可以通过LVS来组建服务器集群，它们有什么区别呢？无论如何，透过这些具体的实现方法，我们更加关心的是能否真正地均衡调度请求，以及是否具备高可用性，还有影响规模扩展的制约因素</p>
</blockquote>
</blockquote>
<h3 id="◆-1-13-优化数据库" tabindex="-1"><a class="header-anchor" href="#◆-1-13-优化数据库" aria-hidden="true">#</a> ◆ 1.13 优化数据库</h3>
<blockquote>
<blockquote>
<p>不合理的应用程序数据访问组件设计、不合理的数据库表结构设计以及对于数据库内部构造缺乏深入的了解。毫不夸张地说，也许你之前的优化全都白干了。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>Web服务器与数据库服务器的数据通信一般基于标准的TCP，即便它们位于同一台物理主机也是如此。其通信连接的建立和释放涉及代表一段内核高速缓冲区的文件描述符的创建和销毁，这需要不少的时间开销，包括系统调用导致的内核态切换以及某些异步阻塞I/O模型采用的文件描述符队列扫描机制。所以，频繁的数据库连接和释放无疑将导致数据访问等待时间的加长，这段时间浪费得毫无意义。</strong></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>使用数据库持久连接有效地解决了这一难题，它包括不同程度上的持久化，本质的区别在于持久连接的应用范围和生命周期，比如某个进程内部的全局数据库连接，供进程内所有计算任务共享，在这个进程终止后便被释放；或者在某个动态内容的执行周期内，代码层面的持久连接对象，在动态内容计算结束后便不复存在；还有跨进程的数据库连接池，保存多个持久连接供应用程序重复使用。在这些采用数据库持久连接的应用设计中，同时还要注意保证数据访问的线程安全性。</strong></p>
</blockquote>
</blockquote>
<h3 id="◆-1-14-考虑可扩展性" tabindex="-1"><a class="header-anchor" href="#◆-1-14-考虑可扩展性" aria-hidden="true">#</a> ◆ 1.14 考虑可扩展性</h3>
<blockquote>
<blockquote>
<p>可扩展性并不是性能和速度的概念，它是指当系统负载增大时，通过增加资源来提高性能的能力。高性能往往需要通过这种能力来实现快速扩展，几乎没有多少团队可以在一个星期内通过增加服务器马上让服务能力扩容100倍。另一方面，可扩展性的目的在于适应负载的变化，从扩展的技术实现上来看，又包含了很多对局部性能的思考，以及了解何时需要扩展，这离不开对站点性能的把握。</p>
</blockquote>
</blockquote>
<h2 id="第2章-数据的网络传输" tabindex="-1"><a class="header-anchor" href="#第2章-数据的网络传输" aria-hidden="true">#</a> 第2章 数据的网络传输</h2>
<p>了解数据在网络传输中的一些原理和动机吗？比如存储转发、流量控制、带宽和响应时间等</p>
<h3 id="◆-2-1-分层网络模型" tabindex="-1"><a class="header-anchor" href="#◆-2-1-分层网络模型" aria-hidden="true">#</a> ◆ 2.1 分层网络模型</h3>
<blockquote>
<blockquote>
<p>普遍采用OSI七层网络模型或TCP四层网络模型展开介绍</p>
<p>铁路：相当于物理层的线路，它为相互通信的主机建立物理连接，提供信号传输介质。物理层线路常见的传导材料有铜线和光纤，我们常用的双绞线使用的是铜线，价格便宜，它传输的是电信号。而光纤价格昂贵，它传输的是光信号。那么信号在光纤中的传输速度和铜线相比，谁更快呢？我们将在下一节中揭开真相。</p>
<p>火车站：相当于网络中的节点，比如计算机、路由器等，通过节点的网络接口（比如计算机的网卡），内部数据总线与网络物理层线路间接连接。网络接口负责将节点内部总线中的字节数据和线路中传输的二进制信号进行相互转换。</p>
<p>火车站地址：相当于节点物理地址，也就是MAC地址，它由设备生产厂家唯一确定。通过现代电路技术，线路中的数据信号可以很容易传输到指定MAC地址的节点。</p>
<p>火车：相当于数据链路层的数据帧，它包括头部信息和数据区，头部信息指明了目的地节点的MAC地址。火车、火车站、火车站地址等一系列涉及的实体和规则可以统称为数据链路层。</p>
<p>火车站串联方式：相当于总线网络拓扑结构，是比较古老的一种主机连接方式，具体介绍可以查看在线资料。它的缺点之一是数据传输采用广播方式，所以所有主机共享网络带宽，并且在这种情况下，由于线路过长会使信号衰减，所以不能无休止地通过中继器延长总线来增加网络节点，限制了网络的扩展。</p>
<p>火车站调度站方式：相当于星形网络拓扑结构，使用额外的中间节点来交换数据，且中央节点负担较重，但是这种形式控制方便，易于扩展。星形网络的带宽是否共享是由中央节点的处理方式决定的，常见的有集线器和交换机，后文中会有详细介绍。</p>
<p>普通调度站：相当于早期的集线器，它通过简单的集成电路实现了星形拓扑网络结构，但由于其只工作在物理层，所以对于来自某一主机的数据帧并不知道它的目的地是哪里，也就是无法识别MAC地址，所以不加判断地直接将数据帧转发到所有其他端口连接的主机。它的本质和总线网络相同，都是广播方式的数据转发，需要有数据帧碰撞检测机制，所有主机共享整个网络的带宽，容易相互影响。</p>
<p>高级调度站：相当于交换机，它拥有独立的处理器和高速缓存，对数据采用存储转发方式，并且使用更加先进的电路设计，可以对数据帧根据MAC地址进行准确转发，抛弃了传统的广播方式，这就使得网络中的主机通信不会相互影响，大家都可以独享网络带宽。注意，到现在为止，已经有好几处出现了“带宽”这个词，下一节将详细探究带宽的真相。</p>
<p>预约线路：相当于电路交换方式。本来要在故事剧情中砍掉这个角色，因为它在数字通信网络中实在是非主流，不过考虑到它在电话网络中的突出贡献，而且涉及数据帧诞生的历史，所以还是让它在故事中露个脸。正如故事中所讲，我们很容易理解电路交换的局限性，打电话占线就是一个很好的例证，但是在电话网络中为什么还一直在用电路交换方式呢？请注意，打电话和计算机通信是完全不同的应用，需求也完全不同。没有人愿意并且有能力同时跟多个人打电话，就算有这个需求那就安装多部电话吧。电话拨通之前需要建立连接，等待的时间对大家来说正好是心理准备的缓冲时间，完全可以接受。另外，电话之间单位时间传输的数据量是恒定的，就是源源不断流入话筒的声音，就算人不说话，也是有环境声音的。而计算机通信的需求就非常复杂，数据传输突发性强，数据量变化频繁，数据可靠性需要保证，传输速度需要不断提高，所以人们觉得电路交换方式无法满足需求，在当时便设计出分组交换方式，即将数据分成若干小块，称为数据帧，分时依次发送。</p>
<p>火车站车流控制：相当于数据链路层的流量控制。数据链路层作为控制数据帧传输的底层，必须保证到达目的地主机网卡的数据帧可以交付给上层应用，这个交付工作具体来说，就是网卡把接收到的二进制信号转换为字节，然后写入由操作系统内核在内存中拥有的一块高速缓存区，同时通过中断切换到相应的应用程序进程并取走这些数据。有很多原因可以导致这块缓存区某时刻被写满，比如同时下载多个文件使得写入速率超过取走速率，这时候如果数据帧的发送端不减慢发送速率，则很多数据帧到达后，网卡会因为缓存区已满而丢弃这些数据，因为网卡对此确实无能为力。这就需要在数据链路层存在一种流量控制机制，故事中车流控制的目的与此相同，但是实际采用的方法要复杂得多，一般采用的是滑动窗口技术，细节这里就不作介绍，感兴趣的朋友可以查阅在线资料。</p>
<p>火车站货物损坏自动重发：相当于数据链路层的差错控制和选择重发。事实上，对于我们使用的以太网，由于线路质量非常好，数据链路层已经不做差错校验和自动重发，而是将校验及重发控制权交给更上层的协议，比如TCP。而对于802．11x等无线网络以及卫星通信领域，数据链路层还是需要进行大量的数据可靠性校验工作。</p>
<p>城市：相当于拥有同一个广播域的网络，可以理解为局域网。</p>
<p>城市广播站：相当于局域网的广播地址，发往这个地址的数据帧会被发送到局域网内所有的节点。</p>
<p>火车站名称：相当于节点网络接口的IP地址，由用户自行设置或者通过DHCP获得。</p>
<p>火车站地址转换：相当于网卡IP地址到MAC地址的转换。因为数据帧在网络中传输时，只可以通过目的地MAC地址来寻址，所以必须将目的地IP地址转换为MAC地址并告知数据帧。具体的转换方式是：数据发送端通过ARP协议发送广播到局域网内所有的主机，询问大家谁拥有指定的IP地址，如果拥有的话就请告之自己的MAC地址。</p>
<p>火车站地址关系绑定：相当于将局域网内某其他主机网卡的MAC地址和IP地址的对应关系写入ARP表，也称为“ARP绑定”。对于绑定后的目标主机，任何时候发送数据都不需要进行广播询问，而是直接从ARP表中寻找对应关系即可。</p>
<p>城市中转站：相当于路由器，它工作在网络层，可以识别数据帧中携带的目的地IP地址，并将其转发至正确的网络。</p>
<p>城市互联：相当于很多网络通过路由器连接，形成互联网。</p>
<p>货物包：相当于网络层的数据包，它的头部信息包含了IP地址，主要用于数据帧的寻址。</p>
<p>发货人和收货人：相当于建立通信的两个主机上运行的进程，发货人和收货人的详细信息相当于这些进程的网络端口号。</p>
<p>传输包：相当于传输层的数据包，包括头部信息和正文，在头部信息中指明了用端口号来代表的进程，它们实现了真正意义上的两个主机进程之间的网络通信，而很好地屏蔽了底层的传输细节。在故事中，传输包在有些时候表现出了不同的特性，它们代表了某些传输层协议的控制机制，包括流量控制、流量限制、错误重发等。常见的传输层协议包括TCP和UDP。</p>
<p>礼品包、订购包、产品包：相当于应用层的各种数据包，它们根据应用程序之间的协议约定，对数据进行再次封装，包括头部信息和正文，比如HTTP、FTP、SSH等。</p>
</blockquote>
</blockquote>
<h3 id="◆-2-2-带宽" tabindex="-1"><a class="header-anchor" href="#◆-2-2-带宽" aria-hidden="true">#</a> ◆ 2.2 带宽</h3>
<blockquote>
<blockquote>
<p>带宽的单位是“bit/s”，也就是单位时间的比特数，所以他们将带宽解释为数据在线路中的移动速度，也就是将带宽的高低视为线路能力的强弱，那么很显然他们认为光纤对数据的传播能力大于铜线，但很可惜，事实上这是错误的。顺便说一下，我们一般常说的比如100M带宽，全称应该是100Mbit/s，或者100Mbps，后边的“bit/s”经常省略。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>说到数据的发送，也就是数据从主机进入线路的这段旅程，一般需要经过以下几个环节：</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>1.应用程序首先得将要发送的数据写入该进程的内存地址空间中，熟悉网络编程的开发者对这个环节一定非常熟悉，通常在程序开发中这只需要一般的运行时变量赋值即可。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>2.应用程序通过系统函数库接口（比如send函数）向内核发出系统调用，由系统内核来进行随后的操作，它将这些数据从用户态内存区复制到由内核维护的一段称为内核缓冲区的内存地址空间。这块地址空间的大小通常是有限的，所有要发送的数据将以队列的形式进入这里，这些数据可能来自于多个进程，每块数据都有一定的额外记号来标记它们的去向。如果要发送的数据比较多，那么该系统调用需要多次进行，每次复制一定的数据大小，这个大小取决于网络数据包的大小以及内核缓冲区的承载能力。重复的系统调用体现在应用编程层面重复调用send函数。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>3.当数据写入内核缓冲区后，内核会通知网卡控制器前来取数据，同时CPU转而处理其他进程。网卡控制器接到通知后，便根据网卡驱动信息得知对应内核缓冲区的地址，将要发送的数据复制到网卡的缓冲区中。注意在以上一系列的数据复制中，数据始终按照连接两端设备的内部总线宽度来复制，也就是字节的整数倍，比如在32位总线的主机系统中，采用PCI-X总线接口的网卡一般使用32位总线宽度，那么从内核缓冲区到网卡缓冲区的数据复制过程中，任何时刻只能复制32位的比特信息。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>4.网卡缓冲区中的数据需要发送到线路中，同时释放缓冲区来获取更多要发送的数据。但是我们知道，只有二进制的数字信号才可以在线路中传输，所以这时候需要对数据进行字节到位的转换，这种转换不难想象，就是将数据的每个位按照顺序依次发出。</p>
<p>5.发送时，网卡会使用内部特定的物理装置来生成可以传播的各种信号，比如在使用铜线线路时，网卡会根据“0”和“1”的变化产生不同的电信号；而使用光纤线路时，网卡会产生不同的光信号。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>不管是电信号还是光信号，只要进入线路后，便能够进行快速的传播，这个速度称为传播速度，它的单位是“m/s”，即单位时间传播的距离。传播速度只与传播介质有关，铜线中电信号的传播速度大约为2．3×108m/s，光纤中光信号的传播速度大约为2．0×108m/s。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>你也许有些疑问，光的传播速度难道不是3．0×108m/s吗？没错，在真空中的光速大约为3．0×108m/s，同时它不仅仅是可见光的传播速度，也是所有电磁波在真空中的传播速度。根据现代物理学，所有电磁波（包括可见光）在真空中的传播速度是常数，即是光速。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>那为什么光纤中的传播速度要低一些呢？这很容易理解，光纤在传播光信号的时候，利用了光的全反射原理，所以光信号在光纤中的实际传播距离要大于光纤的长度，而我们一般所说的传播速度都是针对光纤的长度。同时，正是由于利用了光的全反射原理，微细的光纤封装在塑料护套中，使得它能够弯曲而不影响光信号的传播。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>不同的传播介质中信号的传播速度几乎等于常量。也就是说，不论数据发送装置以多快的发送速度让数据以信号的形式进入线路，在线路中信号的传播速度几乎可以认为是一样快的。根据电磁学的定律也可以说明这一点，发射电磁波的物体的速度不会影响电磁波的传播速度。结合相对性原则，观察者的参考坐标和发射光波的物体的速度不会影响被测量的光速。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>我们所讲的带宽是指数据的发送速度。比如我们的百兆网卡，便是指网卡的最大发送速度为100Mbps，也就是网卡在1秒钟最多可以发出100Mb的数据。那么，我们当然希望发送速度越快越好，究竟这个发送速度的大小跟什么有关呢？简单说包括以下几个因素：</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>数据发送装置将二进制信号传送至线路的能力，也称信号传输频率，以及另一端的数据接收装置对二进制信号的接收能力，同时也包括线路对传输频率的支持程度。比如光纤一端的发射装置使用发光二极管（Light EmittingDiode，LED）或一束激光将光脉冲传送至光纤，光纤另一端的接收装置使用光敏元件检测脉冲，从而将光脉冲中包含的二进制信息转换成数据。值得注意的是，信号的接收能力至关重要，如果接收能力跟不上，发送能力不可能提高，在星球火车系统的故事中，我们知道数据链路层对于数据帧传输的控制机制完全是按照接收方的接收能力来确定发送速度的。</p>
<p>● 数据传播介质的并行度，这里也可以称为“宽度”，完全等价于计算机系统总线宽度的概念。比如在光纤传输中，我们可以将若干条纤细的光纤并行组成光缆，这样就可以在一个横截面上同时传输多个信号，就像在32位的计算机总线中，可以同一时刻传输32位数据。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>有意思的是，要提高计算机总线的带宽，包括提高总线频率和总线宽度两种方法，比如使用64位总线系统或者使用主频更高的处理器等。这两种方法与以上数字通信带宽的两个决定因素完全相似。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>数字网络通信相比于计算机内部数据传输而言，其传输距离要远远大于后者，所以它还隐藏着另一个因素，那就是信号在传播介质中的衰减，这与传播介质以及信号传播方式有着密切的关系。比如我们一般使用的双绞线，传输距离只能达到100m，更长的距离就需要通过中继器来延续信号，而且只能使用有限次的中继，每次中继器转发信号又会消耗一些发送时间。而光纤中光的传导损耗比电在电线传导的损耗低得多，它的传输距离一般在数千米以上，所以光纤一般被用做长距离的数据传输。随着光纤的价格日渐降低，光纤未来一定会走进千家万户。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>为什么要限制带宽？在大多数情况下，我们都将Web站点服务器托管在IDC，通过将其连接到某个交换机，从而接入互联网。这时候，我们的服务器拥有自己的IP地址，当站点用户通过互联网向这台服务器请求数据后，数据从服务器流经交换机到达指定的路由器，这个过程需要交换机的存储转发机制，也就是交换机从连接服务器的端口接收数据，存储到交换机内部的高速缓冲区队列中，然后将其从连接路由器的端口发送出去，再经过路由器的转发，进入另一个网络，接下来依次重复这些过程，直到进入站点用户的PC。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>如果全世界只有你的服务器和你的用户在传输数据，那么这部分数据流经的每个交换节点都会全心全意地做好转发工作，此时你的数据在各节点转发的发送速度都可以达到理论上设备所能达到的最大值。但实际上每处交换节点都有可能同时转发来自其他主机的数据，包括你的数据在内的所有数据都汇集进入路由器的转发队列，路由器按照转发队列中的顺序来交错地发送这些来自不同主机的数据，所以单从来自不同主机的数据而言，其转发时的发送速度必定小于所有从路由器转发出去的数据的发送速度（即该交换节点的出口带宽）。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>两个被交换机限制了带宽的主机，它们都安装了MRTG，可以生成网卡流量报告单，不过我们在这里关心的不是流量，而是报告单顶部的一段信息，请注意这里的Max Speed属性值，它便是从交换机接收端口获得的最大接收速度，同时也是该主机的最大数据发送速度，但并不一定是此刻的实时发送速度，因为每时每刻的发送速度都是传输协议根据接收方的接收能力不断调整的，比如通过数据链路层或者传输层的滑动窗口技术等流量控制机制进行速度的调整。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>这台主机使用了独享10M带宽，也就是10Mbit/s的数据发送速度，换算成字节的话，正好就是上面的1250．0kBytes/s。在这种情况下，如果路由器的出口带宽为100M，交换机的设置应该保证来自广播域内其他主机的数据发送速度总和不超过90Mbit/s，以保证该主机任何时刻都可以以10Mbit/s的速度发送数据，这才叫独享带宽，它独享的是路由器的一部分出口带宽，而不是交换机的带宽，因为交换机本来就是各个端口独享带宽而互不影响。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>如果这台交换机还为其他主机提供独享10M带宽的接入，并且路由器的出口带宽为100M，那么理论上总共只能接入10台主机，这样才可以保证每台主机的实际带宽总是10M。假设带宽运营商为了多赚钱，给该交换机上接入了20台主机，然后对每台交换机仍然都限制了10M带宽，这时候从MRTG的Max Speed属性上仍然看到的是1250．0 kBytes/s，但是你可以观察MRTG流量图或者使用Nmon实时流量监控来分析自己的10M带宽是否真的有所保证。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>另一台主机使用了共享100M带宽，也就是100Mbit/s的数据发送速度，换算成字节便是12．5MBytes/s。事实上很多普通的服务器托管都采用这种带宽方案，它的价格较低，但是请注意这里的“共享”是指交换机不保证你的主机出口带宽能达到100M，原因很简单，假设交换机为50台主机同时提供共享100M带宽接入，每台主机理论上都能以最高100Mbit/s的速度将数据发送到交换机，但是这些数据汇集到路由器后，都得从路由器的出口转发到其他网络，可是路由器的出口带宽是运营商买来的，假设这里也就100M而已，那这些数据就得在路由器的高速缓冲区中以100Mbit/s的速度转发，那么交换机也要配合这个速度来调整自己的发送速度，同时每台主机也会自动调整发送速度，这些都是由底层传输协议自动完成的，本质是为了防止发送速度大于接收速度导致接收端缓冲区的数据还没来得及处理就被新的数据覆盖。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>每台主机的实际带宽就要根据这些主机的总体通信量来决定了，假如50台主机都要发送大量的数据（大文件下载），那么每台的发送速度大概控制在2Mbit/s左右，也就是250kBytes/s；假如有49台主机在某时刻不发送数据，则剩余的1台主机此刻理论可以达到100Mbit/s的速度，但是一般交换机对于共享带宽都会限制最高峰值，比如限制为10Mbit/s，那么即使某个时刻只有1台主机需要发送数据，交换机也会让它减速。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>这些发送速度（即带宽）都是针对从发送端发出的所有数据而言，但是这些数据往往来自于主机的多个进程，而且也要发向五湖四海不同的目的地，比如北京、上海和深圳的3个用户通过HTTP同时下载主机上的数据文件，这些数据不分你我地夹杂在一起从主机进入路由器，这一过程的发送速度前面已经讲得很清楚了，当它们进入某城市交换节点的时候，大家分道扬镳，分别前往三个目的地，这时从各个目的地的角度而言，单位时间接收到的数据一定比从服务器发出的数据少，这种从最终接收端体验到的速度我们一般称为“下载速度”，也就是我们经常看到在浏览器下载进度条上显示的数值，可见下载速度无论如何都小于发送速度。在刚才的例子中，即便是三个目的地的下载速度总和也一定小于服务器对这些数据的发送速度，</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在一般情况下当你使用普通的家庭宽带接入互联网时，宽带服务商往往会在你接入的交换节点上进行带宽限制，比如你申请安装了1M独享宽带，那么交换节点上连接到你家的端口缓冲区将会以1Mbit/s的速度接收来自服务器的数据，这样你的下载速度将受到限制，但是服务器的数据发送速度基本不变，因为它发出的数据不只是到你一家。</p>
</blockquote>
</blockquote>
<h3 id="◆-2-3-响应时间" tabindex="-1"><a class="header-anchor" href="#◆-2-3-响应时间" aria-hidden="true">#</a> ◆ 2.3 响应时间</h3>
<blockquote>
<blockquote>
<p>所谓的下载速度，就是指单位时间里从服务器到达用户PC的数据量的多少，一般用数据量的字节数多少来描述，所以下载速度的单位为“Bytes/s”。由此可见，计算下载速度的方法是：用传输的数据字节数，除以这些数据从服务器开始发送直到完全到达用户PC的时间。形象地说，这段时间是从数据的第一个比特由服务器网卡进入线路开始，直到最后一个比特位进入用户PC的网卡为止。以上的计算方法在任意长度的时间片段中都成立，如果时间片非常短，那么计算结果就是我们一般说的实时下载速度。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>响应时间 = 发送时间 + 传播时间 + 处理时间，发送时间很容易计算，即“数据量/带宽”。比如要发送100Mbit的数据，而且发送速度为100Mbit/s，也就是带宽为100M，那么发送时间便为1s。值得注意的是，在两台主机之间往往存在多个交换节点，每次的数据转发，都需要花费发送时间，那么总的发送时间也包括这些转发时所花费的发送时间。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>传播时间主要依赖于传播距离，因为传播速度我们可以近似认为约等于2．0×108m/s，那么传播时间便等于传播距离除以传播速度。比如两个交换节点之间线路的长度为1000km，相当于北京到上海的直线距离，那么一个比特信号从线路的一端到另一端的传播时间为0．005s。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>处理时间就是指数据在交换节点中为存储转发而进行一些必要的处理所花费的时间，其中的重要组成部分就是数据在缓冲区队列中排队所花费的时间</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>如果全世界只有你的服务器和你的用户在传输数据，那么用于排队的处理时间可以忽略。可见，处理时间的多少，取决于数据流经各交换节点所在网络的数据通信量，它往往是不可预测的，所以它的计算比较复杂，往往没有一个简单的数学计算公式，而是依赖于多变的外部因素，必须结合实际情况具体分析。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>响应时间 = (数据量比特数 / 带宽) + (传播距离 / 传播速度) + 处理时间</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>下载速度的计算公式如下：下载速度 = 数据量字节数 / 响应时间</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>一般我们习惯将比特的单位bit缩写为b，而将字节的单位Byte缩写为B，那么显然1B=8b。同时，比特和字节的换算也不一样，约定如下：1KB = 2<sup>10</sup>B = 1024B</p>
<p>1kb = 10<sup>3</sup>b = 1000b</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在以下的计算中为了方便，我们将认为：1KB ≈ 10<sup>3</sup>B = 1000B</p>
<p>这样引发的误差对于响应时间的计算结果来说微不足道。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>假设这样一个场景，我们的Web服务器托管在北京的某互联网数据中心（IDC），以10M独享带宽的方式接入互联网。位于西安的一位用户通过小区提供的1M独享带宽的方式接入互联网，他通过PC的浏览器下载该Web服务器上的一个100MB大小的文件，换算成比特也就是800Mb，响应时间和下载速度是多少呢？</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在计算之前，我们首先得清楚数据在整个传输过程中流过的路径，也就是经过了多少交换节点，这些节点分别提供多大的带宽。要知道数据流过的路径，一般我们使用Linux的traceroute命令或者Windows的tracert命令，它通过IP包头部的TTL数值，在默认情况下分别发送40字节的测试数据到沿路的每一个交换节点，以此来追踪数据经过的路由路径，同时测算数据到达每一个交换节点的响应时间。但是由于40字节的默认测试数据量比较小，最多也只能设置几KB，而且这种测算机制的实现容易受网络不确定因素影响，所以它的测算结果一般用于检测故障，而对于我们这里计算整个过程的响应时间意义不大，我们的目的是通过前面基于带宽的计算模型来了解响应时间在理论上的决定因素，并希望以此来帮助我们认识到影响下载速度的关键环节。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>为了在这个具体的场景中充分说明时间的花费，我们先假设服务器和用户PC之间的交换节点很少，然后逐步增加。首先，假设Web服务器直接连接在交换节点A，那么从Web服务器到交换节点A，因为使用了10M独享带宽，所以数据发送速度为10Mbit/s，所以这部分发送时间为：800Mbit / (10Mbit/s) = 80s然后，用户的接入方式为1M独享带宽，所以用户PC接入的交换节点B到用户PC的发送速度为1Mbit/s，所以这部分发送时间为：800Mbit / (1Mbit/s) = 800s</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>而至于以上两个交换节点A和B，我们假设它们都是所在城市的城域网顶级节点，而且它们直接通过光缆相连，带宽假设为40G，那么这部分发送时间为：800Mbit / (40Gbit/s) = 0．02s以上的发送时间相加就是总的发送时间。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>接下来，我们计算传播时间，由于北京到西安的高速公路距离大约为1000km，所以我们假设Web服务器到用户PC的线路距离总和为1000km，传播速度我们统一按照2．0×10<sup>8</sup>m/s计算，那么传播时间为：1000km / (2．0×10<sup>8</sup>m/s) = 0．005s这样一来，忽略处理时间，响应时间便为：80s + 800s + 0．02s + 0．005s = 880．025s</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>而下载速度便为：100MByte / 880．025s = 113．63KB/s</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>但这并不是实际的情况，因为我们忽略了太多其他的交换节点。下面我们考虑增加一些交换节点，正是这些节点使得这100MB的文件数据流经各个网络后抵达用户PC，因为前面提到的两个交换节点A和B之间几乎不可能直接用光缆连接，而是通过一系列的交换节点层层相连。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>继续假设，Web服务器直接连接的交换节点A并不是北京的顶级节点，而是服务器所在机柜的交换节点，它还连接着更高一级的交换节点C。我们假设节点A的出口带宽为100M，事实上在IDC的一些机柜正是使用100M的出口带宽，为机柜内的多台服务器提供独享或共享带宽，而这100M带宽则是代理商从上一级交换节点C那里花钱买来的。那么，从节点A到节点C增加的这部分发送时间为：800Mbit / (100Mbit/s) = 8s</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>然而，在数据流经节点C之后，假设还经过了节点D和节点E，才到达了北京的顶级节点。节点D和节点E可以认为是IDC中用于组建网络而设置的节点，因为服务器实在太多了，必须划分多个子网才能够接入网络，实际上有些IDC可能由于设计不良或者历史问题，内部要经过更多的交换节点转发，这可以通过traceroute命令来跟踪了解。为了给IDC中各服务器提供承诺的带宽，我们假设节点C、节点D和节点E之间采用光纤连接，出口带宽分别为1G和10G，而现实中这也毫不夸张。那么这两部分发送时间为：800Mbit / (1Gbit/s) + 800Mbit / (10Gbit/s) = 0．88s这样一来，响应时间增加到了888．905s，下载速度下降到112．50KB/s。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>实际上，当数据到达北京的顶级节点后，并不是直接转发到西安的顶级节点，而是可能要经过更多的国家基础节点，比如省网核心节点以及国家骨干网络核心节点等，但是这些节点的带宽可以认为至少都在10G级别之上。而当数据到达西安顶级节点后，又要经过多个节点才能到达连接用户PC的交换节点B，这些交换节点都是国家的重点基础设施，带宽多数都在千兆以上，所以这部分增加的发送时间就像前面增加的8．88s一样，对于下载速度的影响不值一提，所以我们暂且忽略不计。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>可见，即使增加了交换节点的数据转发次数，但是如果这些节点可以保证较高带宽的话，那么对于最终下载速度的影响其实微不足道，这也就是为什么在一些基础节点以及骨干节点必须要使用较高带宽，比如某IDC采用40G出口带宽直接连接国家计算机网络核心路由器节点。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>那么，这个112．50KB/s的下载速度真的就是实际速度吗？绝对不是。不要忘了我们在以上的计算过程中忽略了很多可能没有提到过的非理想因素，这些因素在实际环境下是不可避免的，它们将导致实际下载速度必然或多或少地低于112．50KB/s。注意这里说的实际速度，其实就是我们用浏览器或其他下载工具时看到的下载速度值。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>这些非理想因素主要包括：</p>
<p>● 在共享带宽以及网络通信数据量过大时，交换节点中数据在转发队列中的等待时间便不能忽略，这部分时间往往也是影响下载速度的重要部分。大家应该都有亲身体验，比如几台PC通过同一宽带路由器上网，如果其中一台PC正在持续下载电影，那么其他几台PC的下载速度必然受到较大的影响。</p>
<p>● 有时候我们下载的数据需要直接写入磁盘，比如下载几百兆的文件到某文件目录下。我们知道，PC网卡接收到的数据进入内存后，便完成了数据接收，但是下载进程一般要将这些数据即时写入磁盘后，才会进行下一次接收数据的系统调用，而写入磁盘的过程需要经历位于内核态内存中磁盘高速缓冲区的转发，而且有些下载进程（如IE的文件下载）为了减少磁盘写操作的压力，会在每次接收数据的系统调用之间进行休眠停顿，这样一来，实际上单线程的下载过程中接收数据并不是绝对连贯的，所以我们一般喜欢用多线程下载工具来加速下载，就是因为多个执行流可以使得下载过程在单位时间内执行相对更多次数的数据接收系统调用，充分占用带宽。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 在一些长距离的传播线路（如跨城光缆）中必须使用中继器，以此防止电磁波信号在传播中过分衰减，所以这些中继器需要对信号进行接收和转发，也存在发送时间的消耗，但是不多。</p>
<p>● 为了容易描述网络结构，在前面我们经常提到的“交换节点”普遍特指“交换路由器”，也就是一个设备同时具备路由器和交换机的功能，很多人在家里使用的宽带路由器便是这样的节点设备。而有些交换节点，可能采用了独立的路由器和交换机，那么一旦数据需要经其转发到其他网络，这个交换节点便需要对数据进行两次转发，也就是转发次数翻倍，不过不必担心，和上边的中继器转发一样，这部分时间的确也不是很多，但是有必要说明这些可能存在的环节。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>所以，要搞清楚响应时间的消耗，必须根据实际情况，找出转发路径中是否存在一些较低带宽的交换节点，正是这些节点成为影响下载速度的瓶颈所在。</p>
</blockquote>
</blockquote>
<h3 id="◆-2-4-互联互通" tabindex="-1"><a class="header-anchor" href="#◆-2-4-互联互通" aria-hidden="true">#</a> ◆ 2.4 互联互通</h3>
<blockquote>
<blockquote>
<p>如果服务器和用户PC处于不同运营商的互联网中，那么不论是否在同一城市，数据都必须经过两个互联网运营商之间的互联节点，这个节点的带宽变成了令大家十分头疼的问题。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在中国，由中国电信运营的互联网，也就是我们常说的“中国宽带互联网（CHINANET）”，它的骨干网络核心节点位于北京上地电信数据中心，它通过直接接入包括北京在内的国内8个重要城市节点，进而连接二级网络，然后层层延伸扩展，一直到周边城市、IDC、家庭宽带接入等。这8座城市为北京、沈阳、西安、成都、上海、南京、广州、武汉。这些城市直接与骨干网络连接的IDC都是国家一级设施，除了提供商业服务以外，还有重要的战略意义。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>目前，电信和网通两大互联网运营商之间的互联互通虽然早已实现，但是互联带宽少得可怜。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>选择IDC的时候要考察出口带宽以及与骨干网络是否直连，如果要同时为多个互联网运营商网络的用户提供服务，则需要考察出口节点与运营商互联节点的带宽，而如果要面向全球用户提供服务，则需要考察国际网络结构和各个国家的国际出口带宽等。另一方面，带宽作为稀缺资源，其价格严格服从市场供求规律，比如同样的独享带宽在北京就比沈阳贵很多，所以我们在选择的时候价格因素也相当重要。</p>
</blockquote>
</blockquote>
<h2 id="◆-第3章-服务器并发处理能力" tabindex="-1"><a class="header-anchor" href="#◆-第3章-服务器并发处理能力" aria-hidden="true">#</a> ◆ 第3章 服务器并发处理能力</h2>
<blockquote>
<blockquote>
<p>一台Web服务器在单位时间内能处理的请求越多越好，这也成了Web服务器的能力高低所在，它体现了我们常说的“服务器并发处理能力”。</p>
</blockquote>
</blockquote>
<h3 id="◆-3-1-吞吐率" tabindex="-1"><a class="header-anchor" href="#◆-3-1-吞吐率" aria-hidden="true">#</a> ◆ 3.1 吞吐率</h3>
<blockquote>
<blockquote>
<p>我们一般使用单位时间内服务器处理的请求数来描述其并发处理能力，但是听起来有点长，我们习惯称其为吞吐率（Throughput），单位是“reqs/s”。</p>
<p>在一些常见的Web服务器软件中，通常会提供当前服务器运行状况以及吞吐率的查看方法，帮助我们对服务器的吞吐率随时进行了解和监控，比如Apache的mod_status模块提供了如下统计：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212133139287.png" alt="image-20211212133139287"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>我们普遍使用“压力测试”的方法，通过模拟足够数目的并发用户数，分别持续发送一定的HTTP请求，并统计测试持续的总时间，计算出基于这种“压力”下的吞吐率，即为一个平均计算值。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>Web服务器并发能力强弱的关键便在于如何针对不同的请求性质来设计最优并发策略</p>
<p>压力的描述一般包括两部分，即并发用户数和总请求数，也就是模拟多少个用户同时向服务器发送多少个请求。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>吞吐率的前提包括如下几个条件：● 并发用户数● 总请求数● 请求资源描述</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>并发用户数就是指在某一时刻同时向服务器发送请求的用户总数。如此多的用户同时请求服务器，显然会给服务器带来不小的压力，这时你可能有一个实际的问题，假如100个用户同时向服务器分别进行10次请求，与1个用户向服务器连续进行1000次请求，效果一样吗？也就是说给服务器带来的压力一样吗？</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>虽然看起来服务器都需要连续处理1000 个请求，其实关键的区别就在于，是否真的“连续”。首先有一点需要明白，对于压力测试中提到的每一个用户，连续发送请求实际上是指在发送一个请求并接收到响应数据后再发送下一个请求。这样一来，从微观层面来看，1 个用户向服务器连续进行1000 次请求的过程中，任何时刻服务器的网卡接收缓冲区中只有来自该用户的1个请求，而100个用户同时向服务器分别进行10次请求的过程中，服务器网卡接收缓冲区中最多有100个等待处理的请求，显然这时候服务器的压力更大。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>吞吐率是指在一定并发用户数的情况下，服务器处理请求能力的量化体现。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>说一个服务器最多支持多少并发用户数，这个“最多”到底是什么意思？</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>通常所讲的最大并发数是有一定利益前提的，那就是服务器和用户双方所期待的最大收益，服务器希望支持高并发数及高吞吐率，而用户不管那么多，只希望等待较少的时间，或者得到更快的下载速度</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>用户访问Web站点通常使用浏览器，而浏览器在下载一个网页以及网页中的多个组件时，采用多线程的并发下载方式，但是对于同一域名下URL的并发下载数是有最大限制的，具体限制视浏览器的不同而不同，比如在HTTP/1．1下，IE7支持两个并发连接，IE8支持6个并发连接，Firefox3支持4个并发连接，我们使用诸如HttpWatch这样的HTTP监视工具可以很清晰地看到这一点。所以，我们前面说到的服务器支持的最大并发数，具体到真实的用户，可能并不是一对一的关系，一个真实的用户可能会给服务器带来两个或更多的并发用户数的压力，一些高明的用户还可以通过一些方法来修改浏览器的并发数限制。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>不是同时来了多少用户请求就建立多少连接，Web服务器一般会限制同时服务的最多用户数，比如Apache的MaxClients参数，所以这个实际并发用户数，有时候大于服务器所维护的文件描述符总数，而多出的这些用户请求，则在服务器内核的数据接收缓冲区中等待处理，所以这些请求在用户看来处于阻塞状态。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>最大并发用户数和最大并发连接数的决定因素从本质上来说是不同的，举两个例子：</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 当实际并发用户数稍稍大于服务器所能维护的文件描述符上限时，如果请求的性质决定了处理每个请求花费的时间非常少，比如请求1KB的静态网页，那么每个请求都可以快速被处理然后释放文件描述符，这样从用户的角度而言，等待时间几乎不会减少太多。所以在这种情况下，我们希望服务的最大并发用户数可以大于最大并发连接数。幸运的是，这种情况在我们后边介绍select模型在大并发下处理小文件请求时会有相应的测试。</p>
<p>● 如果请求的性质决定了处理每个请求要花费相当长的时间，比如下载10MB文件或者请求动态内容，那么即使服务器可以支持较大的并发连接数，比如使用异步I/O理论上可能支持2万个并发连接，然而是否能够为这么多接入的用户提供快速响应的服务至关重要。对于下载10MB文件来说，可能由于带宽的瓜分而导致每个用户的下载速度缓慢，而对于请求动态内容，可能由于CPU的时间瓜分导致每个用户的等待时间过长。所以在这种情况下，我们希望服务的最大并发用户数小于理论上的最大并发连接数。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>从某种意义上可以说，Web服务器所做的工作的本质就是，争取以最快的速度将内核缓冲区中的用户请求数据一个不剩地都拿回来，然后尽最大努力同时快速处理完这些请求，并将响应数据放到内核维护的另一块用于发送数据的缓冲区中，接下来再尽快处理下一拨请求，并尽量让用户请求在内核缓冲区中不要等太久。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>我们所关心的时间有以下两种：● 用户平均请求等待时间● 服务器平均请求处理时间</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>假设并发用户数为1，也就是只有一个用户在向服务器源源不断地发送请求，那么每个请求的等待时间也就是它的处理时间，等于总时间除以总请求数，这时用户平均请求等待时间和服务器平均请求处理时间是相同的</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>假设并发用户数为100，那么便会有100个用户同时向服务器发送请求，简单地说，这时Web服务器一般会采用多进程或多线程的并发模型，通过多个执行流来同时处理多个并发用户的请求，而多执行流体系的设计原则便是轮流交错使用CPU时间片，所以每个执行流花费的时间都被拉长。对每个用户而言，每个请求的平均等待时间必然增加；而对于服务器而言，如果并发策略得当，每个请求的平均处理时间可能减少。</p>
<p>所以，这两个时间的本质在于，用户平均请求等待时间主要用于衡量服务器在一定并发用户数的情况下，对于单个用户的服务质量；而服务器平均请求处理时间与前者相比，则用于衡量服务器的整体服务质量，它其实就是吞吐率的倒数。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>和前一次的测试结果相比，可以看出，当并发用户数从原来的10变为100后，吞吐率从原来的5960．15reqs/s增长到了6364．24reqs/s，服务器平均请求处理时间从原来的0．168ms降到了0．157ms，而用户平均请求等待时间从原来的1．678ms增加到了15．713ms。可见，随着并发用户数的变化，吞吐率、用户平均请求等待时间（以下简称请求等待时间）、服务器平均请求处理时间（以下简称请求处理时间）都发生了相应的变化。为了了解变化的曲线，我们对不同并发用户数的情况分别进行压力测试，得出如表3-2 所示的数据。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212134802157.png" alt="image-20211212134802157"></p>
<p>ab进行一切测试的本质都是基于HTTP，所以可以说它是对于Web服务器软件的黑盒性能测试，它获得的一切数据和计算结果，都可以通过HTTP来解释。另有一些压力测试软件，包括LoadRunner、Jmeter等，则是不同程度上包含了服务器处理之外的时间，比如LoadRunner运行在用户PC上，可以录制浏览器行为，这种测试的结果往往侧重于站点用户的角度，有另外一些层面的参考意义。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>来看吞吐率随并发用户数变化的曲线图，如图3-1所示。在并发用户数达到100之前，随着并发数的增长，服务器的资源被不断地充分利用，所以其吞吐率在不断提高。当并发用户数为100时，吞吐率最高。当并发用户数超过100后，吞吐率开始走下坡路，并且在并发用户数超过150后，吞吐率开始直线下跌，惨不忍睹。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>并发策略的设计就是在服务器同时处理较多请求的时候，如何合理协调并充分利用CPU计算和I/O操作，使其在较大并发用户数的情况下提供较高的吞吐率。</p>
</blockquote>
</blockquote>
<h3 id="◆-3-2-cpu并发计算" tabindex="-1"><a class="header-anchor" href="#◆-3-2-cpu并发计算" aria-hidden="true">#</a> ◆ 3.2 CPU并发计算</h3>
<blockquote>
<blockquote>
<p>服务器之所以可以同时处理多个请求，在于操作系统通过多执行流体系设计使得多个任务可以轮流使用系统资源，这些资源包括CPU、内存以及I/O等。</p>
</blockquote>
</blockquote>
<h4 id="进程" tabindex="-1"><a class="header-anchor" href="#进程" aria-hidden="true">#</a> 进程：</h4>
<blockquote>
<blockquote>
<p>多执行流的一般实现便是进程。从本质上讲，多进程的好处并不仅仅在于CPU时间的轮流使用，还在于对CPU计算和I/O操作进行了很好的重叠利用，这里的I/O主要是指磁盘I/O和网络I/O，它们的速度和CPU相比，就像老牛漫步和超音速飞机一样相距甚远。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>大多数进程的时间都主要消耗在了I/O操作上，现代计算机的DMA技术可以让CPU不参与I/O操作的全过程，比如进程通过系统调用，使得CPU向网卡或者磁盘等I/O设备发出指令，然后进程被挂起，释放出CPU资源，等待I/O设备完成工作后通过中断来通知进程重新就绪。所以对于单任务而言，CPU大部分时间空闲，这时候多进程的作用便显得尤为重要。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>进程的调度由内核来进行，从内核的观点看，进程的目的就是担当分配系统资源的实体。同时，进程也可以理解为记录程序实例当前运行到什么程度的一组数据，多个进程通过不同的进程描述符与这些数据进行关联。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>每个进程都有自己独立的内存地址空间和生命周期。当子进程被父进程创建后，便将父进程地址空间的所有数据复制到自己的地址空间，完全继承父进程的所有上下文信息，它们之间可以通信，但是不互相依赖，也无权干涉彼此的地址空间。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>进程的创建使用fork()系统调用，它的开销虽然不很昂贵，但是在繁忙的服务器上频繁地创建进程，其开销可能成为影响性能的主要因素。Linux 2．6对于fork()的实现进行了优化，减少了一些多余的内存复制，但在早期的版本或者其他平台中，fork()的开销随着当前进程数量的递增而加大。</p>
</blockquote>
</blockquote>
<h4 id="轻量级进程" tabindex="-1"><a class="header-anchor" href="#轻量级进程" aria-hidden="true">#</a> 轻量级进程：</h4>
<blockquote>
<blockquote>
<p>由于进程之间相对独立，它们各自维护庞大的地址空间和上下文信息，无法很好地低成本共享数据，所以采用大量进程的Web服务器（比如Apache的prefork模型）在处理大量并发请求时，内存的大量消耗有时候会成为性能提升的制约因素。但是，进程的优越性有时也恰恰体现在其相互独立所带来的稳定性和健壮性方面。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>轻量级进程的支持，它由一个新的系统调用clone()来创建，并由内核直接管理，像普通的进程一样独立存在，各自拥有进程描述符，但是这些进程已经允许共享一些资源，比如地址空间、打开的文件等。轻量级进程减少了内存的开销，并为多进程应用程序的数据共享提供了直接支持，但是其上下文切换的开销还是在所难免的。</p>
</blockquote>
</blockquote>
<h4 id="线程" tabindex="-1"><a class="header-anchor" href="#线程" aria-hidden="true">#</a> <strong>线程：</strong></h4>
<blockquote>
<blockquote>
<p>POSIX线程另一种实现是LinuxThreads，它可以说是内核级的线程库（kernel-level threads），因为它通过clone()来创建线程，也就是说它的实现原理是将线程和轻量级进程进行一对一关联，每个线程实际上就是一个轻量级进程，这样使得线程完全由内核的进程调度器来管理，所以它对于SMP的支持较好，但是线程切换的开销相比于用户态线程要多一些。</p>
</blockquote>
</blockquote>
<h4 id="进程调度器" tabindex="-1"><a class="header-anchor" href="#进程调度器" aria-hidden="true">#</a> 进程调度器：</h4>
<blockquote>
<blockquote>
<p>在单CPU的机器上，虽然我们感觉到很多任务在同时运行，但是从微观意义上讲，任何时刻只有一个进程处于运行状态，而其他的进程有的处于挂起状态并等待就绪，有的已经就绪但等待CPU时间片，还有的处于其他状态。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>内核中的进程调度器（Scheduler）维护着各种状态的进程队列。在Linux中，进程调度器维护着一个包括所有可运行进程的队列，称为“运行队列（Run Quere）”，以及一个包括所有休眠进程和僵尸进程的列表。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>进程调度器的一项重要工作就是决定下一个运行的进程，如果运行队列中有不止一个进程，那就比较伤脑筋了，按照先来后到的顺序也许不是那么合理，因为运行在系统中的进程有着不同的工作需要，比如有些进程需要处理紧急的事件，有些进程只是在后台发送不太紧急的邮件，所以每个进程需要告诉进程调度器它们的紧急程度，这就是进程优先级。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>进程优先级除了可以由进程自己决定，进程调度器在进程运行时也可以动态调整它们的优先级，比如对有些进程适当提高优先级，对有些进程则进行处罚，降低它们的优先级，这些行为的出发点都是为了让所有进程更好地重叠利用系统资源。</p>
<p>Linux对于进程的动态调整，体现在进程的nice属性中，我们对Lighttpd进行持续压力测试，使用100个并发用户来请求151字节的静态文件，这时候通过top来观察lighttpd进程的优先级和动态调整：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212142328363.png" alt="image-20211212142328363"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>进程的优先级属性为Priority，在top结果中用PR表示，而优先级动态调整值在top中用NI表示。在top的间断刷新中，lighttpd的PR值从16不断上升为25，最后一直停留在25，直到测试结束。</p>
<p>我们对Lighttpd的fastcgi进程进行压力测试，看看fastcgi进程的优先级，我们配置Lighttpd为打开4个fastcgi进程，请求的资源为一个以CPU计算为主的PHP程序，结果如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212142528984.png" alt="image-20211212142528984"></p>
<p>可以看到每个代表fastcgi的php进程的PR值已经低于刚才的lighttpd。</p>
<p>再来看看Apache-prefork，同时是使用100个并发用户来请求151字节的静态文件，Apache子进程数上限设置为100，也许你的经验已经告诉你Apache处理这些请求时的CPU开销要远远超过刚才的Lighttpd，在随后我们将会专门来探讨这个问题。这时候的top结果如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212142552864.png" alt="image-20211212142552864"></p>
<p>httpd的PR值为15左右，操作系统不愿意给它更高的优先级。事实上Linux 2．6的进程调度器更加偏爱I/O操作密集型的进程，因为这些进程在发起I/O操作后通常都会阻塞（除非使用异步I/O），不会占用太多CPU时间，这就意味着其他进程都可以更好地交错运行。</p>
<p>PR所代表的值有什么含义呢？它其实就是进程调度器分配给进程的时间片长度，单位是时钟个数，那么一个时钟需要多长时间呢？这跟CPU的主频以及操作系统平台有关，比如Linux上一般为10ms，那么PR值为15则表示这个进程的时间片为150ms。</p>
<p>在操作系统中，时间片的长度是权衡利弊后的一个博弈结果，如果各进程的时间片太短，那么CPU浪费在进程切换上的时间比例就比较大，整体效率降低；而如果时间片太长，则多任务实时性以及交互性就无法保证。</p>
<p>在传统的UNIX中，进程调度器为所有进程计算nice值的时候，需要锁住进程表，这时候对于使用了多处理器的服务器（SMP）来说非常糟糕，因为当一个CPU锁住进程表进行nice值计算时，其他CPU无法切换进程。这个问题在一些商业化操作系统以及Linux中得以解决，简单地说就是给每个处理器分配一个运行队列，互不影响，进程调度器负责将进程分配到适合的CPU。</p>
</blockquote>
</blockquote>
<h4 id="系统负载" tabindex="-1"><a class="header-anchor" href="#系统负载" aria-hidden="true">#</a> 系统负载：</h4>
<blockquote>
<blockquote>
<p>在进程调度器维护的运行队列中，任何时刻至少存在一个进程，那就是正在运行的进程。而当运行队列中有不止一个进程的时候，就说明此时CPU比较抢手，其他进程还在等着呢，进程调度器应该尽快让正在运行的进程释放CPU。</p>
<p>通过在任何时刻查看/proc/loadavg，可以了解到运行队列的情况：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212143114164.png" alt="image-20211212143114164"></p>
<p>注意10/200这部分，其中的10代表此时运行队列中的进程个数，而200则代表此时的进程总数。最右边的17145代表到此时为止，最后创建的一个进程ID。</p>
<p>接下来，请看左边的三个数值，分别是1．63、0．48、0．21，它们就是我们常说的系统负载。我们都知道，系统负载越高，代表CPU越繁忙，越无法很好地满足所有进程的需要。但是，系统负载是如何计算而来的呢？根据定义，它是在单位时间内运行队列中就绪等待的进程数平均值，所以当运行队列中就绪进程不需要等待就可以马上获得CPU的时候，系统负载便非常低。当系统负载为0．00时，说明任何进程只要就绪后就可以马上获得CPU，不需要等待，这时候系统响应速度最快。</p>
<p>三个数值，便是系统最近1分钟、5分钟和15分钟分别计算得出的系统负载。</p>
<p>还可以通过其他方法获得系统负载，比如top、w等工具：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212143818275.png" alt="image-20211212143818275"></p>
<p>从实现方法上看，这些工具获得的系统负载都是来源于/proc/loadavg。了解了这些内容后，要想提高服务器的系统负载，很简单，我们编写一个没有任何I/O操作并且长时间占用CPU时间的PHP脚本，比如一个循环累加器，如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212143859480.png" alt="image-20211212143859480"></p>
<p>然后用100个并发用户请求这个脚本，进行压力测试，这时候查看系统负载，结果如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212143931025.png" alt="image-20211212143931025"></p>
</blockquote>
</blockquote>
<h4 id="进程切换" tabindex="-1"><a class="header-anchor" href="#进程切换" aria-hidden="true">#</a> <strong>进程切换：</strong></h4>
<p>为了让所有的进程可以轮流使用系统资源，进程调度器在必要的时候挂起正在运行的进程，同时恢复以前挂起的某个进程，这种行为称为进程切换，也就是我们常说的“上下文切换”，这个名称在某种意义上非常形象，“上下文”正是表示进程运行到何种程度。</p>
<p>进程拥有自己独立的内存空间，但是每个进程都只能共享CPU寄存器。一个进程被挂起的本质就是将它在CPU寄存器中的数据拿出来暂存在内核态堆栈中，而一个进程恢复工作的本质就是将它的数据重新装入CPU寄存器，这段装入和移出的数据我们称为“硬件上下文”，它也是进程上下文的一部分，除此之外，进程上下文中还包含了进程运行时需要的一切状态信息。</p>
<p>当硬件上下文频繁地装入和移出时，所消耗的时间是非常可观的，我们使用Nmon工具监视服务器每秒上下文切换次数。Nmon是一个非常不错的Linux性能监视工具，它可以提供基于服务器终端命令行的监视界面，还可以通过专用的分析器将监视数据生成报表和曲线图，在后面关于性能监控的章节中我们将详细介绍Nmon的用法。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212202109145.png" alt="image-20211212202109145"></p>
<p>这时的上下文切换平均每秒28．4次，这是操作系统正常运转所进行的必要工作。</p>
<p>接下来，我们运行ab对Apache进行压力测试，这里Apache使用prefork运行模式，在该模式下，Apache通过一个父进程预先创建一定数量的子进程，所有子进程竞争accept用户请求，一旦某个子进程accept成功后便开始处理这个请求。每个子进程能够处理的请求数可以通过MaxRequestsPerChild进行配置，如果MaxRequestsPerChild设置为1，那就相当于传统的fork模式，为每个请求创建新的进程来处理。同时父进程根据负载和MPM参数配置对子进程数进行必要的增减。以下列出此时prefork的MPM参数配置：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212202233482.png" alt="image-20211212202233482"></p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212202247962.png" alt="image-20211212202247962"></p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212202303315.png" alt="image-20211212202303315"></p>
<p>如果我们希望服务器支持较大的并发数，那么就要尽量减少上下文切换次数，最简单的做法就是减少进程数，尽量使用线程并配合其他I/O模型来设计并发策略</p>
<h4 id="iowait" tabindex="-1"><a class="header-anchor" href="#iowait" aria-hidden="true">#</a> IOWait：</h4>
<p>在CPU使用率报告中，除了用户空间和内核空间的CPU使用率以外，通常我们还关注IOWait，它是指CPU空闲并且等待I/O操作完成的时间比例，注意，它是一个比例，而不是绝对时间，比如我们在top中看到当前的IOWait为12．9%。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212202829704.png" alt="image-20211212202829704"></p>
<p>IOWait往往并不能真实地代表I/O操作的性能或者工作量，它的设计出发点是用来衡量CPU的性能，举个例子，假如有一个任务需要花费10ms的I/O操作时间和10ms的CPU时间，那么总时间为20ms，IOWait便为10ms/20ms，等于50%，这时并不意味着I/O操作的繁忙程度为50%，事实上，即便是IOWait为100%，也不一定代表I/O出现性能问题或者瓶颈，同样，IOWait为0的时候I/O操作也可能很繁忙，所以，如果你希望真正了解当前I/O的性能，可以进行磁盘I/O测试或者查看网络I/O流量等。</p>
<p>在实际的多进程环境中，IOWait的计算方法并没有这么简单，而且各个Linux发行版也有所不同，但既然IOWait是用来描述CPU性能的，那么当IOWait很高的时候，至少说明当前任务的CPU时间开销相对于I/O操作时间来说比较少，通常对于依赖磁盘I/O的应用来说，这比较正常，因为CPU的速度比磁盘I/O越来越快，特别是在随机的磁盘I/O操作中，大量的寻址时间是无法避免的。但是，有时候IOWait的确会误导我们，比如对于提供下载服务的站点，我们采用了Nginx来作为Web服务器，当我们开启了128个nginx进程的时候，通过Nmon得到的CPU监控数据如图3-6所示。</p>
<p>可以看到，此刻的IOWait相对比较高，平均达到了50%以上，也许你认为这是正常的，因为下载服务采用sendfile系统调用来传送数据，几乎不需要用户空间的CPU时间，所以几乎看不出什么问题。但是，当我们将Nginx的进程数降低到8后，IOWait大幅度下降，同时最关键的是，下载服务的网络I/O提高了近一倍。对于流量的提升，我们究其原因，或许是因为nginx进程大量减少后，花费在进程上下文切换的CPU时间也随之减少，更多的CPU时间可以用在发起sendfile系统调用，所以网络I/O流量大幅度提高，而与此同时，磁盘由于存在高速缓存，所以实际的磁盘I/O并没有明显提高，但CPU使用率提高了，所以IOWait减少了。</p>
<p>总之，IOWait有时候是一个耐人寻味的指标，你需要根据站点的实际情况来做出合理的判断。</p>
<h4 id="锁竞争" tabindex="-1"><a class="header-anchor" href="#锁竞争" aria-hidden="true">#</a> 锁竞争：</h4>
<p>只要涉及抢手资源的占有，就必然会存在竞争，这在现实社会中屡见不鲜。然而在服务器处理大量并发请求的时候，多个请求处理任务之间也存在一些资源抢占竞争，这就需要有一种机制来维持秩序，比如多个线程同时写一个日志文件，为了防止写入的数据发生位置错乱，就需要我们自己来控制先后顺序，也就是要保证线程安全。</p>
<p>我们一般采用“锁”机制来控制资源的占用，当一个任务占用资源的时候，我们锁住资源，这时候其他任务都在等待锁的释放，这种现象我们称为锁竞争。</p>
<p>通过锁竞争的本质，我们要意识到尽量减少并发请求对于共享资源的竞争，比如在允许的情况下关闭Web服务器访问日志，这可以大大减少在锁等待时的延迟时间。另外，在分布式系统中，锁的动机仍然相同，你也许会看到多台服务器通过网络来竞争共享资源，同样，我们需要通过良好的设计来最大程度地减少无辜的等待时间。</p>
<h3 id="◆-3-3-系统调用" tabindex="-1"><a class="header-anchor" href="#◆-3-3-系统调用" aria-hidden="true">#</a> ◆ 3.3 系统调用</h3>
<blockquote>
<blockquote>
<p>进程的用户态和内核态两种运行模式，这是Linux为进程设计的两种运行级别，进程可以在两种模式之间切换，这也需要一定的开销。进程通常运行在用户态，这时候可以使用CPU和内存来完成一些任务（如进行数学计算），而当进程需要对硬件外设进行操作的时候（如读取磁盘文件、发送网络数据等），就必须切换到内核态，这时候它将拥有更多的权力来操控整个计算机，当在内核态的任务完成后，进程又切换回用户态。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>进程切换到内核态的一系列过程，对于使用高级语言的开发者来说是透明的，程序代码只在需要的时候进行系统调用即可。内核提供了一系列的系统调用，同时，C库函数（libc）将系统调用封装在编程接口中，提供给用户态的进程。用户态的进程可以直接进行系统调用，也可以使用封装了系统调用的C API，比如write()系统调用，它的封装API之一就是用于发送网络数据的send()。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>这种用户态和内核态的分离，动机主要在于提高系统底层安全性以及简化开发模型。由于所有进程都必须通过内核提供的系统调用来操作硬件，所以不用担心应用程序对硬件进行非法操作，由于将底层的实现都屏蔽在了系统调用中，也大大简化了用户态应用开发的难度。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>由于系统调用涉及进程从用户态到内核态的切换，导致一定的内存空间交换，这也是一定程度上的上下文切换，所以系统调用的开销通常认为是比较昂贵的。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>减少不必要的系统调用，也是Web服务器性能优化的一个方面，举个例子，在Apache中支持通过．htacess文件来为htdocs下各个目录进行局部的参数配置，但它也有一定的副作用。我们将httpd．conf中的AllowOverride设置为All，如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212204012317.png" alt="image-20211212204012317"></p>
<p>这时候，我们使用strace来跟踪Apache的一个子进程，获得某次请求处理中的一系列系统调用，如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212204216866.png" alt="image-20211212204216866"></p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212204227572.png" alt="image-20211212204227572"></p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212204243254.png" alt="image-20211212204243254"></p>
<p>可知一共有30次系统调用，其中的粗体部分表示Apache在检查被访问的文件路径中各级目录下是否存在．htacess文件，共涉及6次open()系统调用。</p>
<p>此时，我们使用ab对其进行压力测试，测试结果如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212204322566.png" alt="image-20211212204322566"></p>
<p>下面我们关闭．htacess功能来减掉这6次系统调用，修改httpd．conf如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212204335928.png" alt="image-20211212204335928"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>另外，我们还看到gettimeofday()和times()这两次系统调用，它用来获取当前的系统时间。我们还可以通过关闭mod_status来减少多余的gettimeofday()系统调用，因为mod_status为了监控每一个请求的处理时间，必须在处理请求的各个环节多次获取系统时间，修改httpd．conf如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212204453502.png" alt="image-20211212204453502"></p>
<p>重启Apache后，我们再次使用strace跟踪，结果如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212204514176.png" alt="image-20211212204514176"></p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212204840246.png" alt="image-20211212204840246"></p>
<p>现在只剩下16个系统调用了，我们再次对其进行同样条件的压力测试，结果如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212204859766.png" alt="image-20211212204859766"></p>
<p>吞吐率比之前大幅增加，可见，系统调用的减少对于降低请求处理时间有着不可忽略的作用。</p>
</blockquote>
</blockquote>
<h3 id="◆-3-4-内存分配" tabindex="-1"><a class="header-anchor" href="#◆-3-4-内存分配" aria-hidden="true">#</a> ◆ 3.4 内存分配</h3>
<blockquote>
<blockquote>
<p>对于传统的应用，各类表达式所消耗的最大开销就在于中间临时变量的内存分配及数据复制时间，而对于Web服务器处理成千上万的HTTP请求而言，内存堆栈的分配和复制次数变得更加频繁。我们可以通过改善数据结构和算法复杂度来适当减少数据复制时间，而对于内存的分配，很多Web服务器使用了各自的策略来提高效率。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>Apache使用了基于内存池策略的内存管理方案，并将它抽象出来后移入APR库中作为通用内存管理模块。这种方案使得Apache在运行开始时便一次性申请大片的内存作为内存池，这样在随后需要的时候只要在内存池中直接获取，而不需要再次分配，我们知道频繁的内存分配和释放会引发一定时间的内存整理，这本身便影响了性能。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>内存池的使用使得Apache的内存管理更加安全，因为即便是某处内存使用后忘记释放也没有关系，内存池在Apache关闭时会彻底释放。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>即便是使用了内存池，由于机制问题，Apache仍然就像拖着沉重身体的傻大个，内存池对于性能的弥补微不足道。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对于使用了单进程模型的Lighttpd，内存的使用量要小很多</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>同样是单进程模型的Nginx（读作Engine X），它的内存使用量更小。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>Nginx对于内存方面的良好表现，也正是来自于它的内存分配策略，它可以使用多线程来处理请求，这使得多个线程之间可以共享内存资源，从而令它的内存总体使用量大大减少，另外，它使用分阶段的内存分配策略，按需分配，及时释放，使得内存使用量保持在很小的数量范围。Nginx的设计初衷便在于支持较大的并发连接数，而内存是否足够往往是阻碍这一目标的关键因素，所以Nginx在内存管理方案的设计上花了不少工夫，官方介绍中声称，Nginx维持10000个非活跃HTTP持久连接只需要2．5MB的内存</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>内存分配策略的设计是Web服务器并发处理能力的重要保障</p>
</blockquote>
</blockquote>
<h3 id="◆-3-5-持久连接" tabindex="-1"><a class="header-anchor" href="#◆-3-5-持久连接" aria-hidden="true">#</a> ◆ 3.5 持久连接</h3>
<blockquote>
<blockquote>
<p>持久连接（Keep-Alive）有时候也称为长连接，它本身是TCP通信的一种普通方式，即在一次TCP连接中持续发送多份数据而不断开连接，与它相反的方式称为短连接，也就是建立连接后发送一份数据便断开，然后再次建立连接发送下一份数据，周而复始。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>连接次数越少，越有利于性能的提升</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>回归TCP传输层，长连接带来的好处显而易见，它对于密集型的图片或网页等小数据请求处理有着明显的加速作用。HTTP/1．1对长连接有了完整的定义，同时，基于标准化的协议，很多浏览器和Web服务器也都纷纷提供了对于长连接的支持。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>HTTP长连接的实施需要浏览器和Web服务器的共同协作，缺一不可。一方面，浏览器需要保持一个TCP连接并重复利用，不断地发送多个请求，另一方面，服务器不能过早地主动关闭连接。要实现这一点并不难，目前的浏览器普遍支持长连接，表现在其发出的HTTP请求数据头中包含关于长连接的声明，如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212210925336.png" alt="image-20211212210925336"></p>
<p>这种声明的含义在于告诉服务器，“如果可以的话，请让我重用这个连接”。言下之意就是告诉服务器不要在处理完当前请求后就马上关闭连接。</p>
<p>同时，在Web服务器上也要打开长连接的支持，幸运的是，目前所有主流的Web服务器软件都支持长连接，比如在Apache 2．2．11中，长连接的支持默认为开启状态，当然你也可以通过以下方法关闭：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212211005211.png" alt="image-20211212211005211"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>当我们不希望浏览器和Web服务器使用长连接方式时，我们可以关闭服务器的长连接支持，这也是唯一的办法，因为我们无权去修改每个用户的浏览器设置。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对于长连接的有效使用，其关键的一点在于长连接超时时间的设置，即长连接在什么时候被关闭呢？这个设置同时出现在浏览器和Web服务器上，因为双方都可以主动关闭。对于IE7，默认的超时时间为1分钟，你也可以通过修改注册表来修改超时时间。对于Web服务器，一般会提供超时时间的配置参数，比如在Apache中，可以通过httpd．conf中的如下参数进行配置：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212211725350.png" alt="image-20211212211725350"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>以上的命令设置超时时间为30秒，而在默认情况下，Apache将其设置为5秒。值得注意的是，浏览器和Web服务器各自的超时时间设置不一定一致，所以在实际运行中，是以最短的超时时间为准。</p>
<p>了解了这些内容后，我们来看看由浏览器发起的一个支持长连接的HTTP请求：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212211757738.png" alt="image-20211212211757738"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212211828135.png" alt="image-20211212211828135"></p>
<p>在请求大量小文件的时候，长连接的有效使用可以减少大量重新建立连接的开销，有效加速性能，ab的启动选项参数中有对长连接的支持，也就是模拟浏览器发起支持长连接的HTTP请求。我们用ab来试试Apache的prefork模式在长连接下的表现。</p>
<p>首先，我们不使用长连接，请求一个122字节的图片，ab测试结果如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212212241455.png" alt="image-20211212212241455"></p>
<p>下面我们使用长连接模式进行测试，结果如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212212301149.png" alt="image-20211212212301149"></p>
<p>从结果中可以看出，吞吐率大幅度提高，使用了长连接的请求为9910个，也就是说这些请求没有专门建立连接，而是重用了已经建立好的连接。</p>
<p>Nginx在长连接情况下的表现，这次我们使用strace的系统调用统计功能，可以直观地进行时间对比。首先我们不使用长连接，对Nginx从压力测试开始到结束进行跟踪，结果如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212212409982.png" alt="image-20211212212409982"></p>
<p>我们使用长连接方式对其进行同样条件的压力测试并跟踪，结果如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212212426863.png" alt="image-20211212212426863"></p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212212434935.png" alt="image-20211212212434935"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在长连接的情况下，总的系统调用数减少了一半左右，其中socket的accept()和close()的减少量在我们的意料之中，注意它们消耗的时间，比不使用长连接的时候减少了很多，这在整体时间中占有不小的比例。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>那么超时时间的长短究竟对Web服务器性能有何影响呢？不难想象，对于Apache这样的多进程模型来说，如果长连接超时时间过长，比如60秒，那么即便是浏览器没有任何请求，而Apache仍然维持着连接该浏览器的子进程，一旦并发用户数较多，那么Apache将维持着大量的空闲进程，严重影响了服务器性能。另一方面，对于使用多线程的轻量级Web服务器（如Nginx），长连接超时时间过长同样也不是一件好事，因为有时由于超时时间过长导致资源无效占有而引发的损失已经超过了由于重复连接所造成的损失，这的确得不偿失。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>持久连接无处不在，除了以上在HTTP通信中的应用，它还可以应用在任何涉及TCP通信的应用中，比如MySQL数据访问时的持久连接以及连接池，持久连接的动机很简单，那就是尽量减少连接次数，尽量重用连接通道。</p>
</blockquote>
</blockquote>
<h3 id="◆-3-6-i-o模型" tabindex="-1"><a class="header-anchor" href="#◆-3-6-i-o模型" aria-hidden="true">#</a> ◆ 3.6 I/O模型</h3>
<blockquote>
<blockquote>
<p>计算机的重要工作之一便是负责各种设备的数据输入/输出，也就是I/O（In/Out）操作。</p>
<p>I/O操作根据设备的不同分为很多种类型，比如内存I/O、网络I/O、磁盘I/O。对于内存I/O，一般我们在讨论Web站点性能时很少提及，因为相比于后两种I/O操作，内存I/O的速度已经足够快了，前面我们介绍过网络数据传输中的瓶颈往往在于带宽最低的交换节点，类似地，计算机性能的瓶颈往往并不在于内存I/O本身。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对于网络I/O和磁盘I/O，它们的速度要慢很多，尽管使用RAID磁盘阵列可以通过并行磁盘访问来加快磁盘I/O速度，购买大量独享网络带宽以及使用高带宽网络适配器可以提高网络I/O的速度。但问题在于，这些I/O操作需要由内核系统调用来完成，同时系统调用显然需要由CPU来调度，而CPU的速度毫无疑问是非常快的，这就使得CPU不得不浪费宝贵的时间来等待慢速I/O操作。</p>
<p>尽管我们通过多进程等方式来充分利用空闲的CPU资源，但我们还是希望能够让CPU花费足够少的时间在I/O操作的调度上，这样就可以腾出更多的CPU时间来完成更多的I/O操作，事实上，如何让高速的CPU和慢速的I/O设备更好地协调工作，这是从现代计算机诞生到现在一直在探讨的话题，很多技术和策略都围绕它们展开。</p>
<p>我们所关注的I/O操作主要是网络数据的接收和发送，以及磁盘文件的访问，我们将其归纳为多种模型，称为I/O模型，它们的本质区别便在于CPU的参与方式。</p>
</blockquote>
</blockquote>
<h5 id="pio与dma" tabindex="-1"><a class="header-anchor" href="#pio与dma" aria-hidden="true">#</a> PIO与DMA</h5>
<blockquote>
<blockquote>
<p>很早以前，磁盘和内存之间的数据传输是需要CPU控制的，也就是说如果我们读取磁盘文件到内存中，数据要经过CPU存储转发，这种方式称为PIO。显然这种方式非常不合理，需要占用大量的CPU时间来读取文件，造成文件访问时系统几乎停止响应。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>后来，DMA（直接内存访问，Direct Memory Access）取代了PIO，它可以不经过CPU而直接进行磁盘和内存的数据交换。在DMA模式下，CPU只需要向DMA控制器下达指令，让DMA控制器来处理数据的传送即可，DMA控制器通过系统总线来传输数据，传送完毕再通知CPU，这样就在很大程度上降低了CPU占有率，大大节省了系统资源，而它的传输速度与PIO的差异其实并不十分明显，因为这主要取决于慢速设备的速度。</p>
</blockquote>
</blockquote>
<h5 id="同步阻塞i-o" tabindex="-1"><a class="header-anchor" href="#同步阻塞i-o" aria-hidden="true">#</a> 同步阻塞I/O：</h5>
<blockquote>
<blockquote>
<p>说到阻塞，首先得说说I/O等待。造成等待的原因非常多，比如Web服务器在等待用户的访问，这便是等待，因为它不知道谁会来访问，所以只能等。随后，当某个用户通过浏览器发出请求，Web服务器与该浏览器建立TCP连接后，又要等待用户发出HTTP请求数据，用户的请求数据在网络上传输需要时间，进入服务器接收缓冲区队列以及被复制到进程地址空间都需要时间。另外，假如浏览器和Web服务器采用HTTP长连接模式，那么在超时关闭连接之前，服务器还要等待浏览器发送其他的请求，这也是I/O等待。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>读取磁盘上某个文件的I/O操作，可能先要等待其他的磁盘访问操作，因为磁头数量是有限的，所以只能一个个排队读取，即使轮到了自己，在磁盘上读取数据本身也要花费时间。值得一提的是，对于RAID磁盘的某些规格（如RAID0），通过磁盘阵列将数据分布在多个磁盘上，大大提高了磁盘访问吞吐率。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>我们说的阻塞是指当前发起I/O操作的进程被阻塞，并不是CPU被阻塞，事实上没有什么能让CPU阻塞的，CPU只知道拼命地计算，对于阻塞一无所知。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>同步阻塞I/O是指当进程调用某些涉及I/O操作的系统调用或库函数时，比如accept()、send()、recv()等，进程便暂停下来，等待I/O操作完成后再继续运行。这是一种简单而有效的I/O模型，它可以和多进程结合起来有效地利用CPU资源，但是其代价就是多进程的大量内存开销。</p>
<p>比如在Apache-prefork模型中，某个子进程在等待请求时，进程阻塞在accept()调用，我们用strace进行跟踪，如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212214820854.png" alt="image-20211212214820854"></p>
<p>可以看出，accept()在等待用户连接的到达，同时该进程停在此处成为阻塞状态。</p>
<p>为了简单说明同步阻塞I/O以及和其他I/O模型的区别，我们举个有意思的例子。比如你去逛街，逛着逛着有点饿了，这时你看到商场里有小吃城，就去一个小吃店买一碗面条，交了钱，可面条做起来得需要时间，你也不知道什么时候可以做好，没办法，只好坐在那里等，等面条做好后吃完再继续逛街。显然，这里的吃面条便是I/O操作，它要等待厨师做面条，还要等待自己把面条吃完。</p>
</blockquote>
</blockquote>
<h5 id="同步非阻塞i-o" tabindex="-1"><a class="header-anchor" href="#同步非阻塞i-o" aria-hidden="true">#</a> 同步非阻塞I/O</h5>
<blockquote>
<blockquote>
<p>同步非阻塞I/O在同步阻塞I/O中，进程实际上等待的时间可能包括两部分，一个是等待数据的就绪，另一个是等待数据的复制，对于网络I/O来说，前者的时间可能要更长一些。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>同步非阻塞I/O的调用不会等待数据的就绪，如果数据不可读或者不可写，它会立即告诉进程。比如我们使用非阻塞recv()接收网络数据的时候，如果网卡缓冲区中没有可接收的数据，函数就及时返回，告诉进程没有数据可读了。相比于阻塞I/O，这种非阻塞I/O结合反复轮询来尝试数据是否就绪，防止进程被阻塞，最大的好处便在于可以在一个进程里同时处理多个I/O操作。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>但正是由于需要进程执行多次的轮询来查看数据是否就绪，这花费了大量的CPU时间，使得进程处于忙碌等待状态。</p>
<p>回到买面条的故事中，假如你不甘心坐着等面条做好，想去顺便逛逛街，可又担心面条做好后没有及时领取，所以你逛一会便跑回去看看面条是否做好，往返了很多次，最后虽然及时地吃上了面条，但是却累得气喘吁吁。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>非阻塞I/O一般只针对网络I/O有效，我们只要在socket的选项设置中使用O_NONBLOCK即可，这样对于该socket的send()或recv()便采用非阻塞方式。值得注意的是，对于磁盘I/O，非阻塞I/O并不产生效果。</p>
</blockquote>
</blockquote>
<h5 id="多路i-o就绪通知" tabindex="-1"><a class="header-anchor" href="#多路i-o就绪通知" aria-hidden="true">#</a> 多路I/O就绪通知</h5>
<blockquote>
<blockquote>
<p>在实际应用中，特别是Web服务器，同时处理大量的文件描述符是必不可少的，但是使用同步非阻塞I/O显然不是最佳的选择，在这种模型下，我们知道如果服务器想要同时接收多个TCP连接的数据，就必须轮流对每个socket调用接收数据的方法，比如recv()。不管这些socket有没有可以接收的数据，都要询问一遍，假如大部分socket并没有数据可以接收，那么进程便会浪费很多CPU时间用于检查这些socket，这显然不是我们所希望看到的。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>多路I/O就绪通知的出现，提供了对大量文件描述符就绪检查的高性能方案，它允许进程通过一种方法来同时监视所有文件描述符，并可以快速获得所有就绪的文件描述符，然后只针对这些文件描述符进行数据访问。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>I/O就绪通知只是帮助我们快速获得就绪的文件描述符，当得知数据就绪后，就访问数据本身而言，仍然需要选择阻塞或非阻塞的访问方式，一般我们选择非阻塞方式，以防止任何意外的等待阻塞整个进程，比如有时就绪通知只代表一个内核的提示，也许此时文件描述符尚未真正准备好或者已经被客户端关闭连接。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>select最早于1983年出现在4．2BSD中，它通过一个select()系统调用来监视包含多个文件描述符的数组，当select()返回后，该数组中就绪的文件描述符便会被内核修改标志位，使得进程可以获得这些文件描述符从而进行后续的读写操作。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>select目前几乎在所有的平台上都支持，其良好跨平台支持也是它的一个优点</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>select的一个缺点在于单个进程能够监视的文件描述符的数量存在最大限制，在Linux上一般为1024，不过可以通过修改宏定义甚至重新编译内核的方式提升这一限制。所以，假如使用了select的服务器已经维持了1024个连接，那么你的请求可能会被拒绝。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>select()所维护的存储大量文件描述符的数据结构，随着文件描述符数量的增大，其复制的开销也线性增长。同时，由于网络响应时间的延迟使得大量TCP连接处于非活跃状态，但调用select()会对所有socket进行一次线性扫描，所以这也浪费了一定的开销。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>poll没有最大文件描述符数量的限制。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>poll和select同样存在的一个缺点就是，包含大量文件描述符的数组被整体复制于用户态和内核的地址空间之间，而不论这些文件描述符是否就绪，它的开销随着文件描述符数量的增加而线性增大。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>select()和poll()将就绪的文件描述符告诉进程后，如果进程没有对其进行I/O操作，那么下次调用select()或poll()的时候将再次报告这些文件描述符，所以它们一般不会丢失就绪的消息，这种方式称为水平触发</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>SIGIO：</strong></p>
<p>Linux 2．4提供了SIGIO，它通过实时信号（Real Time Signal）来实现select/poll的通知方法，但是它们的不同在于，select/poll告诉我们哪些文件描述符是就绪的，一直到我们读写它之前，每次select/poll都会告诉我们；而SIGIO则是告诉我们哪些文件描述符刚刚变为就绪状态，它只说一遍，如果我们没有采取行动，那么它将不会再次告知，这种方式称为边缘触发（Edge Triggered）。SIGIO几乎是Linux 2．4下性能最好的多路I/O就绪通知方法。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>SIGIO也存在一些缺点，在SIGIO机制中，代表事件的信号由内核中的事件队列来维护，信号按照顺序进行通知，这可能导致当一个信号到达的时候，该事件已经过期，它所描述的文件描述符已经被关闭。另一方面，事件队列是有长度限制的，无论你设置多大的上限，总有可能被事件装满，这就很容易发生事件丢失，所以这时候需要采用其他方法来弥补损失。</p>
<p><strong>epoll</strong></p>
<p>Linux 2．6才出现了由内核直接支持的实现方法，那就是epoll，它几乎具备了之前所说的一切优点，被公认为Linux 2．6下性能最好的多路I/O就绪通知方法。</p>
<p>epoll可以同时支持水平触发和边缘触发，理论上边缘触发的性能要更高一些，但是代码实现相当复杂，因为任何意外的丢失事件都会造成请求处理错误。在默认情况下epoll采用水平触发，如果要使用边缘触发，则需要在事件注册时增加EPOLLET选项。在Lighttpd的epoll模型代码（src/fdevent_linux_sysepoll．c）中，可以看到它注释掉了EPOLLET，并没有使用边缘触发方式</p>
<p>epoll同样只告知那些就绪的文件描述符，而且当我们调用epoll_wait()获得就绪文件描述符时，返回的并不是实际的描述符，而是一个代表就绪描述符数量的值，你只需要去epoll指定的一个数组中依次取得相应数量的文件描述符即可，这里也使用了内存映射（mmap）技术，这样便彻底省掉了这些文件描述符在系统调用时复制的开销。</p>
<p>另一个本质的改进在于epoll采用基于事件的就绪通知方式。在select/poll中，进程只有在调用一定的方法后，内核才对所有监视的文件描述符进行扫描，而epoll事先通过epoll_ctl()来注册每一个文件描述符，一旦某个文件描述符就绪时，内核会采用类似callback的回调机制，迅速激活这个文件描述符，当进程调用epoll_wait()时便得到通知。</p>
<p>回到买面条的故事中，虽然有了电子屏幕，但是显示的内容是所有餐品的状态，包括正在制作的和已经做好的，这显然给你造成阅读上的麻烦，就好像select/poll每次返回所有监视的文件描述符一样。如果能够只显示做好的餐品，那该多好，随后小吃城管理处改进了大屏幕，实现了这一点，这就像/dev/poll一样只告知就绪的文件描述符。在显示做好的餐点时，如果只显示一次，而不管你有没有看到，这就相当于边缘触发，而如果在你领取餐点之前，每次都显示，就相当于水平触发。</p>
<p>但尽管这样，一旦你走得比较远，就还得花时间走到小吃城去看电子屏幕，能不能让你更加轻松地获得通知呢？管理处这次采用了手机短信通知的方法，你只需要到管理处注册后，便可以在餐点就绪时及时收到短信通知，这类似于epoll的事件机制。</p>
<p><strong>kqueue：</strong></p>
<p>FreeBSD中实现了kqueue，它像epoll一样可以设置水平触发或边缘触发，同时kqueue还可以用来监视磁盘文件和目录，但是它的API在很多平台都不支持，而且文档相当匮乏。kqueue和epoll的性能非常接近。</p>
<p>对于以上介绍的多种实现方法，在Linux平台的Web服务器中，我们常用的主要有select、poll、SIGIO、epoll，尤其是在Linux 2．6中，epoll成为首要选择。</p>
<p>在安装一些Web服务器软件的时候，configure过程中会检查当前系统支持的多路I/O就绪通知方法，比如Nginx在configure时，其中一部分内容如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212222619898.png" alt="image-20211212222619898"></p>
</blockquote>
</blockquote>
<h5 id="内存映射" tabindex="-1"><a class="header-anchor" href="#内存映射" aria-hidden="true">#</a> <strong>内存映射：</strong></h5>
<blockquote>
<blockquote>
<p>Linux内核提供一种访问磁盘文件的特殊方式，它可以将内存中某块地址空间和我们要指定的磁盘文件相关联，从而把我们对这块内存的访问转换为对磁盘文件的访问，这种技术称为内存映射。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在大多数情况下，使用内存映射可以提高磁盘I/O的性能，它无须使用read()或write()等系统调用来访问文件，而是通过mmap()系统调用来建立内存和磁盘文件的关联，然后像访问内存一样自由地访问文件。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>有两种类型的内存映射，共享型和私有型，前者可以将任何对内存的写操作都同步到磁盘文件，而且所有映射同一个文件的进程都共享任意一个进程对映射内存的修改；后者映射的文件只能是只读文件，所以不可以将对内存的写同步到文件，而且多个进程不共享修改。显然，共享型内存映射的效率偏低，因为如果一个文件被很多进程映射，那么每次的修改同步将花费一定的开销。</p>
<p>Apache 2．x中使用了内存映射，使用strace跟踪了Apache子进程，当时我们请求的URL指向一个很小的静态文件，只有151字节，Apache对于较小的静态文件，选择使用内存映射来读取：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212223113681.png" alt="image-20211212223113681"></p>
<p>我们访问的文件是/data/www/site/htdocs/test．htm，Apache使用open()系统调用打开这个文件，获得文件描述符为20，然后通过mmap2()系统调用完成了共享型内存映射的关联，mmap2()在mmap()的基础上进行了一定的扩展，原理是一样的。随后，Apache读取文件中的内容，这个操作并没有体现出系统调用，因为它只是进程读取地址空间数据的用户态行为。接下来，Apache使用writev()系统调用将HTTP响应数据的头信息和151字节的正文数据合并后发送，然后调用munmap()来撤销映射。</p>
</blockquote>
</blockquote>
<h5 id="直接i-o" tabindex="-1"><a class="header-anchor" href="#直接i-o" aria-hidden="true">#</a> 直接I/O</h5>
<p>在Linux 2．6中，内存映射和直接访问文件没有本质上差异，因为数据从进程用户态内存空间到磁盘都要经过两次复制，即在磁盘与内核缓冲区之间以及在内核缓冲区与用户态内存空间。</p>
<p>引入内核缓冲区的目的在于提高磁盘文件的访问性能，因为当进程需要读取磁盘文件时，如果文件内容已经在内核缓冲区中，那么就不需要再次访问磁盘；而当进程需要向文件中写入数据时，实际上只是写到了内核缓冲区便告诉进程已经写成功，而真正写入磁盘是通过一定的策略进行延迟的。</p>
<p>对于一些较复杂的应用，比如数据库服务器，它们为了充分提高性能，希望绕过内核缓冲区，由自己在用户态空间实现并管理I/O缓冲区，包括缓存机制和写延迟机制等，以支持独特的查询机制，比如数据库可以根据更加合理的策略来提高查询缓存命中率。另一方面，绕过内核缓冲区也可以减少系统内存的开销，因为内核缓冲区本身就在使用系统内存。</p>
<p>Linux提供了对这种需求的支持，即在open()系统调用中增加参数选项O_DIRECT，用它打开的文件便可以绕过内核缓冲区的直接访问，这样便有效避免了CPU和内存的多余时间开销。</p>
<p>在MySQL中，对于Innodb存储引擎，其自身可以进行数据和索引的缓存管理，所以它对于内核缓冲区的依赖不是那么重要。MySQL提供了一种实现直接I/O的方法，在my．cnf配置中，可以在分配Innodb数据空间文件的时候，通过使用raw分区跳过内核缓冲区，实现直接I/O，这在MySQL的官方手册上略有介绍，但是不多，主要涉及raw分区的使用，这是一种特别的分区，它不能像其他分区格式（比如ext2）一样通过mount来挂接使用，而是需要使用raw设备管理程序来加载。为Innodb使用raw分区的配置如下所示：innodb_data_file_path =/dev/sda5:100Gnewraw</p>
<p>假设/dev/sda5是raw分区，在分区大小后面增加newraw关键字，便可以将该raw分区作为数据空间，并由Innodb存储引擎直接访问。</p>
<p>MySQL还提供了innodb_flush_method配置选项，你可以将它设置为如下形式：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212223601433.png" alt="image-20211212223601433"></p>
<p>这样便可以通过另一种方式来实现直接I/O。</p>
<p>与O_DIRECT类似的一个选项是O_SYNC，后者只对写数据有效，它将写入内核缓冲区的数据立即写入磁盘，将机器故障时数据的丢失减少到最小，但是它仍然要经过内核缓冲区。</p>
<h5 id="sendfile" tabindex="-1"><a class="header-anchor" href="#sendfile" aria-hidden="true">#</a> sendfile：</h5>
<p>我们都在向Web服务器请求静态文件，比如图片、样式表等，根据前面的介绍，我们知道在处理这些请求的过程中，磁盘文件的数据先要经过内核缓冲区，然后到达用户内存空间，因为是不需要任何处理的静态数据，所以它们又被送到网卡对应的内核缓冲区，接着再被送入网卡进行发送。</p>
<p>数据从内核出去，绕了一圈，又回到内核，没有任何变化，看起来真是浪费时间。在Linux 2．4的内核中，尝试性地引入了一个称为khttpd的内核级Web服务器程序，它只处理静态文件的请求。引入它的目的便在于内核希望请求的处理尽量在内核完成，减少内核态的切换以及用户态数据复制的开销。</p>
<p>同时，Linux通过系统调用将这种机制提供给了开发者，那就是sendfile()系统调用。它可以将磁盘文件的特定部分直接传送到代表客户端的socket描述符，加快了静态文件的请求速度，同时也减少了CPU和内存的开销。</p>
<p>在OpenBSD和NetBSD中没有提供对sendfile的支持。</p>
<p>还记得在介绍内存映射的时候，我们通过strace的跟踪看到了Apache在处理151字节的小文件时，使用了mmap()系统调用来实现内存映射，但是在Apache处理较大文件的时候，内存映射会导致较大的内存开销，得不偿失，所以Apache使用了sendfile64()来传送文件，sendfile64()是sendfile()的扩展实现，它在Linux 2．4之后的版本中提供。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212224848817.png" alt="image-20211212224848817"></p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212224859492.png" alt="image-20211212224859492"></p>
<p>这里只列出了我们关注的20个系统调用，可以清楚地看到，Apache使用open()系统调用来打开我们请求的文件，获得的文件描述符为20，然后通过writev()系统调用发送出HTTP响应头，接着对1．2MB的正文数据使用sendfile64()进行分段发送，其调用参数中包含了两个文件描述符，分别是代表磁盘文件的20和代表客户端的19。</p>
<p>在这种机制下，我们用ab进行压力测试，得到的结果如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212224920449.png" alt="image-20211212224920449"></p>
<p>sendfile真的发挥预期的作用了吗？我们关掉Apache的sendfile，再来试一次。Apache提供了sendfile开关，修改httpd．conf如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212224935104.png" alt="image-20211212224935104"></p>
<p>这时我们再来请求这个1．2MB的文件，用strace跟踪如下：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212224956015.png" alt="image-20211212224956015"></p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212225004364.png" alt="image-20211212225004364"></p>
<p>的确没有使用sendfile64()，而是用普通的read()和write()多次复制数据。我们看到，前面sendfile64()某一次发送了49152字节的数据，而这里的read()和write()一次发送了8192字节的数据，所以需要更多的发送次数，也就是更多次的系统调用。最后来看看性能上的差距，用ab对关闭sendfile的Apache进行压力测试，结果如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212225018304.png" alt="image-20211212225018304"></p>
<p>相比于前面的590．93 reqs/s，这里的吞吐率确实降低了不少。但是，这并不意味着sendfile在任何场景下都能发挥显著的作用。对于请求较小的静态文件，sendfile发挥的作用便显得不那么重要，通过压力测试，我们模拟100个并发用户请求151字节的静态文件，是否使用sendfile的吞吐率几乎是相同的，可见在处理小文件请求时，发送数据的环节在整个过程中所占时间的比例相比于大文件请求时要小很多，所以对于这部分的优化效果自然不十分明显。我们可以用ab来大概了解一下两者的比例差别，其中Processing部分的时间在这里可以理解为发送文件的时间，而Connect则是指建立TCP连接的时间。对于小文件的测试结果如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212225031411.png" alt="image-20211212225031411"></p>
<h5 id="异步i-o" tabindex="-1"><a class="header-anchor" href="#异步i-o" aria-hidden="true">#</a> 异步I/O：</h5>
<p>阻塞和非阻塞是指当进程访问的数据如果尚未就绪，进程是否需要等待，简单说这相当于函数内部的实现区别，即未就绪时是直接返回还是等待就绪；而同步和异步是指访问数据的机制，同步一般指主动请求并等待I/O操作完毕的方式，当数据就绪后在读写的时候必须阻塞，异步则指主动请求数据后便可以继续处理其他任务，随后等待I/O操作完毕的通知，这可以使进程在数据读写时也不发生阻塞。</p>
<p>POSIX1003．1标准为异步方式访问文件定义了一套库函数，这里的异步I/O（AIO）实际上就是指当用户态进程调用库函数访问文件时，进行必要的快速注册，比如进入读写操作队列，然后函数马上返回，这时候真正的I/O传输还没有开始呢。可以看出，这种机制是真正意义上的异步I/O，而且是非阻塞的，它可以使进程在发起I/O操作后继续运行，让CPU处理和I/O操作达到更好的重叠。</p>
<h3 id="◆-3-7-服务器并发策略" tabindex="-1"><a class="header-anchor" href="#◆-3-7-服务器并发策略" aria-hidden="true">#</a> ◆ 3.7 服务器并发策略</h3>
<blockquote>
<blockquote>
<p>从本质上讲，所有到达服务器的请求都封装在IP包中，位于网卡的接收缓冲区中，这时候Web服务器软件要做的事情就是不断地读取这些请求，然后进行处理，并将结果写到发送缓冲区，这其中包含了一系列的I/O操作和CPU计算，而设计一个并发策略的目的，就是让I/O操作和CPU计算尽量重叠进行，一方面要让CPU在I/O等待时不要空闲，另一方面让CPU在I/O调度上尽量花费最少的时间。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>几种常见的Web服务器并发策略</strong></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>（1）一个进程处理一个连接，非阻塞I/O</strong></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>既然一个进程处理一个连接，那么在并发请求同时到达时，服务器必然要准备多个进程来处理请求。早期的一种方式是采用fork模式，由主进程负责accept()来自客户端的连接，一旦接收连接，便马上fork()一个新的worker进程来处理，处理结束后，这个进程便被销毁。fork()的开销成为影响性能的关键。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>另一种方式是prefork模式，这种方式由主进程预先创建一定数量的子进程，每个请求由一个子进程来处理，但是每个子进程可以处理多个请求。父进程往往只负责管理子进程，根据站点负载来调整子进程的数量，相当于动态维护一个进程池。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对于accept()的方式，有以下两种策略：</p>
<p>● 主进程使用非阻塞accept()来接收连接，当建立连接后，主进程将任务分配给空闲的子进程来处理；</p>
<p>● 所有子进程使用阻塞accept()来竞争接收连接，一旦一个子进程建立连接后，它将继续进行处理。</p>
<p>Apache 2．x便采用上述第2种策略，在这种accept()阻塞竞争的情况下，虽然从代码上看似只有一个子进程的accept()可以返回，但实际上，按大多数TCP栈的实现方法，当一个请求连接到达时，内核会激活所有阻塞在accept()的子进程，但只有一个能够成功获得连接并返回到用户空间，而其余的子进程由于得不到连接而继续回到休眠状态，这种“抖动”也造成了一定的额外开销。</p>
<p>我们知道Apache这种多进程模型的开销限制了它的并发连接数，但是Apache也有自身的优势，比如从稳定性和兼容性的角度看，多进程模型的优势正体现在它相对安全的独立进程，任何一个子进程的崩溃都不会影响Aapche本身，Apache父进程可以创建新的子进程；另一方面，Apache毕竟经过长期的考验和广泛的使用，它的功能模块非常丰富，比如各种动态脚本的支持、虚拟主机管理、URLRewrite、SSL加密、SSI（服务器端静态网页包含）、目录浏览和管理等，而且安装和配置都相当简单，有大量的官方文档可以参考。所以，对于一些并发数要求不高（如150以内）的站点，如果同时对其他功能有所依赖，那么Apache便是非常不错的选择。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>（2）一个线程处理一个连接，非阻塞I/O</strong></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>这种方式允许在一个进程中通过多个线程来处理多个连接，其中每个线程处理一个连接。Apache的worker多路处理模块便采用这种方式，它的目的主要在于减少prefork模式中太多进程的开销，使Apache可以支持更多的并发连接。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>Apache的worker模型可以说是一种多进程和多线程的混合方式，它的MPM配置有些类似于prefork，但是增加了每个进程最大线程数以及最大总线程数的配置，根据配置，Apache主进程会创建很少的子进程，每个子进程又拥有一定数量的线程。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>根据strace的跟踪，我们发现只有一个子进程负责accept()，一旦接收到连接后，便将任务分配给适当的线程，线程中的网络I/O操作使用非阻塞方式。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在实际测试中，这种方式的表现并不比prefork有太大的优势，因为虽然它使用大量线程代替进程，但是这些线程实际上都是由内核进程调度器管理的轻量级进程，它们的上下文切换开销依然存在，我们分别给worker和prefork两种模型的Apache进行同样的压力测试，并用Nmon监视上下文切换。</p>
<p>压力测试模拟100个并发用户，请求151字节的静态文件，一共发送100000个请求。prefork模型的测试结果如下所示：它的平均每秒上下文切换次数如下所示：</p>
<p>吞吐率只增加了1000左右。整个测试过程中，Apache子进程只有4个</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>通过Nmon的监视，我们发现在同样条件的压力测试下，worker的上下文切换次数几乎是prefork的两倍以上。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212231419673.png" alt="image-20211212231419673"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在实际应用中，worker模型的处境比较尴尬，人们几乎很少使用它，因为它的优点并不明显，一旦人们意识到Apache无法满足需要时，便会马上使用其他的轻量级Web服务器。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>（3）一个进程处理多个连接，非阻塞I/O</strong></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>一个进程处理多个连接，存在一个潜在条件，就是多路I/O就绪通知的应用</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>通常我们将处理多个连接的进程称为worker进程，或者服务进程，有些使用这种并发模型的Web服务器支持worker进程数量的配置，比如在Nginx中可以进行配置</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>Lighttpd也可以配置worker进程的数量，默认的配置文件中没有这个参数，其默认值为0，但并不是说没有worker进程，而是由主进程来进行worker进程的工作。当它设置为非0数值（如2）后，则使得lighttpd主进程创建两个worker进程，而主进程则不进行连接处理。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在这种模型中，有时候还会设计独立的listener进程，专门负责接收新的连接，然后分配任务给各个worker，这样做的好处是可以根据各个worker的负载来平衡调度任务，但是任务调度有一定的开销，所以在我们常见的模型中一般都是由worker进程来进行接收。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对于超时连接的管理，理论上也可以由独立的进程来处理，但是为了减少进程切换，一般也在worker中完成。</p>
<p>有些时候Web服务器会使用worker线程来代替worker进程，但是这种线程实际上通常都是内核级线程，它们在处理多路I/O的方式上基本相同，所以可以看成是worker进程的一个迷你版。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>下面我们分别使用select、poll、epoll，通过strace来跟踪lighttpd主进程从检查数据就绪到接收一个请求这一过程中的系统调用。这里我们使用了lighttpd的默认worker数进行配置，由主进程来负责worker的工作。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212232032180.png" alt="image-20211212232032180"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>select在20000并发用户数下的卓越表现的确是事实，但是不要忘了前提条件，我们请求的是151字节的静态文件，每次请求的处理时间非常短，几乎在0．1ms左右，这比一个进程的时间片都要小很多，所以可以快速处理完一个连接后处理其他连接，再加上Lighttpd对于连接队列的良好控制，实际上，lighttpd这时候并没有打开20000个并发连接，通过lighttpd的server-status监视，我们发现它的连接数基本保持在300以内，便足够应付这些请求了。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>另一个问题是，用151 字节的静态文件进行压力测试时，为什么epoll的性能和select/poll没什么区别呢？如果你被这个问题困扰，那么请不要犹豫，马上看看前面关于epoll优势的介绍。事实上，我们的Web服务器通常维护着大量的空闲连接，它们有些可能是由于使用长连接而在等待超时，有些可能是网络传输的延时，还有的甚至是黑客故意制造的死连接。这时候，epoll只关注活跃连接，而不在死连接上浪费时间，但是select和poll会扫描所有文件描述符，比如select在一次系统调用时传入包括了所有socket连接的数组</p>
<p>采用epoll和poll两种方式的系统调用统计中，poll()的总时间要比epoll_wait()多出几乎33倍，占所有系统调用总时间的比例也大大超过epoll_wait()，这就是epoll的优势所在。</p>
<p>当存在大量非活跃连接的时候，epoll的优势不言而喻。一个典型的应用是图片服务器，它们希望为用户提供网页中大量图片的快速下载，所以采用长连接方式，但是这些大量的连接在等待超时关闭前，便处于空闲状态，在这种情况下，epoll依然能够很好地工作。但在其他一些场景下，仔细想想epoll在你的站点中真的发挥优势了吗？</p>
<p>关于worker进程的数量，既然可以由我们来设置，那么是不是越多越好呢？显然不是，别忘了在任何时刻，从CPU的角度来看，只有一个进程在运行。对于不同的动态内容和静态文件，实际要考虑的因素非常多，比如在前面介绍IOWait时提到的例子中，我们通过将Nginx的进程数从128降到8，使得网络I/O流量大幅度提升，但这时候的HTTP并发连接数并不高，我们通过Nginx的stub_status_module可以看到统计结果如下：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212232404146.png" alt="image-20211212232404146"></p>
<p>可以看到，当前活跃的连接有2624个，其中有2142个连接正在发送响应数据。而在Nginx进程数为128时，虽然网络I/O流量降低，但是并发连接数比较高</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211212232429777.png" alt="image-20211212232429777"></p>
<p>大量的worker进程可以维持更多的活跃连接数，但此时每个连接的下载速度要远远小于前者，那么，如何决定worker进程数完全取决于你的倾向，即你希望为更多的用户同时提供慢速下载服务，还是希望为有限的用户提供快速的下载服务。</p>
<p>对于动态内容，比如PHP脚本，worker进程通常只是负责转发请求给独立的fastcgi进程，或者作为反向代理服务器将请求转发给后端服务器，这时候，worker进程并不依赖太多的本地资源，所以为了提高并发连接数，我们可以适当地提高worker进程数，但在一般情况下，动态内容本身的吞吐率是相当有限的，由于存在脚本解释器的开销，通常2000reqs/s的吞吐率就已经很高了，所以worker进程的压力并不是很大，但如果作为基于反向代理的负载均衡调度器时，多台后端服务器扩展了动态内容计算能力，worker进程数会逐渐成为整体性能的瓶颈。当然，太多的worker进程又会带来更多的上下文切换开销和内存开销，从而整体上使所有连接的响应时间变长，所以，没有一个绝对的公式告诉你如何选择worker进程数，而你要做的是根据站点的实际情况来进行分析和调整。</p>
<p><strong>（4）一个线程处理多个连接，异步I/O</strong></p>
<p>即便是拥有高性能的多路I/O就绪通知方法，但是磁盘I/O操作的等待还是无法完全避免，当我们对磁盘文件调用read()或者通过sendfile()直接发送数据时，设置文件描述符为非阻塞没有任何意义，如果需要读取的数据不在磁盘缓冲区，磁盘便开始动用物理设备来读取数据，这个时候整个进程的其他工作都必须等待。</p>
<p>更加高效的方法便是对磁盘文件操作使用异步I/O，在前面比较详细地介绍过异步I/O及其实现，但在实际上，目前很少有Web服务器支持这种真正意义上的异步I/O，理论上在某种特定的场景中它的性能要比sendfile更加出色，但是对于大量小文件的并发请求，文件传送可能不是关键，而多路I/O就绪通知方法的性能更加重要。</p>
</blockquote>
</blockquote>
<h2 id="◆-第4章-动态内容缓存" tabindex="-1"><a class="header-anchor" href="#◆-第4章-动态内容缓存" aria-hidden="true">#</a> ◆ 第4章 动态内容缓存</h2>
<h3 id="◆-4-1-重复的开销" tabindex="-1"><a class="header-anchor" href="#◆-4-1-重复的开销" aria-hidden="true">#</a> ◆ 4.1 重复的开销</h3>
<blockquote>
<blockquote>
<p>4.1 重复的开销</p>
</blockquote>
</blockquote>
<h3 id="◆-4-2-缓存与速度" tabindex="-1"><a class="header-anchor" href="#◆-4-2-缓存与速度" aria-hidden="true">#</a> ◆ 4.2 缓存与速度</h3>
<blockquote>
<blockquote>
<p>由动态内容自行实现的缓存机制，这其中包括整页缓存、局部缓存、数据缓存等，除此之外，还有代码解释器缓存、Web服务器缓存等</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>缓存的目的就是把需要花费昂贵开销的计算结果保存起来，在以后需要的时候直接取出，而避免重复计算，一切缓存的本质都是如此。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>CPU缓存，它是位于CPU和内存之间的临时存储器，它的容量不大，但是交换速度要高于内存，CPU将频繁交换的数据放在缓存中，如果以后需要则直接读取缓存，从而避免访问速度较慢的内存，不可否认，尽管我们认为内存速度已经很快，但是在CPU缓存面前，它还是力不从心。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>缓冲的原意出自物理学，那就是减缓冲击力，在计算机应用场景中，我们使用它的引申含义，其目的在于改善各部件之间由于速度不同而引发的问题。比如将用户态地址空间的数据写入磁盘时，显然内存的速度比磁盘速度要快得多，所以人们设计了磁盘缓冲区，让数据源源不断地流进缓冲区，再由缓冲区负责写入磁盘，这样内存便可以不必随着磁盘的慢节奏来工作，所以磁盘缓冲区起到了将快速设备和慢速设备平滑衔接的作用，另外我们在线观看视频的时候，视频缓冲区的意义也是如此。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>缓冲和缓存有一些相似之处，比如它们都需要一块存储区，而且它们的本质都与速度不一致有关，即便是缓存，如果计算速度和读取缓存的速度差不多，那么它也毫无意义。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>凡是使用缓存，都一定要意识到命中率的重要性</p>
</blockquote>
</blockquote>
<h3 id="◆-4-3-页面缓存" tabindex="-1"><a class="header-anchor" href="#◆-4-3-页面缓存" aria-hidden="true">#</a> ◆ 4.3 页面缓存</h3>
<blockquote>
<blockquote>
<p>对于动态内容缓存的具体方法，你可能并不陌生，也许你在使用类似Smarty的模板框架，或者使用Cakephp、Django、Zend等MVC框架，不管它们支持多么丰富的特性，有一点是相同的，就是将视图（View）和控制器（Controller）进行分离，这样便可以让控制器自身拥有缓存控制权，从而提供丰富灵活的缓存控制方法。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>Smarty缓存</strong></p>
<p><strong>缓存持久化与查找</strong></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>通常，我们将动态内容的缓存存储在磁盘上，通常磁盘有足够而且廉价的空间来存储大量的文件，我们不用担心因为空间的限制而淘汰缓存，这是一种比较简单也容易部署的方法。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>如果缓存文件非常多的话，cache目录下会拥挤大量的文件。首先，这不是没有可能，即便你的站点规模不大，也有可能在不长的时间里毫不费力地积累数万个缓存文件，并且糟糕的是这些缓存文件不会自己删除。一旦cache目录下的文件数量达到数以万计后，CPU花在遍历目录上的时间便非同寻常，如果缓存文件的读写比较频繁的话，可能CPU使用率很容易达到100%。你可以使用支持目录hash等加速目录遍历的文件系统来缓解该情况，如XFS和reiserfs，可是如果你的站点已经在提供服务，则重新安装文件系统显然不太现实。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>我们可以通过缓存目录分级来解决这一问题，Smarty对它提供了支持。</p>
<p>smarty缓存存储的就是序列化后的模板数据，里面保存了写入时间和过期时间戳。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213104449264.png" alt="image-20211213104449264"></p>
<p>它们的值都是UNIX Timestamp格式，且都表示一个时间点，前者对应的是缓存文件的创建时间，后者对应的是该缓存过期的时间，为什么要同时记录这两个时间呢？这是因为它们对应着两种过期检查方法：● 每次检查时，根据缓存的创建时间、缓存有效期设置的长度，以及当前时间，来判断是否过期，也就是说，如果当前时间距离缓存创建时间的间隔超过有效期长度的话，认为缓存过期，这是一种相对比较。● 每次检查时，根据缓存的过期时间和当前时间来判断是否过期，这个方法比较简单，直接检查当前时间是否超过缓存过期时间即可，这是一种绝对比较。</p>
<p>Smarty的这种缓存过期检查方法是否存在一定的开销呢？答案是肯定的，开销是难免的，因为每次请求动态内容时，即使是命中缓存，Smarty仍然要通过is_cached()和display()对缓存文件的内容进行两次分析，每次分析都调用了smarty_core_read_cache_file核心函数，包括提取缓存标志信息和HTML等，这涉及大量的字符串操作。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213104544380.png" alt="image-20211213104544380"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>是否放弃Smarty缓存</strong></p>
<p>我打算放弃Smarty提供的缓存方法，自己来写一个简单够用的缓存方法，设计思路是在缓存文件中只存储HTML，而将文件的修改时间作为过期判断依据，通过PHP的stat()方法可以获得文件的最后一次修改时间。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213104633218.png" alt="image-20211213104633218"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>这次执行动态网页后，我们看到在cache目录下生成了以下文件：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213104647092.png" alt="image-20211213104647092"></p>
<p>进行同样的压力测试，相比于使用Smarty缓存时的173．95 reqs/s，这次的吞吐率只增加了不到10%，而且不要忘了，新的缓存方法只实现了最基本的功能，除此之外一无所有，没有错误处理，没有缓存目录分级，代码量也只有短短几行，仅仅代码计算量的减少带来10%的性能提升，这也不足为怪。</p>
<p>我们必须清楚一个原则，在动态网页加载HTML缓存并终止程序之前，我们要让它尽可能只加载必要的其他文件，这些文件越少越好，比如引用的某些PHP库文件，因为这些PHP库文件可能在直接输出缓存的时候根本不需要。往往我们积极地引入缓存，但是却不舍得花一些时间来考虑哪些计算可以在加载缓存前省略，如果你使用缓存的目的是跳过数据库访问，那么请做得彻底一些，在加载缓存之前，将与数据库访问相关的库文件引用关闭掉。事实上，不只是缓存文件的加载存在磁盘I/O开销，脚本文件和页面模板也一样，不过幸运的是，脚本加速器可以将脚本代码进行缓存和优化。</p>
<p>在输出缓存之前，不要加载没用的东西。例如我们找到了下面这段加载代码，完全可以省略：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213105040327.png" alt="image-20211213105040327"></p>
<p>然后再次测试ab压力：</p>
<p>吞吐率竟然会有如此大的提升，这种差异来自于我们减去了加载Smarty库文件和初始化Smarty对象的时间，前者是磁盘I/O开销，后者是CPU和内存交换开销，我们用strace来跟踪PHP处理进程，观察跟磁盘I/O相关的几个系统调用。</p>
<p>同样是接受100个请求，当缓存检查代码之前包含Smarty库文件的时候，结果如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213105147891.png" alt="image-20211213105147891"></p>
<p>去掉缓存检查代码之前的Smarty库文件引用后，结果如下所示：</p>
<p>****<img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213105213845.png" alt="image-20211213105213845"></p>
<p>在去掉Smarty库文件的引用后，read()和open()的执行次数和执行时间都明显减少，同时文件内存映射的开销也减少了，这些都是吞吐率大幅度提升的原因，当然还有一个主要的原因是Smarty实例初始化的时间也被省掉了，这是脚本解释器的开销。</p>
<p><strong>把缓存放到内存中</strong></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>前面的两种缓存实现都将缓存数据存储在磁盘文件中，每次缓存加载和过期检查都存在磁盘I/O的开销，反过来它也受到磁盘负载的影响，如果这个磁盘同时还运行着如数据库这样的磁盘I/O密集型应用，那么缓存文件的I/O操作便会存在一定的延迟。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>还可以将动态网页的HTML缓存在其他地方，比如本机内存中，借助PHP的APC模块，我们可以轻松地将任何PHP运行时的数据或对象缓存在内存中，这样一来，加载缓存的过程将没有任何磁盘I/O操作。注意这里我们只是使用了APC的数据缓存方法，并没有使用APC的opcode缓存功能</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>APC数据缓存提供了Key/Value的存储方式，即使在保存大量Key的时候也能保证高效的查找性能，所以我们不用为缓存目录分级的事情操心。同时，每个Key都可以设置有效期长度，一旦过期它便会被删除。使用了APC后的代码片段如下所示：</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213111417088.png" alt="image-20211213111417088"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213111434400.png" alt="image-20211213111434400"></p>
<p>473 reqs/s？也许你非常失望，相比于前面的461．08 reqs/s，为什么性能的提高如此微不足道？我们这次可是在内存中读写缓存，避免了之前文件缓存加载时的磁盘I/O开销，为什么效果不明显呢？原因很简单，其实用strace跟踪PHP处理进程，你会发现，在动态网页处理过程中，需要加载的磁盘文件非常多，而缓存文件只是其中的一个，仅仅把它放到内存里，对整体的影响当然微不足道。</p>
<p>与APC类似的另一个PHP缓存扩展是XCache，我们稍微修改一下代码，使用XCache的数据缓存API，修改后的代码如下所示：</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213131026021.png" alt="image-20211213131026021"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213131037419.png" alt="image-20211213131037419"></p>
<p>使用APC和XCache数据缓存方法没有什么本质的区别，测试结果的微小差异并不是绝对的，它们的表现都非常出色。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>缓存服务器：</strong></p>
<p>我们还可以将HTML缓存存储在一台独立的缓存服务器中，利用memcached，我们可以很容易地通过TCP将缓存存储在其他服务器中，而且memcached同样也是使用内存空间来保存缓存数据，减少了不必要的磁盘I/O。另一方面，memcached在存储区中对于每一个key都维护一个过期时间，一旦达到这个过期时间，memcached便会自动删除这个key，这使得我们的过期检查非常容易，只需要在保存缓存数据时指定过期时间即可</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213131310307.png" alt="image-20211213131310307"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>进行压力测试后，结果还可以，毕竟它存在TCP socket操作的开销，但也不是很糟糕，至少比Smarty内置缓存方法的性能要好很多。</p>
<p><strong>如何选择：</strong></p>
<p>我们将前面的几种缓存方法对应的压力测试结果汇总一下，如表4-1所示。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213131429292.png" alt="image-20211213131429292"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>无论是APC还是memcached，都使用内存来存储HTML缓存，以上我们的测试都只针对一个固定的动态网页，每次访问都处于缓存有效期内，所以缓存命中率是100%，而在实际情况中，一个站点包含了大量的动态网页，如果你为缓存分配的内存空间不足以容纳所有的缓存数据，便会使得缓存命中率大幅度下降，吞吐率也会随之降低。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>假如你的站点有数十万个动态网页需要保存HTML缓存，假设每个缓存为100KB，那么10万个缓存的总大小就是1GB，如果你希望为每个缓存保持较长的有效期，那么就需要准备好足够的内存空间</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>就速度而言，memcached不如使用本机内存的速度快，但出于多方面的考虑，结合你的站点规模，也许你会觉得这很值得。首先，Web服务器特别是应用服务器本身的内存是相当宝贵的，它要满足HTTP进程和脚本解释器的大量开销，无法拿出大量的空间来存放HTML缓存；其次，使用本机内存不具备良好的扩展性，一旦缓存数据和站点负载大幅度增加，为了保证较高的缓存命中率，必须加大缓存空间，本机内存显然成为瓶颈，而使用独立的缓存服务器可以便于扩展，构成多台服务器组成的分布式缓存系统。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对于小规模或者初创时期的Web站点，如果需要缓存的动态网页比较少，这时候使用APC内存缓存仍然不失为一个快速有效的方案，即便是规模膨胀后，快速重构缓存机制并且迁移缓存数据也不是一件多么复杂的事情，从前面的代码片段可以看出，从APC过渡到memcached非常容易</p>
</blockquote>
</blockquote>
<h3 id="◆-4-4-局部无缓存" tabindex="-1"><a class="header-anchor" href="#◆-4-4-局部无缓存" aria-hidden="true">#</a> ◆ 4.4 局部无缓存</h3>
<blockquote>
<blockquote>
<p>对于有些特殊的动态网页，需要页面中某一块区域的内容及时更新，比如一个新闻正文页面的阅读量统计区域和评论区域，如果为了这一块区域的及时更新，就将整个页面重新创建缓存的话，的确有点不值得。在流行的模板框架中，在整页缓存的基础上，都提供了局部无缓存的支持，它允许在页面中指定一块包含动态数据的HTML代码段，每次这些动态数据都需要实时计算，然后和其余部分的缓存合成为最终的网页。</p>
<p>Smarty中要实现局部无缓存，可以增加一个模板扩展标记，并且注册到Smarty对象中，如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213131740492.png" alt="image-20211213131740492"></p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213131749985.png" alt="image-20211213131749985"></p>
</blockquote>
</blockquote>
<h3 id="◆-4-5-静态化内容" tabindex="-1"><a class="header-anchor" href="#◆-4-5-静态化内容" aria-hidden="true">#</a> ◆ 4.5 静态化内容</h3>
<h5 id="直接访问缓存" tabindex="-1"><a class="header-anchor" href="#直接访问缓存" aria-hidden="true">#</a> 直接访问缓存</h5>
<blockquote>
<blockquote>
<p>为什么不让用户直接请求HTML缓存呢</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>之前我们是在静态网页所在的服务器上执行测试，数据没有流出服务器，所以不存在出口带宽限制的问题。而在实际情况中，吞吐率往往受限于Web服务器的出口带宽，比如我们购买了100M独享带宽，那么按照这个13KB的网页来计算，如果不考虑IP包的其他信息长度，理论上最大的吞吐率为：((100Mbit / 8) × 1000) / 13KB = 961．53 reqs/s</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>要想让静态化页面达到或接近理论上的吞吐率，你可以购买更多的独享带宽（如1G带宽），然后为服务器安装千兆网卡，服务器所在网络也要使用千兆交换机，这样一来，静态化页面的性能才可以发挥到极限。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>另一方面，我们做一个比较现实的估算，如果仅仅站点页面就需要消耗1G的带宽，那么页面中的组件比如视频、图片、样式表、JS脚本等，其消耗的带宽将会在数倍以上，这要根据站点的性质来决定，以一个最极端的要数视频网站为例，根据最近Alexa上对于youtube的保守统计，它的PV平均每天至少为1,776,895,200，那么平均每秒的吞吐率为20566 reqs/s，注意这只是HTML网页的吞吐率，而且其中不包括大量通过AJAX加载的局部页面，如果这些网页都至少包含一个视频，每个视频平均按照10MB来计算，那么每天的视频流量为17,352,492GB，平均每秒为200GB，换算成比特，即视频总带宽为1600Gbit/s，别忘了这还只是平均计算值，实际在访客高峰时期还要更多。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>尽管静态化网页的性能要大大高于动态缓存的性能，但是我们要知道，静态化网页在请求的时候不涉及内容计算，这并不代表它不需要计算，它仍然需要由动态程序来创建和更新，我们不可能手动维护一个站点的所有静态网页，那简直无法想象，与之相比，动态内容缓存的一部分优越性便体现在这一点，通过封装良好的缓存管理机制，它成为名副其实的缓存代理人。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>更新策略对于静态化内容的更新策略，一般有以下两种：● 在数据更新时重新生成静态化内容。● 定时重新生成静态化内容。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对于前者，在数据更新的时候重建静态化缓存，往往由用户的某些动作触发，比如新闻站点的网络编辑发表一篇新闻后，程序便创建一个新的静态化新闻页面，同时更新新闻列表页面。显然，这种方式在数据更新频繁时存在大量重建静态内容的开销，尤其是当一个动作引发大量静态内容需要更新时，比如大型新闻站点的CMS（内容管理系统）在工作高峰期间，所有编辑都修改新闻标题而引发大量的页面或局部页面频繁更新，同时伴随着频繁的数据库操作，这将导致CMS系统的服务响应大幅度降低，当然，这可能并不直接影响现存静态化页面的访问，但是不要忘了，这种更新机制也正是静态化缓存方案的一部分，所以它的性能也至关重要。另一方面，如果站点的静态化内容需要分发到更多的服务器，那么频繁的更新也会给文件的同步带来较大的压力</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>一个常用的办法是引入延迟更新机制，将更新任务放入队列，一旦队列写满或者达到超时时间，便一次性将它们更新到磁盘，这听起来有点像文件系统的磁盘缓冲区的设计动机，没错，你也可以把它理解为静态化缓冲区。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>另一种更新策略是定时重建静态化内容，它一般通过定时任务来执行，比如crontab或者专用的daemon程序，然后通过CMS系统进行方便的管理。如果站点存在大量的静态化内容，而且它们的性质和实时性需求不尽相同，我们可以通过维护一定的对应关系来指定特定范围的静态化内容进行重建，一个常见的例子是，对于热门滚动新闻列表进行每隔1分钟的定时更新，而对于其他重要级别稍低的内容可以延长更新时间，从而节省开销。</p>
</blockquote>
</blockquote>
<h5 id="局部静态化" tabindex="-1"><a class="header-anchor" href="#局部静态化" aria-hidden="true">#</a> 局部静态化</h5>
<blockquote>
<blockquote>
<p>静态网页也可以不必整页更新，它可以通过SSI（服务器端包含）技术实现各个局部页面的独立更新，这样便大大节省了重建整个网页时的计算开销和磁盘I/O开销，甚至包括分发时的网络I/O开销。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>SSI技术现在可以在任何一个主流的Web服务器中找到相应的模块，比如Apache的mod_include和Lighttpd的mod_ssi</p>
<p>我找来一个站点的首页index．shtml，它自身的大小为8472个字节，同时它包含了19个子页面，分别通过include方法进行加载，最终的总大小为50456个字节，Web服务器使用Lighttpd对这个静态化的首页进行压力测试，结果如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213132626416.png" alt="image-20211213132626416"></p>
<p>吞吐率为1445．72 reqs/s，我想这个结果对于50KB左右的静态网页来说确实不高，是否它在处理的时候加载了太多的其他文件，导致处理时间延长呢？答案是肯定的，我想如果你在阅读的时候没有直接跳到这里的话，你一定对磁盘I/O的缓慢速度印象深刻。接下来，我们将最终合成的网页直接保存在服务器上，取名为index_without_ssi．shtml，然后对它进行同样的压力测试，结果如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213132647434.png" alt="image-20211213132647434"></p>
<p>吞吐率提高了近一倍！可见打开这19个子页面确实消耗了不少开销，我们用strace跟踪lighttpd进程来看看磁盘I/O的变化。在使用SSI时候，结果如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213132705131.png" alt="image-20211213132705131"></p>
</blockquote>
<p>在不使用SSI的时候，结果如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213132726355.png" alt="image-20211213132726355"></p>
</blockquote>
<blockquote>
<blockquote>
<p>同样都是接受了1000个请求，在使用SSI的时候，open()和stat64()的执行次数较多，等待时间也较长，所占时间比例较大，它们用于打开文件和查看文件状态，是服务器加载子页面时执行的系统调用，同时，用于传输文件的sendfile64()系统调用所占比例较小。而在不使用SSI的时候，sendfile64()占据了主要时间，也就是说更多的时间都花在了传输文件上，所以吞吐率要高一些。</p>
<p>但是，2681．50 reqs/s的吞吐率似乎还是不高，还记得我们之前对13KB的静态网页压力测试可以达到上万的吞吐率吗？那么这个50KB的静态网页也不至于降低到如此地步吧？问题就出在SSI的原理上，一旦网页支持SSI，那么每次请求的时候服务器必须要通读网页内容，查找include标签，这需要大量的CPU开销。</p>
<p>请求的文件越大，花在文件传输上的时间越多，单位时间的数据传输量也越大，同时花在处理socket连接以及打开文件等的时间比例越小</p>
</blockquote>
</blockquote>
<h2 id="◆-第5章-动态脚本加速" tabindex="-1"><a class="header-anchor" href="#◆-第5章-动态脚本加速" aria-hidden="true">#</a> ◆ 第5章 动态脚本加速</h2>
<blockquote>
<blockquote>
<p>我们知道加载缓存仍然需要动态脚本的运行，那么，为了提高动态内容的处理速度，我们还能做些什么呢？</p>
</blockquote>
</blockquote>
<h3 id="◆-5-1-opcode缓存" tabindex="-1"><a class="header-anchor" href="#◆-5-1-opcode缓存" aria-hidden="true">#</a> ◆ 5.1 opcode缓存</h3>
<h5 id="什么是opcode" tabindex="-1"><a class="header-anchor" href="#什么是opcode" aria-hidden="true">#</a> 什么是opcode</h5>
<blockquote>
<blockquote>
<p>也许你曾经尝试过用C/C++编写动态内容，虽然开发过程极其烦琐，但为了获得性能提升，这样做或许是值得的，它们可将动态内容编译成二进制可执行文件，也就是目标代码，由操作系统进程直接装载运行。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>比如PHP、Ruby、Python，它们都属于解释型语言，所以用它们编写的动态内容都需要依赖相应的解释器程序来运行。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>解释器程序也是一个二进制可执行文件，比如/bin/ruby，它同样可以直接在进程中运行，在运行过程中，解释器程序需要对输入的脚本代码进行分析，领会它们的旨意，然后执行它们。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>解释器核心引擎根本看不懂这些脚本代码，无法直接执行，所以需要进行一系列的代码分析工作，当解释器完成对脚本代码的分析后，便将它们生成可以直接运行的中间代码，也称为操作码（Operate Code, opcode）。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>从程序代码到中间代码的这个过程，我们称为解释（parse），它由解释器来完成。与此相似的是，编译型语言中的编译器（如C语言的编译器gcc），也要将程序代码生成中间代码，这个过程我们称为编译（compile）。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>编译器和解释器的一个本质不同在于，解释器在生成中间代码后，便直接执行它，所以运行时的控制权在解释器；而编译器则将中间代码进一步优化，生成可以直接运行的目标程序，但不执行它，用户可以在随后的任意时间执行它，这时控制权在目标程序，和编译器没有任何关系。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>解释和编译本身而言，它们的原理是相似的，都包括词法分析、语法分析、语义分析等，所以，有些时候将解释型语言中生成opcode的过程也称为“编译”，需要你根据上下文来理解。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>因为解释器每次运行的时候都将脚本代码作为输入数据来分析，所以它的数据结构可以动态改变，这使得解释型语言具备了很多丰富的动态特性，在开发和调试中有很多优势，特别是一些流行的Web开发框架，其中的一些特性如果没有动态语言的支持是无法实现的</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>PHP解释器的核心引擎为Zend Engine</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>需要安装PHP的Parsekit扩展，它是一个用C编写的二进制扩展，由PECL来维护。有了Parsekit扩展后，我们就可以通过它提供的运行时API来查看任何PHP文件或者代码段的opcode。我们直接在命令行中调用parsekit_compile_string()，如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213133346057.png" alt="image-20211213133346057"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213133609399.png" alt="image-20211213133609399"></p>
<p>opcode和汇编代码非常相似，解释器核心引擎正是沿用了这种思想，将所有的操作抽象为类似汇编语言一样的操作码形式，这种操作码称为三地址码，它是中间代码的一种理想的表现形式，顾名思义，它的每一个运算由不超过三个地址组成：op1、op2、result，它们可以表示多种运算形式。</p>
</blockquote>
</blockquote>
<h5 id="生成opcode" tabindex="-1"><a class="header-anchor" href="#生成opcode" aria-hidden="true">#</a> 生成opcode</h5>
<blockquote>
<blockquote>
<p>我们的脚本代码可以看成是一系列单词的集合，这些单词包括关键字、标识符、运算符等，所以解析器首先需要对所有单词进行分类，并给它们打上记号（token），这个过程称为词法分析。我们在PHP源代码的Zend目录中，可以找到PHP解释器的词法规则文件，其中便有print对应的记号：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213134144584.png" alt="image-20211213134144584"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>可见，print对应的记号是T_PRINT。如果所有代码都顺利通过词法分析后，接下来，解释器要对这些记号进行语法分析：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213134216383.png" alt="image-20211213134216383"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>语法分析器将T_PRINT标记以及上下文替换成了zend_do_print()函数，我们接着找到这个函数的实现代码，如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213134237198.png" alt="image-20211213134237198"></p>
<p>以上的zend_do_print()函数实现了到opcode的转换，它设置了opcode的指令以及op1的数据，这样一来我们便得到了opcode。所有opcode指令都用整数来表示，以下是它们的宏定义：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213134327754.png" alt="image-20211213134327754"></p>
</blockquote>
</blockquote>
<h5 id="避免重复编译" tabindex="-1"><a class="header-anchor" href="#避免重复编译" aria-hidden="true">#</a> 避免重复编译</h5>
<blockquote>
<blockquote>
<p>生成opcode的开销肯定存在，甚至非常可观</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>通过opcode缓存，来避免重复的opcode编译。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>要缓存opcode，我们在应用层是无能为力的。幸运的是，有一些优秀的opcode缓存器扩展，比如PHP可以选择APC、eAccelerator、XCache等，它们都可以将opcode缓存在共享内存中，而且你几乎不需要修改任何代码。</p>
</blockquote>
</blockquote>
<h5 id="apc" tabindex="-1"><a class="header-anchor" href="#apc" aria-hidden="true">#</a> APC</h5>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213134641232.png" alt="image-20211213134641232"></p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213134713706.png" alt="image-20211213134713706"></p>
<blockquote>
<blockquote>
<p>如此大的性能提升，不仅因为避免了重复的opcode编译开销，还得益于APC将opcode缓存在高速的内存中。既然将opcode缓存在内存中，我们就必须了解内存的使用情况以及缓存命中率，APC提供了这样的API，你可以通过它获得APC运行时的一些必要信息</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213134743954.png" alt="image-20211213134743954"></p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213134755643.png" alt="image-20211213134755643"></p>
<p>通过这些信息，我们可以知道当前有哪些PHP程序缓存了opcode，以及它们的命中率和内存使用情况。另外，你也许看到了被缓存文件的inode值，这是文件系统中文件的唯一索引号，操作系统可以通过这个索引号快速找到文件，比如这里的place_posts．php，我们可以查看它的inode值：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213134831745.png" alt="image-20211213134831745"></p>
<p>为了更好地监控opcode内存缓存的状态，我们完全可以利用Cacti等监控系统来定时获取这些信息，描绘出需要的曲线图。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>缓存过期检查也是缓存机制中的一个重要部分，在默认情况下，缓存了opcode的动态程序在每次请求时，都需要检查程序是否有所变化，如果发现程序自上次访问后被修改，则重新编译opcode。APC提供了一种跳过过期检查的机制，如果动态程序长期不会变化，那么可以跳过过期检查以获得更好的性能表现。要跳过过期检查，可以修改以下配置：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213135017017.png" alt="image-20211213135017017"></p>
<p>需要注意的是，在这种情况下，如果程序代码发生了修改，则必须通过重启Web服务器来使其生效。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>使用opcode缓存机制可以大大减少动态内容的处理时间，这也意味着减少了一定的CPU和内存开销。脚本解释器通常运行在Web服务器的进程中（如Apache-prefork模型的子进程），或者以fastcgi进程的形式独立运行。</p>
<p>在没有使用opcode cache之前，用户态CPU使用率较高，这主要花在了解释器生成opcode，而内核态CPU使用时间相应被挤占，同时内存的剩余量也较小；当使用了opcodecache之后，用户态CPU使用率明显降低，更多的时间用于内核态CPU的使用，比如发起I/O操作的系统调用，同时内存的剩余量也有所提高。</p>
<p>并不是所有的动态内容都在应用了opcode cache之后有大幅度的性能提升，因为opcode cache的目的是减少CPU和内存开销，如果动态内容的性能瓶颈不在于CPU和内存，而在于I/O操作，比如数据库查询带来的磁盘I/O开销，那么opcode cache的性能提升是非常有限的。从前面的对比数据中可以看到，place_posts．php在没有使用任何动态缓存时，每次请求都需要多次数据库访问，在没有使用opcode cache时，它的吞吐率是51．59 reqs/s，而使用了opcode cache后，吞吐率只增加到了92．96 reqs/s，与应用了动态缓存后opcode cache的吞吐率提升幅度相比，它显得无足轻重。</p>
</blockquote>
</blockquote>
<h5 id="xcache" tabindex="-1"><a class="header-anchor" href="#xcache" aria-hidden="true">#</a> XCache</h5>
<blockquote>
<blockquote>
<p>另一款opcode缓存器——XCache</p>
<p>APC和XCache不相上下，这里需要指出，从数值上看，它们的差距不是绝对的，实际中选择APC，还是XCache，或者eAccelerator，并没有定论，它们都在不断地成长和完善，只要了解了opcode的本质以及性能测试的方法，我想如何选择的问题你完全可以自己搞定。</p>
</blockquote>
</blockquote>
<h3 id="◆-5-2-解释器扩展模块" tabindex="-1"><a class="header-anchor" href="#◆-5-2-解释器扩展模块" aria-hidden="true">#</a> ◆ 5.2 解释器扩展模块</h3>
<blockquote>
<blockquote>
<p>充分考虑扩展模块可能对性能带来的副作用</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>举个例子，对于一些PHP开发者来说，要想直接在Web应用程序中引用Java类库，就得在PHP中加载Java扩展模块：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213135756233.png" alt="image-20211213135756233"></p>
<p>在加载了Java扩展模块后，我们这里并不打算执行包含Java代码的应用程序，而只是编写了一个简单的PHP程序，代码如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213135822354.png" alt="image-20211213135822354"></p>
<p>我们来看看压力测试的结果：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213135838986.png" alt="image-20211213135838986"></p>
<p>吞吐率居然只有381．71reqs/s，我们去掉Java扩展模块后，再次进行测试，结果如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213135852914.png" alt="image-20211213135852914"></p>
<p>PHP解释器每次都需要初始化Java解释器，这带来了巨大的开销，不过，如果这样做可以解决你的迫切需求，那无可厚非，或许性能对你来说是其次的，但是，我不理解为什么Java扩展模块要设计得如此匪夷所思，即便是PHP代码中并没有实际使用Java扩展，仍然会初始化Java解释器，这样做不是很合理。</p>
</blockquote>
</blockquote>
<h3 id="◆-5-3-脚本跟踪与分析" tabindex="-1"><a class="header-anchor" href="#◆-5-3-脚本跟踪与分析" aria-hidden="true">#</a> ◆ 5.3 脚本跟踪与分析</h3>
<blockquote>
<blockquote>
<p>动态内容在计算过程中，还有很多开销是无法通过opcode缓存来避免的，如何让脚本执行得更快呢？至少我们需要知道时间主要消耗在哪些代码上，这样才可以进一步分析，究竟这些开销来自于脚本程序本身，还是其他外部的原因。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>我们需要了解如何测量脚本程序中各处代码的执行时间，为此，代码跟踪是少不了的，我们必须掌握脚本的高级调试技巧，几乎所有脚本语言都内置了或多或少的调式方法，但是也许它们并不专业，而且不够完整。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>Xdebug是一个PHP的PECL扩展，它提供了一组用于代码跟踪和调试的API。</p>
</blockquote>
</blockquote>
<h5 id="代码段执行时间" tabindex="-1"><a class="header-anchor" href="#代码段执行时间" aria-hidden="true">#</a> 代码段执行时间</h5>
<blockquote>
<blockquote>
<p>当我们需要计算任意一段代码的执行时间时，一般会在开始处获得时间，然后在结束处再次获得时间，计算出时间差并打印到屏幕或者日志中。Xdebug提供了更加高效的替代方法，你可以在程序中的任何位置调用xdebug_time_index()方法，它将返回从脚本开始处执行到该位置所花费的时间。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213140128575.png" alt="image-20211213140128575"></p>
</blockquote>
</blockquote>
<h5 id="上下文信息收集" tabindex="-1"><a class="header-anchor" href="#上下文信息收集" aria-hidden="true">#</a> 上下文信息收集</h5>
<blockquote>
<blockquote>
<p>为了配合代码跟踪收集信息，我们往往需要在调式信息中记录当前的上下文信息，比如当前的行号、在哪里被调用等。PHP内置了一套预定义常量，比如__LINE__记录了当前的代码行号，但是，它们的行为非常愚蠢，我想你和我一样都曾经对它们感到失望。比如以下这段PHP代码，我希望在函数add()中获得它被调用的上下文信息，可是结果却并不是我想要的，无论add()函数在哪里被调用，结果总是一样的。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213140250928.png" alt="image-20211213140250928"></p>
<p>与此相比，Xdebug要更加懂得我的心思，它提供了类似的几个方法，但不是预定义常量的形式。我们看以下修改后的代码，它可以聪明地告诉我，add()函数是在do_add()函数中被调用，并且被调用的行号是9，完全正确。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213140332174.png" alt="image-20211213140332174"></p>
</blockquote>
</blockquote>
<h5 id="代码覆盖范围" tabindex="-1"><a class="header-anchor" href="#代码覆盖范围" aria-hidden="true">#</a> 代码覆盖范围</h5>
<blockquote>
<blockquote>
<p>Xdebug还提供了一个非常不错的跟踪功能，它可以告诉你，代码在执行过程中“走”过了哪些行，这对于逻辑和分支比较复杂的程序来说，的确是一个不错的功能，我们来看看下面的代码，当然，这只是一个简单的例子，我们根据结果数据将代码中执行的语句加粗表示。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213140518172.png" alt="image-20211213140518172"></p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213140528267.png" alt="image-20211213140528267"></p>
<p>Xdebug还提供了强大的栈跟踪功能，它可以在程序发生异常的时候，收集当前的上下文数据，类似Core dump帮助我们调试程序</p>
</blockquote>
</blockquote>
<h5 id="函数跟踪" tabindex="-1"><a class="header-anchor" href="#函数跟踪" aria-hidden="true">#</a> 函数跟踪</h5>
<blockquote>
<blockquote>
<p>Xdebug的另一个重要跟踪功能便是函数跟踪，它可以根据程序在实际运行时的执行顺序，跟踪记录所有函数的执行时间，以及函数调用时的上下文，包括实际参数和返回值。</p>
<p>值得一提的是，Xdebug提供的函数跟踪功能非常强大，一旦你开启这个选项，它便可以在动态内容执行的过程中，自动跟踪并将记录的数据保存在你指定的目录下。根据你的需要，可以设置各种级别的记录模式，它们分别提供了不同类型的记录格式，比如可以记录每个函数的实际参数内容，但如果你觉得没有必要，也可以只记录这些参数的数据类型和长度，我们采用了后者。另外，它还可以记录每个函数所在的文件名，为了突出重点，我们暂时屏蔽了文件名的记录。</p>
<p>在开始跟踪之前，你还需要在php．ini中设置记录文件的存储目录和文件名前缀，尽量根据自己的环境来做好规划，这将有助于你长期维护不同历史阶段的记录文件，它们也许在若干时间后还能派得上用场。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213140756440.png" alt="image-20211213140756440"></p>
<p>我们开启跟踪模式，首先访问那个没有任何页面缓存和opcode缓存的动态网页，当时我们压力测试的结果是51．59 reqs/s。我们从Xdebug的记录文件中截取了一些片段，如下所示，内容很容易理解，左边第一列的时间代表从脚本开始执行到此处的时间长度，是不断累积的，单位是秒。我们来看第一段：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213140831418.png" alt="image-20211213140831418"></p>
<p>注意粗体部分的时间跨度，很显然，Smarty初始化消耗的时间明显比其他代码的执行时间多出很多。我们继续往下看：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213140859402.png" alt="image-20211213140859402"></p>
<p>以上片段要说明的是mysql_connect()消耗的等待时间，的确，建立TCP连接相比于前后的代码要消耗更多的时间，如果MySQL部署在不同的网络，那么这部分时间可能还要更长。下面的代码片段涉及数据库查询：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213141005280.png" alt="image-20211213141005280"></p>
<p>上面的mysql_query()，这里它消耗了大约0．0055s（5．5ms）的时间</p>
<p>现在，我们对这个动态网页开启APC opcode cache，但并不使用任何页面动态缓存，再次访问后，我们在记录文件中找出几处片段来和前面比较一下：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213141145761.png" alt="image-20211213141145761"></p>
<p>有了opcode缓存后，Smarty的初始化时间大大减少了，只需要0．2ms。再来看下面的数据库查询，别忘了我们只是使用了opcode cache，所以它还是需要访问数据库获取内容。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213141158114.png" alt="image-20211213141158114"></p>
<p>mysql_query()的等待时间为5．6ms，和前面的时间几乎相同，也就是说数据访问的时间仍然不变，因为通过前面的介绍你已经了解opcode cache的目的。</p>
</blockquote>
</blockquote>
<h5 id="瓶颈分析" tabindex="-1"><a class="header-anchor" href="#瓶颈分析" aria-hidden="true">#</a> 瓶颈分析</h5>
<blockquote>
<blockquote>
<p>Xdebug同样为我们提供了性能跟踪器（Profiler），它的工作方式类似于函数跟踪，也是在脚本程序运行的时候自动将性能记录文件写入我们指定的目录中，我们可以设置如下：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213141846075.png" alt="image-20211213141846075"></p>
<p>后边的“%p”是运行时PHP解释器所在进程的PID。</p>
<p>我们可以使用图形界面的分析工具来分析这些性能记录文件，Linux KDE下可以使用KCacheGrind，Windows下可以使用WinCacheGrind，它们可以直接打开性能记录文件，我们这里用WinCacheGrind打开前面访问动态网页时的性能记录文件，界面如图5-7所示。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213141912990.png" alt="image-20211213141912990"></p>
</blockquote>
</blockquote>
<h2 id="◆-第6章-浏览器缓存" tabindex="-1"><a class="header-anchor" href="#◆-第6章-浏览器缓存" aria-hidden="true">#</a> ◆ 第6章 浏览器缓存</h2>
<h3 id="◆-6-1-别忘了浏览器" tabindex="-1"><a class="header-anchor" href="#◆-6-1-别忘了浏览器" aria-hidden="true">#</a> ◆ 6.1 别忘了浏览器</h3>
<h5 id="浏览器不只是用户的" tabindex="-1"><a class="header-anchor" href="#浏览器不只是用户的" aria-hidden="true">#</a> 浏览器不只是用户的</h5>
<blockquote>
<blockquote>
<p>尽可能地让Web站点的内容缓存在用户浏览器中，这样将在一定程度上减少了服务器的计算开销，而且也避免了有些内容由于不必要的重复传输而带来的带宽浪费</p>
</blockquote>
</blockquote>
<h5 id="缓存放在哪里" tabindex="-1"><a class="header-anchor" href="#缓存放在哪里" aria-hidden="true">#</a> 缓存放在哪里</h5>
<blockquote>
<blockquote>
<p>浏览器一般会在用户的文件系统中创建一个目录，用于存放缓存文件，并给每个缓存文件打上一些必要的标记，比如过期时间等。不同的浏览器采用不同的方式来存储缓存，了解这些内容将有助于我们准确地使用浏览器缓存。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>IE浏览器在用户本地设置了临时文件目录，你可以在IE的缓存设置中找到对应的路径</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对于Firefox浏览器，它存储缓存文件的方式有所不同。它并不像IE那样将每个文件独立存储，而是采用二进制文件的方式来存储和管理缓存文件</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>这种二进制存储方式不便于我们了解每个URL缓存文件的状态细节，所以Firefox浏览器提供了一种简单的方式来帮助我们查看所有缓存内容，只需要在浏览器地址栏输入about:cache即可</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>Firefox浏览器在使用磁盘来存储缓存文件的同时，还使用了内存，它将命中率较高的缓存内容同时也装入内存中，这样浏览器在查找缓存的时候，将先在高速内存中查找，如果内存中没有需要的缓存，便前往磁盘缓存目录中继续查找。对于有些用户来说，这种从内存到磁盘的多级缓存机制带来了更快的缓存加载速度，比如用户在一个站点下浏览不同的资讯页面，而这些页面有着大量的重复内容，比如样式表、脚本、公共图片等，由于它们的命中率较高，所以浏览器会将它们装入内存。这种多级缓存的应用很常见，比如CPU的L1 Cache和L2 Cache。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213142509357.png" alt="image-20211213142509357"></p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213142517363.png" alt="image-20211213142517363"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>看到IE和Firefox的这些缓存内容后，你也许注意到它们的过期时间、上次修改时间、上次检查时间等，既然是缓存，那么过期检查肯定是少不了的，那么浏览器是如何完成的呢？仔细一想便知道，浏览器和Web服务器进行沟通的唯一途径便是HTTP，那么如果浏览器希望了解本地的内容是否过期，当然也得通过HTTP和服务器进行沟通。可见，有效地使用浏览器缓存，其本质在于对HTTP协议缓存部分的深入理解。</p>
</blockquote>
</blockquote>
<h3 id="◆-6-2-缓存协商" tabindex="-1"><a class="header-anchor" href="#◆-6-2-缓存协商" aria-hidden="true">#</a> ◆ 6.2 缓存协商</h3>
<blockquote>
<blockquote>
<p>服务器端的动态内容缓存包括缓存内容的创建、存储，以及过期检查等一系列过程，它们都可以由诸如PHP这样的服务器端动态程序来实现。然而，对于浏览器缓存，就完全不是那么回事了，缓存内容存储在浏览器本地，而内容由Web服务器生成，任何一方都不可能独立完成这一系列过程，所以它们之间必须有一种沟通机制，这就是HTTP中的“缓存协商”。</p>
</blockquote>
</blockquote>
<h5 id="如何协商" tabindex="-1"><a class="header-anchor" href="#如何协商" aria-hidden="true">#</a> 如何协商</h5>
<blockquote>
<blockquote>
<p>当浏览器向Web服务器请求一些内容时，Web服务器需要告诉浏览器哪些内容可以被缓存，一旦浏览器知道某个内容可以缓存后，下次当浏览器需要请求这个内容时，它便不会直接向服务器请求完整内容，而是询问服务器是否可以使用本地的缓存，服务器在收到浏览器的询问后需要作出果断的回应，到底是允许浏览器使用本地缓存还是将最新的内容传回浏览器。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213153619115.png" alt="image-20211213153619115"></p>
<p>可以看到HTTP响应的状态码为200，这意味着Web服务器将请求的结果内容全部返回给浏览器，长度为10个字节。对于上面这一组请求和响应我们并不陌生，它们协作得非常成功。然而我们关注的是重复请求时的情况，我们连续刷新了好几次浏览器，注意，是单击浏览器的刷新按钮，或者按F5键，浏览器在随后的几次请求并没有使用本地的缓存，也没有向服务器作出任何表示。我们打开IE的临时文件目录，可以看到刚才请求的http_cache．php缓存文件，如图6-8所示。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213153808219.png" alt="image-20211213153808219"></p>
</blockquote>
</blockquote>
<h5 id="last-modified" tabindex="-1"><a class="header-anchor" href="#last-modified" aria-hidden="true">#</a> Last Modified：</h5>
<blockquote>
<blockquote>
<p>事实上动态内容一般不存在传统意义上的最后修改时间，静态文件可以通过stat()系统调用获得它在物理文件系统中的最后修改时间，一般Web服务器会为静态文件的HTTP响应头自动生成最后修改时间。我们来为HTTP响应增加最后修改时间的标记，修改代码后如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213154034980.png" alt="image-20211213154034980"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>这意味着我们为动态内容增加了一些HTTP响应头信息。我们刷新浏览器，再次请求http_cache．php，浏览器同样发出了我们熟悉的请求：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213154128334.png" alt="image-20211213154128334"></p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213154146919.png" alt="image-20211213154146919"></p>
<p>HTTP协议中规定使用GMT时间，也就是格林威治标准时间，而我们国家使用的是GMT+8时区，所以在HTTP头信息中的时间会比我们的正常时间早8 个小时</p>
</blockquote>
</blockquote>
<h5 id="处理浏览器的询问" tabindex="-1"><a class="header-anchor" href="#处理浏览器的询问" aria-hidden="true">#</a> 处理浏览器的询问</h5>
<blockquote>
<blockquote>
<p>再次刷新浏览器，它发出了以下的HTTP请求：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213154346092.png" alt="image-20211213154346092"></p>
<p>意味着浏览器在询问Web服务器：“我请求的内容在这个时间之后是否有更新？”此时浏览器肩负起了重要责任，它需要检查这个内容在该时间后是否有过更新，并反馈给浏览器，这一过程便相当于我们传统意义上的缓存过期检查，对于静态内容来说，Web服务器可以轻松搞定，只要获得静态文件的最后修改时间并将其和浏览器询问的时间进行对比即可，但是对于动态内容，Web服务器可做不了主，这部分工作需要动态程序自己来完成，然后告诉Web服务器。我们暂且停下来，看看同样内容的一个静态文件如何响应这样的询问，我们在Web服务器上创建另一个静态网页，名为http_cache．htm，它的内容也是一串长度为10个字节的数字。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213154616221.png" alt="image-20211213154616221"></p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213154629842.png" alt="image-20211213154629842"></p>
<p>304 Not Modified意味着Web服务器告诉浏览器，这个内容没有更新，浏览器可以使用本地缓存的内容。同时，Web服务器没有将内容的正文传送给浏览器。</p>
</blockquote>
</blockquote>
<h5 id="另一种协商方法" tabindex="-1"><a class="header-anchor" href="#另一种协商方法" aria-hidden="true">#</a> 另一种协商方法</h5>
<blockquote>
<blockquote>
<p>HTTP/1．1还支持另一种缓存协商方法，那就是ETag，它与前面所讲的协商方式非常类似，但它没有采用内容的最后修改时间，而是采用了一串编码来标记内容，称为ETag。一个原则是，如果一个内容的ETag没有变化，那么这个内容也一定没有更新。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>ETag由Web服务器来生成，比如Apache为一个静态文件的HTTP响应头增加了以下标记：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213154731323.png" alt="image-20211213154731323"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>浏览器获得这个内容的ETag后，便会在下次请求该内容时，在HTTP请求头中附加以下标记来询问服务器该内容是否发生变化：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213154749011.png" alt="image-20211213154749011"></p>
<p>这时服务器就需要重新计算这个内容的ETag值，并与HTTP请求中的ETag进行比较，如果相同的话，便返回304状态码，如果不同的话，则将最新的内容返回给浏览器。</p>
<p>HTTP/1．1并没有规定ETag的具体格式和计算方法，也就是说，Web服务器可以自由定义ETag的格式和计算方法，比如一种简单的方法是对文件内容计算md5值作为ETag，总之只要能够起到标识内容的作用即可。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>使用基于最后修改时间的缓存协商存在一些缺点，比如，有时候一些文件需要频繁的更新，但是内容可能并没有变化，那么如果采用基于最后修改时间的缓存协商，那么每次文件的修改时间变化后，不论内容是否真的变化，浏览器都会重新获取全部内容；再比如，同一个文件存储在多台Web服务器上，用户的请求在这些服务器之间轮询，实现负载均衡，而这些服务器上同一个文件的最后修改时间很难保证完全相同，这便会导致用户的请求每次切换到新的服务器时就需要重新获取所有内容。这时候，如果使用直接标记内容的某种ETag算法，就可以避免这些问题。</p>
</blockquote>
</blockquote>
<h5 id="让动态内容学会和浏览器交流" tabindex="-1"><a class="header-anchor" href="#让动态内容学会和浏览器交流" aria-hidden="true">#</a> 让动态内容学会和浏览器交流</h5>
<blockquote>
<blockquote>
<p>Last-Modified：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213155537626.png" alt="image-20211213155537626"></p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213155719945.png" alt="image-20211213155719945"></p>
<p>1028．87 reqs/s的吞吐率在我们的意料之中，下面我们对刚才实现了浏览器缓存的版本进行测试，这时候我们必须通过ab的选项来增加HTTP请求中的If-Modified-Since属性，如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213155749362.png" alt="image-20211213155749362"></p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213155756242.png" alt="image-20211213155756242"></p>
<p>吞吐率增加到刚才的2倍以上，你可能觉得这主要来源于后者避免了HTTP响应正文的传输开销，可以看到测试结果中Document Length指示的正文长度为0 bytes，而前面测试结果指示的长度为13000 bytes，但是，不要忘记这里的动态缓存是需要PHP程序来加载的，这部分开销也随之被节省，而事实上它才是以上吞吐率提升的主要因素。</p>
<p>浏览器缓存只是针对它所属的用户有效，而服务器端的缓存是针对所有用户有效</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在使用浏览器缓存后，带宽使用大大降低，数据传输率只有329．98 KB/s，这些数据基本上只是响应HTTP头信息，换算成比特，也就是2．64M带宽，而之前的传输率为13781．59 KB/s，也就是需要110．25M的带宽，这个差距使得浏览器缓存的意义更加深远，这意味着你只需要购买2．64M的独享带宽，就可以提供2347．14 reqs/s的吞吐率，而前者需要购买110．25M的独享带宽，却只能提供1028．87 reqs/s的吞吐率，这就是浏览器缓存带来的最大价值。</p>
</blockquote>
</blockquote>
<h3 id="◆-6-3-彻底消灭请求" tabindex="-1"><a class="header-anchor" href="#◆-6-3-彻底消灭请求" aria-hidden="true">#</a> ◆ 6.3 彻底消灭请求</h3>
<blockquote>
<blockquote>
<p>HTTP缓存的目的就是要彻底消灭不必要的请求，也许你早就感到疑惑，之前的缓存协商为什么非要和Web服务器沟通呢？只要在浏览器缓存中标记出过期时间不就可以了吗？</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>HTTP中还有另一个标记，那就是Expires，它告诉浏览器该内容在何时过期，暗示浏览器在该内容过期之前不需要再询问服务器，而直接使用本地缓存即可。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>Expires标记更像善于放权的管理者，浏览器一旦看到某个内容附带Expires标记后，便拥有了极大的权力，它无须在过期之前每次都询问服务器，完全可以自作主张，而Last-Modified标记让浏览器感到拘束，它们不得不每次都询问服务器，即便它们认为这样做毫无意义。Expires的格式类似于Last-Modified，它指示了内容过期的绝对时间</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对于静态内容，Web服务器在默认情况下不会开启Expires标记的支持，我们需要进行一定的配置，比如在Apache中提供了mod_expires的支持：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213160141567.png" alt="image-20211213160141567"></p>
<p>而在Lighttpd中，一个简单的配置如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213160203374.png" alt="image-20211213160203374"></p>
<p>对于常见的静态文件格式，即便是Web服务器返回的HTTP响应头中没有Expires标记，浏览器也会根据一些其他的线索来猜测一个过期时间，比如IE在某种缓存模式下，对于GIF图片设置为永不过期，除非我们配置Expires为马上过期，也就是将过期时间设置为当前时间或者0。浏览器这样做的目的在于尽量避免向Web服务器发送请求，除非Web服务器明确指出不允许它这样做。</p>
<p>对于动态内容，Expires仍然需要程序自身来添加，类似于之前的Last-Modified，随后我们会在动态程序中进行尝试。</p>
</blockquote>
</blockquote>
<h5 id="如何请求页面" tabindex="-1"><a class="header-anchor" href="#如何请求页面" aria-hidden="true">#</a> 如何请求页面</h5>
<blockquote>
<blockquote>
<p>对于主流浏览器，一般有以下三种请求页面的方法：</p>
<p>Ctrl + F5：</p>
<p>这种方式可以叫强制刷新，它使得网页以及其中的所有组件都直接向Web服务器发送请求，并且不使用缓存协商，这样的目的是获取所有内容的最新版本。你也可以按住Ctrl键然后单击浏览器的刷新按钮获得同样的结果。在实际使用中，很少有用户会这样操作。</p>
<p>F5：</p>
<p>这种方式便是一般的刷新，我们经常使用，它等同于单击浏览器的刷新按钮。它允许浏览器在请求中附加必要的缓存协商，但不允许浏览器直接使用本地缓存，也就是说，它能够让Last-Modified发挥效果，但是对Expires无效。</p>
<p>单击浏览器地址栏的“转到”按钮或者通过超链接跳到此页：</p>
<p>我想这种方式大家使用最多，还有一种操作也等同于这种方式，那就是在浏览器地址栏中输入URL后按回车键，Firefox中常用这种方式，因为它没有“转到”按钮。这几种方式允许浏览器以最少的请求来获取网页的数据，浏览器会对所有没有过期的内容直接使用本地缓存，所以，Expires标记只对这种方式有效，以后你不用在按了F5键后对浏览器没有使用本地缓存感到奇怪。</p>
</blockquote>
</blockquote>
<h5 id="添加expires标记" tabindex="-1"><a class="header-anchor" href="#添加expires标记" aria-hidden="true">#</a> 添加Expires标记</h5>
<blockquote>
<blockquote>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213160621156.png" alt="image-20211213160621156"></p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213160630502.png" alt="image-20211213160630502"></p>
</blockquote>
</blockquote>
<h5 id="适应本地的过期时间" tabindex="-1"><a class="header-anchor" href="#适应本地的过期时间" aria-hidden="true">#</a> 适应本地的过期时间</h5>
<blockquote>
<blockquote>
<p>通过Expires指定的过期时间，是来自于Web服务器的系统时间，如果用户本地的时间和服务器时间不一致的话，那一定会影响到本地缓存的有效期检查。</p>
<p>比如服务器端为某个内容设置的过期时间为1个小时，可是假如浏览器的时间比服务器晚了2个小时，那么这个内容将被浏览器认为立即过期。当然，一般我们使用的操作系统（如Windows）都会使用基于GMT的标准时间，然后本地时间通过时区来进行偏移计算，而HTTP中使用的也是GMT时间，所以一般不会因为时区而导致本地和服务器相差数个小时。但是，没有人能保证用户本地的时间都是与你的服务器一致的，甚至有时候你的服务器时间也许就是错误的，这些都会影响到浏览器缓存的正常工作</p>
<p>HTTP/1．1中还有一个标记用于弥补Expires的不足，那就是Cache-Control，它的格式如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213160838616.png" alt="image-20211213160838616"></p>
<p>max-age指定了缓存过期的相对时间，单位是秒，并且这个时间是相对于浏览器本地时间而言。对于静态内容，事实上Web服务器在开启Expires的同时，也会自动添加响应的Cache-Control标记，用于兼容HTTP/1．1，我们来请求一个GIF图片，它位于Apache服务器上，我们为Apache设置了GIF的过期策略，如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213160914094.png" alt="image-20211213160914094"></p>
<p>接下来我们在浏览器上请求这个图片的URL，然后跟踪HTTP响应头，如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213160928175.png" alt="image-20211213160928175"></p>
<p>Expires时间刚好是Date时间之后的1个小时，同时，max-age的值为3600秒。</p>
<p>目前的主流浏览器都将HTTP/1．1作为首选，所以当HTTP响应头中同时含有Expires和Cache-Control时，浏览器会优先考虑Cache-Control。对于没有Cache-Control的情况，浏览器则会服从Expires的指示，之前我们的动态程序http_cache．php便只有Expires，它工作得很好。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213161016204.png" alt="image-20211213161016204"></p>
</blockquote>
</blockquote>
<h2 id="◆-第7章-web服务器缓存" tabindex="-1"><a class="header-anchor" href="#◆-第7章-web服务器缓存" aria-hidden="true">#</a> ◆ 第7章 Web服务器缓存</h2>
<p>动态内容缓存和静态化，基本上都是通过动态程序自身来实现缓存机制，包括缓存持久化、过期检查、缓存更新等。是否可以让Web服务器自己实现缓存机制呢？</p>
<h3 id="◆-7-1-url映射" tabindex="-1"><a class="header-anchor" href="#◆-7-1-url映射" aria-hidden="true">#</a> ◆ 7.1 URL映射</h3>
<h3 id="◆-7-2-缓存响应内容" tabindex="-1"><a class="header-anchor" href="#◆-7-2-缓存响应内容" aria-hidden="true">#</a> ◆ 7.2 缓存响应内容</h3>
<blockquote>
<blockquote>
<p>一个URL在一段较长的时间内对应一个唯一的响应内容，比如静态内容或者更新不太频繁的动态内容，一旦将最终内容缓存后，下次Web服务器便可以在收到请求后立即拿出事先缓存好的响应内容并返回给浏览器，因为这个操作发生在所有其他行为之前，所以必然会节省一定的时间。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>主流的Web服务器软件都会提供类似的支持，比如Apache的mod_cache，它在URL映射开始时检查缓存，如果缓存存在并处于有效期内，那么将直接取出作为响应内容返回给浏览器。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>将缓存内容存储在什么位置，一般来说，本机内存和磁盘是主要选择，当然，也可以采用分布式设计，存储到其他服务器的内存或磁盘中。</p>
<p>Apache也提供了两个扩展，分别是mod_disk_cache和mod_mem_cache，它们为mod_cache提供存储引擎，前者使用磁盘文件系统来存储缓存，后者使用内存。但实际上，这两个存储引擎一直都处于试验阶段，到目前为止，我看到Apache官方已经将mod_mem_cache从Apache最新文档的模块列表里清除掉了，根据Apache社区的一些讨论，可能是mod_mem_cache的实现机制导致它在Apache多进程模型下共享内存缓存的开销较大，官方推荐使用mod_disk_cache来取代mod_mem_cache，因为它在磁盘上维护了一块多个进程共享的缓存区，并且由于磁盘文件系统缓冲区以及MMAP的作用，磁盘缓存的访问速度甚至要超过mod_mem_cache实现的内存缓存。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>Lighttpd、Nginx等也都有类似机制的缓存支持，比如Lighttpd提供了mod_trigger_b4_dl模块，可以在响应数据输出之前将它缓存到memcached缓存服务器中，并且在随后请求的URL映射之前查找缓存，所以它们的本质都是相似的，这样才可以最大程度地发挥Web服务器缓存的意义。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在Apache中开启磁盘缓存很容易，但是你必须在Apache编译的时候，为configure追加必要的模块：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213161712392.png" alt="image-20211213161712392"></p>
<p>然后，为Apache增加以下的配置：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213161727772.png" alt="image-20211213161727772"></p>
</blockquote>
</blockquote>
<h3 id="◆-7-3-缓存文件描述符" tabindex="-1"><a class="header-anchor" href="#◆-7-3-缓存文件描述符" aria-hidden="true">#</a> ◆ 7.3 缓存文件描述符</h3>
<blockquote>
<blockquote>
<p>对于静态内容，特别是拥有大量小文件的站点，Web服务器相当大的一部分开销花在了打开文件上，即open()系统调用，所以我们还可以考虑将打开后的文件描述符直接缓存到Web服务器的内存中，当然，文件描述符是反映系统资源的数据结构，它也只能存在于本机内存中。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>Apache提供了相应的扩展mod_file_cache，它需要我们提供希望缓存文件描述符的文件列表，比如我们进行如下配置：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213162044943.png" alt="image-20211213162044943"></p>
<p>这样一来，Apache在启动的时候便会打开这些文件，并持续到Apache关闭为止，在这期间，如果有对这些文件的请求，Apache将通过文件描述符直接把文件传送出去。</p>
<p>缓存文件描述符的缓存方案只适用于静态内容，与前面将静态内容整体缓存到磁盘相比，这种方法可以减少打开文件的开销，而前者仍然需要从磁盘缓存区打开文件，但是，由于前者发生在URL映射的最前端，所以它完全弥补了由于打开文件的开销带来的性能损失，事实上前者的测试结果还要略高于缓存文件描述符时的吞吐率。</p>
<p>缓存文件描述符对于较大的文件来说也是不适合的，因为处理它们的主要时间花在传送数据上，而不是打开文件。另外，由于在Apache启动的时候就打开了大量的文件，并且需要在多个子进程中共享文件描述符，所以这必然带来了更多的内存开销，要说唯一节省的资源，那就是避免了磁盘缓存区的大量静态文件冗余，但是这算不了什么。还有，当这些文件更新后，必须重启Apache才能生效。从以上这些方面来看，似乎缓存文件描述符的意义不是不大，它的处境比较尴尬，但是至少通过我们的尝试，你已经了解了它的本质，也许在你的站点中会有它的一席之地。需要说明的是，不要完全被以上的压力测试结果所影响，因为我们测试的只是来自于Apache的实现，对于其他Web服务器或许有更加高效的实现，至少缓存文件描述符的思想是可取的。</p>
</blockquote>
</blockquote>
<h2 id="◆-第8章-反向代理缓存" tabindex="-1"><a class="header-anchor" href="#◆-第8章-反向代理缓存" aria-hidden="true">#</a> ◆ 第8章 反向代理缓存</h2>
<h3 id="◆-8-1-传统代理" tabindex="-1"><a class="header-anchor" href="#◆-8-1-传统代理" aria-hidden="true">#</a> ◆ 8.1 传统代理</h3>
<blockquote>
<blockquote>
<p>很久以前，我们通常需要通过代理服务器来访问互联网上的Web站点，代理服务器本身接入了互联网，而我们通过内部网络与代理服务器相连。即便是现在，有些时候为了访问一些由于某种原因无法直接访问的站点，我们也会通过特定的代理服务器，绕过某些限制来访问目标站点。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>这些代理服务器所做的工作都是一样的，那就是将来自用户的HTTP请求转发给最终的Web服务器，然后再将从Web服务器收到的响应数据返回给用户浏览器。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>现在，我们普遍已经不使用代理服务器了，当我们的PC处于内部网络时，网关会使用NAT（网络地址转换）技术，将PC的内部IP地址和网关的外网IP地址进行相互转换，使得PC发出的请求可以顺利到达外部网络的Web服务器，同时将返回的数据正确地传送给内部网络的PC。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>NAT在这里起到的作用等同于代理服务器，但是它们的不同在于，代理服务器工作在应用层，所以只有当它支持某个协议的时候才可以转发该协议的数据，而NAT工作在应用层以下，它可以透明地转发应用层协议的数据，比如HTTP、FTP、SMTP等</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>这样一来，我们的PC便不用直接暴露在互联网中，有效提高了安全性能，因为攻击者是无法主动找到我们的，代理服务器或者网关就像一道防火墙保护着我们</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>正是因为代理服务器工作在应用层，所以它可以很容易地提供基于缓存的加速功能，比如一个机构的内部网络通过代理服务器上网，一旦某个用户访问过的网页被缓存在代理服务器上，那么随后一段时间里，内部网络的其他用户便可以快速获得这个网页，而不需要再次经过外部网络请求Web服务器，随之还带来的另一个好处就是节省了带宽。</p>
</blockquote>
</blockquote>
<h3 id="◆-8-2-何为反向" tabindex="-1"><a class="header-anchor" href="#◆-8-2-何为反向" aria-hidden="true">#</a> ◆ 8.2 何为反向</h3>
<blockquote>
<blockquote>
<p>传统代理服务器的特点，即用户隐藏在代理服务器之后，那么，反向代理服务器的特点便与此刚好相反，那就是Web服务器隐藏在代理服务器之后。我们将这种代理机制称为反向代理（Reverse Proxy），同时，实现这种机制的服务器，便称为反向代理服务器</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>隐藏在反向代理服务器之后的Web服务器，我们习惯称它为后端服务器</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>反向代理服务器在这里便成了前端服务器</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>反向代理服务器暴露在互联网中，而后端的Web服务器通过内部网络与它相连，当然，你也可以将反向代理服务器和Web服务器运行在同一台物理服务器上。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>引入反向代理后，用户将通过反向代理服务器来间接访问Web服务器，不过用户并不关心这些，因为反向代理服务器可以完美地充当用户心目中的Web服务器，至于反向代理服务器和后端Web服务器的沟通，则仍然是基于HTTP，这一点和传统代理的本质是一样的。</p>
<p>将Web服务器隐藏在后端，同样也带来了一定的安全性，但这不是反向代理的主要目的，因为完全可以使用iptables来作为防火墙以达到同样的安全性目的。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>反向代理的主要目的是什么呢？有一点肯定没错，那就是基于缓存的加速</p>
</blockquote>
</blockquote>
<h3 id="◆-8-3-在反向代理上创建缓存" tabindex="-1"><a class="header-anchor" href="#◆-8-3-在反向代理上创建缓存" aria-hidden="true">#</a> ◆ 8.3 在反向代理上创建缓存</h3>
<blockquote>
<blockquote>
<p>如果没有缓存：</p>
<p>为了测试反向代理缓存带来的性能提升，我们需要一些对比数据，主要包括：● 不使用反向代理时的性能数据● 使用反向代理但不使用缓存时的性能数据</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>用Nginx作为反向代理服务器，它的配置非常简单，只需要设置proxy_pass指令即可，同时别忘了打开mod_proxy模块。我们将nginx运行在Apache所在的物理服务器上，nginx监听在8002端口，它作为Apache的反向代理，将用户的请求都转发到Apache监听的80端口</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213202851809.png" alt="image-20211213202851809"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在使用反向代理但不缓存内容的时候，吞吐率下降了10%左右，多出的处理时间不难想象，原本直达Web服务器的请求，现在需要“绕路”转达，所以时间必然会有所增加，幸好这部分时间和动态内容处理的时间相比微乎其微，且吞吐率下降只有10%左右。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>人们喜欢用这种方式将Web服务器和应用服务器分离，即前端的Web服务器处理一些静态内容，同时又作为反向代理，将动态内容的请求转发给后端的应用服务器来处理。比如Nginx或Lighttpd将PHP程序的请求转发给后端的Apache，在Nginx中配置非常简单：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213203214693.png" alt="image-20211213203214693"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>Nginx便包揽了静态文件的处理工作，对于静态内容，Nginx利用epoll模型可以在较大并发用户数的情况下依然提供较高的吞吐率，这是它所擅长的工作，而后端Apache则可以专注地处理动态内容。对于动态内容的请求比例较多的情况，这种方式也有明显的不足，通过前面的测试结果你不难分析出原因。也许随后的反向代理缓存可以很好地解决这个问题。</p>
</blockquote>
</blockquote>
<h5 id="引入缓存" tabindex="-1"><a class="header-anchor" href="#引入缓存" aria-hidden="true">#</a> 引入缓存</h5>
<blockquote>
<blockquote>
<p>反向代理只是Nginx的一个扩展模块，并且它的缓存机制到目前为止还在不断完善，所以我们暂且放弃它。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>Squid在这方面家喻户晓，不过，除了作为反向代理，它还热衷于很多其他的工作，比如传统代理、访问控制、身份验证、流量管理等，正因为这样，它呈现出过于重量级的身躯，而且它的配置也过于复杂</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>仅仅是基于反向代理缓存的加速功能，所以我们选择了Varnish，它更加专注于反向代理，而且对于后面要介绍的负载均衡也有很好的支持。</p>
<p>先将Varnish配置成为Apache的代理，修改Varnish的配置文件default．vcl如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213204213584.png" alt="image-20211213204213584"></p>
<p>然后我们启动varnishd，在启动时需要指定它自身的监听端口，以及配置文件路径和存储引擎，如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213204234841.png" alt="image-20211213204234841"></p>
<p>varnishd监听在8010端口，并且在8011端口提供命令行管理服务，同时，我们使用了磁盘文件作为缓存引擎，预分配了512MB的空间，随后我们可以看到缓存文件，Varnish在一个文件中采用特殊的存储结构来维护缓存，类似于MySQL的Innodb存储引擎。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213204452362.png" alt="image-20211213204452362"></p>
<p>内容是否可以被缓存，这不是通过HTTP来决定的吗？没错，反向代理服务器正是通过修改流经它的数据的HTTP头信息来达到这个目的，同时，它们还可以针对一些HTTP头信息进行必要的缓存策略配置，这就好比反向代理服务器是浏览器和Web服务器之间的协调员，浏览器和Web服务器通过HTTP将自己的需求告诉反向代理服务器，但是反向代理服务器可能有更好的想法，它在不伤害另外两方的情况下，可以进行必要的协调，达到更好的效果。</p>
<p>我们所关心的是前端和后端的实际吞吐率，因为它们决定了我们何时需要对站点规模进行扩展。结合表8-1中的示例，我们举几个例子：</p>
<p>● 对于示例11，假设我们的一台反向代理服务器可以处理最高6000reqs/s的吞吐率，那么为了实现处理10000reqs/s的需要，我们就要考虑使用两台反向代理服务器来扩展处理能力。另外，假设我们的一台后端Web服务器对动态内容的处理可以达到最高200reqs/s的吞吐率，而后端目前只需要达到27．8reqs/s的实际吞吐率即可，那么我们只需要使用一台后端服务器，将前端的两台反向代理服务器同时指向这台后端服务器。</p>
<p>● 对于示例10，前端同样需要达到10000reqs/s的吞吐率，我们仍然使用两台反向代理服务器，但此时后端只有一台Web服务器是不够的，为了实现10000reqs/s的需要，我们要使用多台后端服务器来扩展后端的处理能力，假设1台后端服务器可以处理最高200reqs/s的吞吐率，那么可能需要50台后端服务器，或许更多。</p>
<p>为此，我们可以将多台反向代理服务器指向同一台后端服务器，也可以将一台反向代理服务器指向多台后端服务器。</p>
</blockquote>
</blockquote>
<h5 id="esi" tabindex="-1"><a class="header-anchor" href="#esi" aria-hidden="true">#</a> ESI</h5>
<blockquote>
<blockquote>
<p>在实际情况中，一个网页中各部分内容的更新频率各不相同，例如展示一篇新闻的网页中，新闻内容长期不变，但是旁边的推荐新闻列表却需要定时更新，那么，反向代理能否只向后端服务器请求推荐新闻列表这部分更新后的内容，而不需要获得整个网页的内容呢？这样便可以在一定程度上减轻后端服务器的开销。</p>
<p>反向代理服务器可以做到这一点，前提是它必须实现ESI（Edge Side Includes），ESI是由W3C制定的标准，它的语法非常类似于SSI（ServerSide Includes），可以像SSI一样在网页中嵌入子页面，但不同的是，SSI是在Web服务器端组装内容，而ESI则是在HTTP代理服务器上组装内容，包括反向代理。</p>
<p>我们可以在HTML中添加ESI语句，比如一个简单的ESI语句如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213205939698.png" alt="image-20211213205939698"></p>
<p>Varnish便支持ESI，我们可以用它来实现网页局部缓存，接着刚才新闻网页的例子，我们做一个简单的原型，假如新闻网页为news．php，它的内容如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213210009918.png" alt="image-20211213210009918"></p>
<p>我们看到news．php里嵌入了一个子页面recommend．php，它是一个动态内容，由后端服务器生成。接下来，我们还需要在VCL中进行相应的配置，用来对不同的内容设置不同的缓存有效期，如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213210038473.png" alt="image-20211213210038473"></p>
<p>可以看出，我们为recommend．php指定了较短的有效期（1分钟），而对news．htm指定了24小时的有效期，这便达到了我们的目的。</p>
<p>在处理只有局部更新的动态内容时，这种方案的确是不错的选择，不过它过于依赖ESI，一旦你的反向代理服务器不支持ESI，那么一切都毫无意义。另一种更好的选择是利用AJAX（Asynchronous JavaScript andXML）将需要频繁更新的局部内容采用异步请求的方式，这种做法更加容易控制，你唯一需要注意的问题是AJAX的跨域问题，你的局部内容应该和父页面所在的主机保持相同的顶级域名。</p>
<p>对于原本大量使用SSI的内容，可以非常快速地迁移到ESI，因为它们的语法和需要的页面结构都非常相似，这样一来，内容的组装由Web服务器转移到反向代理服务器上，这本身没有绝对的好处，就像原本后端服务器是运输组装好的成品到反向代理服务器，而现在只运送原材料，由反向代理服务器来负责组装，总的工作量没有什么变化，但是在有多个后端服务器的情况下，便可以避免多个后端的重复组装，减少总工作量。</p>
<p><strong>如何检查动态内容是否如你所想的一样被反向代理缓存？</strong></p>
<p>一个简单的办法是前面提到的通过反向代理状态监控来分析缓存命中率，从而了解它是否达到你预期的区间，同时也可以观察后端服务器的实时吞吐率和访问日志等，另外，如果允许的话，也可以编写自动化测试用例，自动覆盖每一个动态内容，检查它们的HTTP响应头是否符合缓存规范。这一切的检查有时需要较长的周期，在发现问题之前，我们希望它的影响范围降到最低，所以，当我们没有十足的把握时，启用动态内容自身的页面缓存便引入了另一道防线，我喜欢称它为“备用缓存”，你也可以把它看成是反向代理服务器的L2Cache（二级缓存）。</p>
<p><strong>暴露后端：</strong></p>
<p>为什么会暴露后端的Web服务器呢？通常我们希望后端服务器可以隐藏在反向代理之后，但在有些时候，比如反向代理主要用于跨地域加速，这时候反向代理服务器和后端服务器可能位于不同的数据中心，它们通过基于DNS策略的负载均衡分别服务于不同地域的用户，这时候，后端服务器的页面缓存也必须同样正常的工作。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>首次加载</strong></p>
<p>任何时候都不要忘了反向代理在首次加载一个内容或者缓存过期的时候，仍然需要向后端服务器获取，总是有一个不幸的用户会执行缓存预热的工作，所以同时启用后端动态内容的页面缓存便显得尤为重要，尤其是采用静态化方式，这样便可以让用户即使在没有命中反向代理缓存的时候，也可以享受后端缓存的优待。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>多台反向代理服务器</strong></p>
</blockquote>
</blockquote>
<h3 id="◆-8-4-小心穿过代理" tabindex="-1"><a class="header-anchor" href="#◆-8-4-小心穿过代理" aria-hidden="true">#</a> ◆ 8.4 小心穿过代理</h3>
<blockquote>
<blockquote>
<p>反向代理服务器充当了用户和后端Web服务器的中介，它只是将用户的HTTP请求转发给后端服务器，使得后端服务器知道用户的意图。但是，用户的有些信息并不存在于HTTP请求中，所以它们对后端服务器是不可见的。比如用户的IP地址和发送请求的TCP端口，它们分别位于分层网络模型中的IP层和传输层，反向代理服务器可以轻易地获得它们，而后端服务器却无法直接获得。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对于一些动态程序，获取用户的IP可能至关重要，虽然这与性能本身无关，但这是由于性能优化可能产生的副作用，我们希望反向代理能够将用户IP地址也转发给后端服务器，它是怎么做到的呢？</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>我们看到Web服务器获得的客户端IP为10．0．1．11，也就是Varnish所在服务器的内部网络地址，而在下面多出了一个服务器变量，它便是Varnish在转发请求时添加的标记，代表了用户的IP地址，我们看到它和刚才直接访问Web服务器看到的是一样的。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>当然，不是所有的反向代理服务器都会这样，有些时候需要你提醒它如何来做，比如在Nginx做反向代理时，我们需要进行以下配置：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213211958999.png" alt="image-20211213211958999"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>这样一来，我们通过Nginx访问后端的动态程序便可以得到以下的服务器变量：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213212015991.png" alt="image-20211213212015991"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>总之，对由于穿过反向代理而引发的此类问题，我们都可以通过让反向代理请求后端服务器时携带附加的HTTP头信息来实现。同样的，如果后端服务器想要告知浏览器一些额外的信息，也可以通过在响应HTTP头信息中携带一定的自定义信息穿过反向代理。比如当同时存在多个后端服务器时，我们在动态程序中添加以下的HTTP头信息：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213212114166.png" alt="image-20211213212114166"></p>
<p>这样一来，我们通过反向代理访问该内容时，浏览器便会获得以下的HTTP响应头信息：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213212327587.png" alt="image-20211213212327587"></p>
</blockquote>
</blockquote>
<h3 id="◆-8-5-流量分配" tabindex="-1"><a class="header-anchor" href="#◆-8-5-流量分配" aria-hidden="true">#</a> ◆ 8.5 流量分配</h3>
<blockquote>
<blockquote>
<p>反向代理服务器担当了用户端和后端的枢纽，它需要同时和两端进行数据交换，这使得它流入和流出的数据量要大于仅使用Web服务器时的数据量。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>如果反向代理服务器和后端服务器通过外网交换机进行数据交换，那么这部分流量必然会消耗其用于对外服务的带宽</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>这时候，我们可以组建内部私有网络，如图8-6所示，目的是让反向代理服务器和后端服务器通过内网交换机来交换数据，同时，我们需要为反向代理服务器和后端服务器都配备双网卡，并设置私有IP地址，这些都很容易，市面上的标准服务器机型一般都至少配置两个网卡。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213212513976.png" alt="image-20211213212513976"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对于商用数据中心，一般是按照机位和带宽来收取费用，一旦你将服务器部署在数据中心，便可以将其接入外网交换机，获得一个固定IP地址。同时，你也可以自己动手组建内部网络，将自己的服务器群进行合理的规划。</p>
</blockquote>
</blockquote>
<h2 id="◆-第9章-web组件分离" tabindex="-1"><a class="header-anchor" href="#◆-第9章-web组件分离" aria-hidden="true">#</a> ◆ 第9章 Web组件分离</h2>
<h3 id="◆-9-2-因材施教" tabindex="-1"><a class="header-anchor" href="#◆-9-2-因材施教" aria-hidden="true">#</a> ◆ 9.2 因材施教</h3>
<blockquote>
<blockquote>
<p>所指的Web组件是指Web服务器提供的所有基于URL访问的资源，比如动态内容、图片、JavaScript脚本、CSS样式表等。</p>
<img src="\img\9.构建高性能Web站点\image-20211213212829247.png" alt="image-20211213212829247" style="zoom:80%;" />
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>每种Web组件都有着相似的特点，接下来得考虑给它们分别采取什么样的优化方法呢？这些方法可能包括以下条目中的一种或多种：</p>
<p>● 是否使用epoll模型</p>
<p>● 是否使用sendfile()系统调用</p>
<p>● 是否使用异步I/O</p>
<p>● 是否支持HTTP持久连接（HTTP Keep-alive）</p>
<p>● 是否需要opcode缓存</p>
<p>● 是否使用动态内容缓存以及有效期为多长</p>
<p>● 是否使用Web服务器缓存以及有效期为多长</p>
<p>● 是否使用浏览器缓存以及有效期为多长</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 是否使用反向代理缓存以及有效期为多长</p>
<p>● 是否使用负载均衡策略</p>
</blockquote>
</blockquote>
<h3 id="◆-9-3-拥有不同的域名" tabindex="-1"><a class="header-anchor" href="#◆-9-3-拥有不同的域名" aria-hidden="true">#</a> ◆ 9.3 拥有不同的域名</h3>
<blockquote>
<blockquote>
<p>为了给不同类型的Web组件采取有针对性的措施，我们将这些Web组件分别进行独立部署，它们可能位于不同的物理服务器，或者同一个物理服务器上的不同逻辑单元中，同时，我们将不同的域名指向不同的Web组件服务器。</p>
</blockquote>
</blockquote>
<h3 id="◆-9-4-浏览器并发数" tabindex="-1"><a class="header-anchor" href="#◆-9-4-浏览器并发数" aria-hidden="true">#</a> ◆ 9.4 浏览器并发数</h3>
<blockquote>
<blockquote>
<p>当我们用不同的域名对Web组件进行分离后，另一个好处随之而来，那就是提高了浏览器在下载Web组件时的并发数。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>当用浏览器打开一个网页的时候，浏览器首先下载网页本身，也就是HTML，然后分析这些HTML标记，同时逐步下载其中包含的一系列组件。然而，浏览器下载组件的过程受到最大并发数的限制，也就是浏览器同一时刻最多只可以下载一定数量的组件，不同的浏览器拥有不同的默认限制</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>浏览器会为每个域名维护不同的下载队列，每个队列的最大并发数限制均为表9-1中列出的数值，这些队列同时运行。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213213301126.png" alt="image-20211213213301126"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>表9-1 常见浏览器的最大并发数限制</p>
<p>浏览器的下载并发数将会增多，整体下载速度也随之提高。</p>
<p>当实施了组件分离后，你可以使用HttpWatch等浏览器监视程序来观察你的站点，你会发现不同域名下的组件下载过程互不影响，整体上提高了浏览器的下载速度。HttpWatch目前只支持IE和Firefox浏览器，如果你希望使用其他浏览器来测试组件分离带来的浏览器下载速度提升，可以试试一个在线工具，它的地址是http://site-perf．com。</p>
</blockquote>
</blockquote>
<h3 id="◆-9-5-发挥各自的潜力" tabindex="-1"><a class="header-anchor" href="#◆-9-5-发挥各自的潜力" aria-hidden="true">#</a> ◆ 9.5 发挥各自的潜力</h3>
<h5 id="动态内容" tabindex="-1"><a class="header-anchor" href="#动态内容" aria-hidden="true">#</a> 动态内容</h5>
<blockquote>
<blockquote>
<p>动态内容包括了一切需要动态脚本在运行时创建的内容，比如动态生成的HTML网页、动态生成的图片、动态生成的XML数据等。</p>
</blockquote>
</blockquote>
<h5 id="开启opcode缓存" tabindex="-1"><a class="header-anchor" href="#开启opcode缓存" aria-hidden="true">#</a> 开启opcode缓存</h5>
<blockquote>
<blockquote>
<p>在关于动态脚本加速的章节中，我们看到了APC和XCache等基于PHP的opcode缓存器，其他动态脚本语言同样也拥有自己的opcode缓存器，它们的应用都不需要修改任何脚本程序，可以完全透明地为动态内容进行加速。</p>
</blockquote>
</blockquote>
<h5 id="足够大的内存" tabindex="-1"><a class="header-anchor" href="#足够大的内存" aria-hidden="true">#</a> 足够大的内存</h5>
<blockquote>
<blockquote>
<p>动态内容的运行需要不少的内存开销，一旦物理内存不够用，内存管理器会使用swap，这导致磁盘和内存之间发生频繁的数据复制，这可不好玩，我们需要坚决避免。通过前面介绍过的vmstat等系统工具，以及后面要介绍的性能监控系统，我们可以很好地观察到物理内存和swap的使用情况，但是，在一台运行中的服务器上扩展内存可不是容易的事情，你需要做好提前的打算，比如为服务器配备4GB的物理内存。</p>
</blockquote>
</blockquote>
<h5 id="多进程" tabindex="-1"><a class="header-anchor" href="#多进程" aria-hidden="true">#</a> 多进程</h5>
<blockquote>
<blockquote>
<p>与动态内容自身的开销相比，多进程切换的开销微不足道，尽管多进程需要消耗大量的内存，但是它也带来了很多好处，这包括可以同时处理更多的动态内容、整体上减少I/O等待时间，以及获得进程的稳定性。这些进程可以是Web服务器进程或者fastcgi进程。</p>
</blockquote>
</blockquote>
<h5 id="与数据库保持高速连接" tabindex="-1"><a class="header-anchor" href="#与数据库保持高速连接" aria-hidden="true">#</a> 与数据库保持高速连接</h5>
<blockquote>
<blockquote>
<p>大多数的动态内容都需要涉及数据库访问，这相当于一种远程I/O操作，它同样存在等待时间，但是不同于本地I/O等待时间，它还包括网络I/O的等待时间。将动态内容服务器与数据库服务器保持高速连接，可以最大化地减少网络I/O等待时间。最理想的情况就是它们位于同一台物理服务器上，小规模站点完全可以这样做，比如用Wordpress搭建的一个blog，动态内容和数据库完全可以通过UNIXSocket来建立更加快速的数据交换。</p>
<p>对于稍具规模的站点，动态内容服务器只能通过TCP通信来与数据库服务器建立连接，通过内部专用的百兆交换机来为它们提供足够的带宽和顺畅的通信链路，达到高速的数据库访问。同时，你还需要做的是，进行必要的网络拓扑结构规划，考虑未来可能存在的内部网络扩展。总之，我们希望将动态内容服务器部署在离数据库最近的地方。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>动态内容服务器和数据库服务器部署在不同的数据中心，这可能是因为有些动态内容服务器需要进行地理位置就近部署，虽然我不建议这样做，但是如果一定要这样做的话，租用连接两地数据中心的DDN专线，也可以提供相对较快的响应速度，不过它的价格不菲。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>没钱怎么办？我们可以用其他廉价的解决办法，比如将动态内容的静态副本分发到离用户较近的数据中心，而不是将计算过程整体搬过去，这就好像一家食品生产公司只需要将生产好的食品运到各地的超市去销售，而不需要将生产工厂也搬到超市去，那样的话原料的运输成本巨大。</p>
</blockquote>
</blockquote>
<h5 id="可靠的数据中心" tabindex="-1"><a class="header-anchor" href="#可靠的数据中心" aria-hidden="true">#</a> 可靠的数据中心</h5>
<blockquote>
<blockquote>
<p>为了对不同的动态内容采取更加富有针对性的措施，我们还将功能不同的动态内容划分到不同的域名和服务器上，使得它们相对独立。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>比如我们已经将站点动态内容放在反向代理的后端，并且对一些动态内容使用了反向代理缓存，这时候我们推出了站点的API服务，我们不希望给API服务加上任何缓存，也不需要从反向代理服务器转发请求，所以我们将API服务部署在另一台服务器的80端口上，并且准备了新的域名</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>这些服务想必你都有体验，分离还带来了另外一些好处：● 域名更具有可读性，用户容易记忆。● 有利于各种服务的独立访问量统计。● 有利于各服务的独立扩展，这涉及后面要介绍的基于DNS的负载均衡。</p>
</blockquote>
</blockquote>
<h5 id="静态网页" tabindex="-1"><a class="header-anchor" href="#静态网页" aria-hidden="true">#</a> 静态网页</h5>
<blockquote>
<blockquote>
<p>静态网页也就是直接存储在服务器磁盘上的HTML文档，它和其他静态内容一样，都不需要动态脚本解释器的参与，所以节省了一定的CPU和内存开销，同时，它们需要全身心地进行I/O操作，所以是I/O密集型的应用。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>除了静态网页之外，静态内容还包括图片、样式表等，与动态内容不同的是，静态内容的吞吐率很大程度上取决于服务器的并发处理能力</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>对于静态内容都可以采取哪些措施。</strong></p>
<p>支持epoll：它可以使得Web服务器在大量并发用户数的情况下保持较稳定的吞吐率。</p>
<p>非阻塞I/O：避免不必要的I/O等待。</p>
<p>异步I/O：如果可能的话，使用真正意义上的异步I/O。</p>
<p>使用sendfile()系统调用：避免文件系统磁盘缓冲区到用户地址空间的数据复制。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>单进程：避免多进程切换的不必要开销。对于磁盘I/O密集型的静态内容处理，多进程并不能带来多大的意义。</p>
<p>使用高速磁盘：磁盘转速是磁盘数据吞吐率的一个重要因素，使用15krpm转速的磁盘将会获得比7200rpm转速的磁盘更高的性能表现。</p>
<p>使用RAID分区：使用RAID分区存储使得磁盘可以实现并行读写，大大提高了磁盘的整体吞吐率。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>购买足够的带宽：由于处理静态内容时Web服务器的主要时间都用来输出内容，所以相对于动态内容来说，需要消耗更多的带宽。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>其他的静态内容具备哪些特点呢？</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>站点中的图片是我们非常熟悉的Web组件，它的尺寸从几十字节到几百KB各不相同，但是有一个特点，那就是一个网页中往往会包含很多张图片，有时候甚至超过500张，这一点都不夸张，当我使用刚刚推出不久的WebQQ时，我所有好友的头像都被下载到浏览器端。这时候，HTTP持久连接（HTTP Keep-alive）便大有用武之地，试想一下，500张图片的HTTP请求，如果每次都需要从建立TCP连接到关闭TCP连接，那是多么的浪费，通过HTTP持久连接便可以大大提高图片服务器的吞吐率。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>一般而言，浏览器会非常积极地在对静态内容的请求中附加持久连接的声明，所以我们只需要设置Web服务器端支持持久连接即可。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>视频下载有其自身的特点，那就是下载速度只要超过视频码率（也称为位速，即媒体文件播放1秒所需要的尺寸大小）即可，这样视频便可以流畅地播放。</p>
<p>**样式表：**CSS样式表的更新并不是那么频繁，我们看到Google将日历服务中CSS样式表的有效期设置为1年，真是相当的漫长。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213215337625.png" alt="image-20211213215337625"></p>
<p>即便是在缓存有效期内更新了样式表，并且希望浏览器及时下载新的样式表，也是有办法的，举个例子，比如我们的站点有一个样式表文件，将Expires时间设置到一年以后，可是没过几天，我们需要更新这个样式表，很简单，我们只需要在引用样式表时，让样式表的地址发生变化即可，比如增加一些个性化的参数，如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213215408903.png" alt="image-20211213215408903"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>这样一来，浏览器就会认为这是一个本地缓存区没有的新Web组件，从而发出HTTP请求来下载这个样式表。</p>
<p>但是，如果连网页本身也在浏览器缓存有效期中的话，那么新的样式表引用代码也毫无意义，其实，不用担心新的样式表会在一年后才被用户更新，因为用户会单击浏览器的刷新按钮，你可以在站点中引导用户这样做，以获得更新后的内容。</p>
<p>**视频：**我想你一定有过在线观看视频的经历，也体会过等待视频下载的漫长时间，几年前我们会认为原因在于我们家庭宽带的带宽不够，可是现在，问题多数已经不在我们了，而在于视频服务器的出口带宽，大家正拥挤在那里。</p>
<p>我们来看看视频服务器能消耗多大的出口带宽，下面我们对一个40MB左右的视频文件进行压力测试，模拟1000个并发用户数同时下载这个视频，注意，这次我们仍然位于服务器本地进行测试，所以不存在带宽限制，测试结果如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213215653127.png" alt="image-20211213215653127"></p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213215715161.png" alt="image-20211213215715161"></p>
<p>对于视频下载的压力测试，我们并不关心吞吐率，因为视频文件的尺寸大小各不相同，吞吐率没有任何意义，而视频下载有其自身的特点，那就是下载速度只要超过视频码率（也称为位速，即媒体文件播放1秒所需要的尺寸大小）即可，这样视频便可以流畅地播放。</p>
<p>我们来计算下载速度，视频大小为40MB，用户平均请求等待时间（Time per request）为69秒，很容易算出，平均下载速度为580KB/s，这相当于可以提供4．6Mbps的码率，这意味着什么呢？我们知道一个较好音质的MP3 的码率为128Kbps，而一个普通FLV视频的码率为几十Kbps到几百Kbps，毫不客气地说，4．6Mbps的码流如果提供视频服务的话，基本上接近DVD效果，当然前提是用户的家庭带宽足够大。</p>
<p>刚刚的测试结果其实只是一个峰值，不要忘了我们的所有请求都指向了同一个文件，由于磁盘高速缓存的作用，减少了大量磁盘寻址和数据传输的时间，使得读取速度得到了最大程度地发挥，而在实际情况中，用户的请求通常指向不同的文件，虽然在一些特定的时间段会有大量请求指向个别热点文件，但总体来说，磁盘读操作的位置过于随机，这导致磁盘的寻址时间必不可少，读取速度大量下降，而要准确地预测它并不容易，影响它的因素非常多，通过观察运行中的一台下载服务器，我们看到在带宽充足的情况下，磁盘读取速度最高保持在70Mbps到80Mbps，这相当于占用600Mbps左右的带宽，假如我们希望提供100Kbps码率的视频下载，那么理论上可以支持6000人同时观看，一旦用户超过这个数目，你就得考虑扩展视频服务的规模。</p>
<p>另一方面，要支持6000人同时观看，这意味着Web服务器至少要能承受6000个并发连接同时下载视频，为Web服务器使用多进程非常重要，对于刚才提到的那台下载服务器，我们为Nginx配置了128个进程，这使得它可以同时处理更多的慢速连接。除此之外，你还可以充分使用诸如sendfile这样的I/O优化措施，当然，它几乎已经成为Linux下所有主流Web服务器进行大文件传输的默认开启选项。</p>
</blockquote>
</blockquote>
<h2 id="◆-第10章-分布式缓存" tabindex="-1"><a class="header-anchor" href="#◆-第10章-分布式缓存" aria-hidden="true">#</a> ◆ 第10章 分布式缓存</h2>
<p>在有些时候，使用页面缓存显得尤为笨重，这可能来自于以下几个原因：</p>
<p>● 一个网页中不同区域的内容，自身更新频率和呈现及时度要求各不相同，如果为了迁就频繁更新的区域，而使整个页面频繁重建缓存，则影响整体吞吐率。</p>
<p>● 即便是采用局部动态缓存，如果局部区域过多，则会使得页面结构过于复杂，而且整合各个局部页面也存在不小的开销。</p>
<p>● 有些计算是无法作为页面来缓存的，比如有些动态内容中需要获取用户的登录状态，并根据不同用户呈现不同的内容。</p>
<p>● 这些页面缓存都只是提高了读数据的速度，并没有提高写数据的速度。</p>
<h3 id="◆-10-1-数据库的前端缓存区" tabindex="-1"><a class="header-anchor" href="#◆-10-1-数据库的前端缓存区" aria-hidden="true">#</a> ◆ 10.1 数据库的前端缓存区</h3>
<blockquote>
<blockquote>
<p>还记得我们曾经介绍过的文件系统内核缓冲区（Buffer Area）吗？它位于物理内存的内核地址空间，除了使用O_DIRECT标记打开的文件之外，所有对磁盘文件的读写操作都要经过它，所以你也可以把它看成是磁盘的前端设备。这块内核缓冲区也称为页高速缓存（Page Cache），实际上它包括以下两部分：</p>
<p>● 读缓存区</p>
<p>● 写缓存区</p>
<p>读缓存区中保存着最近系统从磁盘上读取的数据，一旦下次需要读取这些数据的时候，内核将直接从这里获得，而不需要访问磁盘。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>写缓存区的目的主要在于减少磁盘的物理写操作，通常情况下向磁盘中写入数据并不着急，进程不需要因为写操作而等待，内核缓冲区可以将多次写操作的指令累积起来，通过一次物理磁头的移动来完成。当然，写缓存区导致数据真正写入磁盘会产生几秒的延迟，在实际写入磁盘之前，这些数据被称为脏页（Dirty Page）。</p>
<p>同样的，类似于页高速缓存，我们也可以在数据库和动态内容之间建立一层缓存区，它可以部署在独立的服务器上，用于加速数据库的读写操作，这个缓存区实际上是由动态内容来控制的。</p>
</blockquote>
</blockquote>
<h3 id="◆-10-2-使用memcached" tabindex="-1"><a class="header-anchor" href="#◆-10-2-使用memcached" aria-hidden="true">#</a> ◆ 10.2 使用memcached</h3>
<blockquote>
<blockquote>
<p>为了实现高速缓存，我们不会将缓存内容放置在磁盘上，否则将毫无意义。基于这个原则，memcached使用物理内存来作为缓存区，当我们启动memcached的时候，需要指定分配给缓存区的内存大小，比如我们分配了4GB的内存来作为缓存区：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213222055249.png" alt="image-20211213222055249"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>memcached使用key-value的方式来存储数据，这是一种单索引的结构化数据组织形式，我们将每个key以及对应的value合起来称为数据项，所有数据项之间彼此独立，每个数据项都以key作为唯一索引，你可以通过key来读取或者更新这个数据项。</p>
<p>在稍具规模的应用中，缓存的数据项可能会非常多，足够达到天文数字，为了在内存中为如此之多的数据项提供高速的查找，memcached使用了高效的基于key的hash算法来设计存储数据结构，并且使用了精心设计的内存分配器，它们使得数据项查询的时间复杂度达到O(1)，这意味着不论你存储多少数据项，查询任何数据项所花费的时间都不变。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>由于缓存区空间是有限的，一旦缓存区没有足够的空间存储新的数据项时，memcached便会想办法淘汰一些数据项来腾出空间，淘汰机制基于LRU（LeastRecently Used）算法，将最近不常访问的数据项淘汰掉。</p>
</blockquote>
</blockquote>
<h5 id="网络并发模型" tabindex="-1"><a class="header-anchor" href="#网络并发模型" aria-hidden="true">#</a> 网络并发模型：</h5>
<blockquote>
<blockquote>
<p>作为分布式缓存系统，memcached可以运行在独立的服务器上，动态内容通过TCP Socket来访问它，这样一来，memcached本身的网络并发处理能力便显得尤为重要。memcached使用libevent函数库来实现网络并发模型，其中包括我们前面详细介绍过的epoll，所以你可以在较大并发用户数的环境下仍然放心使用memcached。</p>
<p>来看看memcached中set操作的性能，我们用PHP编写了以下的代码：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213223107898.png" alt="image-20211213223107898"></p>
<p>我们做压力测试，从结果中我们知道memcached服务器平均每秒处理了大约2338个set请求，当然，处理过程中不仅仅包括set操作，还包括了建立连接和释放连接，它们的开销不可忽视。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>我们希望尽可能地了解set操作本身的性能，办法是有的，我们在一个连接中重复进行多次set操作就可以了。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213223221992.png" alt="image-20211213223221992"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>这次我们不看结果中的吞吐率，而是记录下总时间，为12．248秒。在这段时间内，memcached一共处理了50个请求，而每个请求包括10000次set操作，那么我们可以算出每秒执行的set次数为：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213223255004.png" alt="image-20211213223255004"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>这是极端前提下的结果，实际情况中根据TCP连接复用程度的不同，memcached的吞吐率会存在上下浮动</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>基于序列化（Serialize）机制，我们可以将更高层的抽象数据类型转化为二进制字符串，以便通过网络进入缓存服务器，同时，在读取这些数据的时候，二进制字符串又可以转换回原有的数据类型。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>当我们试图将一个类的对象（或类的实例）进行序列化时，对象的成员函数是不被序列化的，而被序列化存储的只是对象的数据成员</p>
</blockquote>
</blockquote>
<h3 id="◆-10-3-读操作缓存" tabindex="-1"><a class="header-anchor" href="#◆-10-3-读操作缓存" aria-hidden="true">#</a> ◆ 10.3 读操作缓存</h3>
<p>还记得磁盘缓存区中的两部分吗？没错，读缓存区和写缓存区。那么我们也按照这个思路，先将memcached作为数据库的读缓存区，来看一个例子。</p>
<h5 id="重复的身份验证" tabindex="-1"><a class="header-anchor" href="#重复的身份验证" aria-hidden="true">#</a> 重复的身份验证</h5>
<p>大多数站点都有自己的用户系统，这给我们带来了不少的麻烦，有些事情不可不做，那就是在每个用户请求页面的时候，都需要检查用户的登录状态。</p>
<p>很早以前，我喜欢将登录用户的ID直接写入浏览器cookies，并以此为荣，可是在随后的几年里，破坏者对cookies更是情有独钟，他们可以很容易地利用cookies的无知，篡改本地cookies来冒充其他用户，这让我感到时代在飞速前进，我们必须改变。</p>
<p>新的方式问世了，用户在登录站点的时候会获得一个ticket字符串，并将它写入浏览器cookies，随后每次请求新的页面时，都需要上报这个ticket，接受重复的身份检查。非常好，它工作得一切正常，破坏者由于不知道我们发给其他登录用户的ticket，所以无法冒充。补充一点，事实上，永远不要低估破坏者的本领，它们可以通过其他手段获得他们想要的一切东西。</p>
<p>现在，我们将身份检查部分的代码抽取出来，它的工作很简单，读取cookies中的ticket字符串，然后通过条件SQL语句查询数据库，寻找拥有这个ticket的用户。</p>
<h5 id="缓存用户登录状态" tabindex="-1"><a class="header-anchor" href="#缓存用户登录状态" aria-hidden="true">#</a> 缓存用户登录状态</h5>
<p>即便使用了索引，查询本身还是存在开销，这很大程度上在于数据库的I/O操作，这时候，轮到memcached出场了。我们接下来希望将用户状态缓存在memcached中，没错，对象序列化派上用场了，它帮你简化了工作量，我们来看看新的代码：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213224223289.png" alt="image-20211213224223289"></p>
<p>这样一来，用户在1个小时的缓存有效期内，便不需要访问数据库。有一点需要注意，当用户选择注销时，你必须主动清空memcached中该用户的登录状态缓存，否则会让用户感到匪夷所思。</p>
<h3 id="◆-10-4-写操作缓存" tabindex="-1"><a class="header-anchor" href="#◆-10-4-写操作缓存" aria-hidden="true">#</a> ◆ 10.4 写操作缓存</h3>
<blockquote>
<blockquote>
<p>对于一个数据库写操作频繁的站点来说，通过引入写缓存来减少写数据库的次数显得至关重要。我们知道，通常的数据写操作包括插入、更新、删除，这些操作又同时可能伴随着条件查找和索引的更新。</p>
</blockquote>
</blockquote>
<h5 id="直接更新" tabindex="-1"><a class="header-anchor" href="#直接更新" aria-hidden="true">#</a> 直接更新</h5>
<h5 id="线程安全和锁竞争" tabindex="-1"><a class="header-anchor" href="#线程安全和锁竞争" aria-hidden="true">#</a> 线程安全和锁竞争</h5>
<blockquote>
<blockquote>
<p>在此之前，我们先来看一种传统的分布式加运算，以下的代码只是例子中的一个片段，它先从缓存服务器上取回一个数值，然后在本地加1，接下来写回缓存服务器。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213225107098.png" alt="image-20211213225107098"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>看起来没有任何问题，但是，别忘了可能会有多个用户同时触发这样的计算，你一定能想象的到会有什么糟糕的后果，最后的累计访问量总是小于实际访问量。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>事实上，这并不涉及memcached本身线程安全的问题，而是以上这种加运算的方式不是线程安全的。如果要保证这种加运算可以正常无误地同时进行，那就要考虑一定的事务隔离机制，简单的办法是使用锁竞争，并且将锁保存在memcached上，存在竞争关系的动态内容可以争夺这个锁，一旦某个会话抢到锁，那么其他的会话必须等待。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>这里要说的不是如何实现这种分布式锁机制，而是并不鼓励这样做，因为锁竞争带来的等待时间是无法容忍的，这将使得引入memcached作为写缓存的唯一优势立刻烟消云散。</p>
</blockquote>
</blockquote>
<h5 id="原子加法" tabindex="-1"><a class="header-anchor" href="#原子加法" aria-hidden="true">#</a> 原子加法</h5>
<blockquote>
<blockquote>
<p>memcached提供了原子递增操作，事实上，也正是因为它，我们才考虑在访问量递增更新的应用中引入写缓存。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213225342977.png" alt="image-20211213225342977"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在新的代码中，完全改变了之前的“直接更新”方式，当需要增加一次访问量的时候，它做了以下工作：</p>
<p>1.为memcached缓存中的对应数据项加1，如果该数据项不存在，则创建该数据项，并且赋值为1，代表这个页面是第一次被访问；</p>
<p>2.如果memcached缓存中存在对应数据项，并且累加后的数值为1000，则将这个数据项置0，同时更新数据库，将数据库中的对应数值加1000。</p>
<p>如果你的数据库因为大量的写操作而繁忙不堪，那么仔细考虑一下，哪些写操作可以缓存在memcached中呢。</p>
</blockquote>
</blockquote>
<h3 id="◆-10-5-监控状态" tabindex="-1"><a class="header-anchor" href="#◆-10-5-监控状态" aria-hidden="true">#</a> ◆ 10.5 监控状态</h3>
<blockquote>
<blockquote>
<p>memcached提供了这样的协议，可以让你获得它的实时状态，我们通过PHP扩展可以十分容易地做到。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213230852152.png" alt="image-20211213230852152"></p>
<p>你可以从这些数据中获得很多的信息，比如uptime表示memcached持续运行的时间；cmd_get表示读取数据项的次数；cmd_set表示更新数据项的次数；get_hits表示缓存命中的次数；bytes_read表示读取的总字节数；bytes表示缓存区已使用空间的大小；limit_maxbytes表示缓存区空间的总大小。</p>
<p>对于这些丰富的状态信息，我们可以简单地从以下三个方面来看。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>空间使用率：持续关注缓存空间的使用率，可以让我们知道何时需要为缓存系统扩容，以避免由于缓存空间已满造成的数据被动淘汰，有些数据项在过期之前被LRU算法淘汰可能会造成一定的不良后果。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>缓存命中率：</p>
<p>I/O流量：我们需要关注memcached中数据项读写字节数的增长速度，这反映了它的工作量，我们可以从中得知memcached是空闲还是繁忙。</p>
<p>我们也希望在监控系统中集成对于memcached的监控，如图10-1及图10-2所示便是在cacti监控系统中对memcached的某些监控图片，我们会在本书的末尾介绍cacti监控系统。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213231124079.png" alt="image-20211213231124079"></p>
</blockquote>
</blockquote>
<h3 id="◆-10-6-缓存扩展" tabindex="-1"><a class="header-anchor" href="#◆-10-6-缓存扩展" aria-hidden="true">#</a> ◆ 10.6 缓存扩展</h3>
<blockquote>
<blockquote>
<p>当存在多台缓存服务器后，我们面临的问题是，如何将缓存数据均衡地分布在多台缓存服务器上。现假设我们拥有以下两台缓存服务器，它们的内部IP地址如下所示，它们都运行着memcached。</p>
<p>关键在于如何做到“均衡”呢？这个问题归结于如何对数据项进行分区。</p>
<p>相比于关系型数据库的分区设计，key-value型数据缓存的分区要容易得多，你可以根据数据项的商业逻辑进行分区设计，拿前面的例子来说，我们可以将用户登录状态的缓存部署在独立的一台缓存服务器上，而将访问量统计的缓存部署在另一台独立的服务器上。这样一来，这两台缓存服务器的用途如下：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213231358764.png" alt="image-20211213231358764"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>一个简单而有效的方法是“取余”运算，这就像打扑克时的发牌，让所有数据项按照一个顺序在不同的缓存服务器上轮询，这可以达到较好的相对平衡，想想发牌的动机也正是为了让大家彼此公平。这种方法是一种比较常用的基本散列算法，事实上在很多时候都用得到，而且你可以根据实际情况对它进行改造，达到更好的散列效果。</p>
<p>把key进行md5后，取32个字符串中的前五个字符串，然后转为10进制数字，然后除3取余数就是要放到哪个服务器上。</p>
<p>字符串转为10进制：echo  hexdec(&quot;adsff&quot;);</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211213231826378.png" alt="image-20211213231826378"></p>
<p>当调整缓存分区算法后，我们需要时间来等待缓存重建和预热，但这往往并不影响站点的正常运转，前提是你按照前面读缓存和写缓存的理念来进行设计。顺便一提的是，与此相比，数据库规模扩展引发分区（Shard）之间的数据迁移就要复杂得多</p>
</blockquote>
</blockquote>
<h2 id="◆-第11章-数据库性能优化" tabindex="-1"><a class="header-anchor" href="#◆-第11章-数据库性能优化" aria-hidden="true">#</a> ◆ 第11章 数据库性能优化</h2>
<blockquote>
<blockquote>
<p>帮助你尽量少访问数据库，为此我们使用了各种缓存方法，将动态内容不断地靠近用户，从Web服务器到反向代理服务器，甚至到用户本地浏览器中，事实上，这已经进入了用户本地内存，没有什么比这更容易让用户随手可得的了。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>大量的数据库访问依旧在所难免，一部分原因包括：</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 即便是有较长的缓存有效期，缓存命中率很理想，但是缓存的创建和过期后的重建是需要访问数据库的，而且我们曾经在介绍反向代理缓存时描述了一个数学模型，通过它我们知道，当存在大量动态内容的时候，后端数据库仍然会面临不小的工作量，这的确是一个长尾效应。</p>
<p>● 对于数据库的写操作，往往不是很容易引入缓存策略，尽管前面我们在例子中通过memcache实现了访问量累加计算的缓冲，但是在实际情况中，很多时候不像累加计算那么简单，而且有时我们还需要保证一定的事务级别。</p>
<p>● 也许你根本没有使用过前面介绍的任何缓存方法。</p>
</blockquote>
</blockquote>
<h3 id="◆-11-1-友好的状态报告" tabindex="-1"><a class="header-anchor" href="#◆-11-1-友好的状态报告" aria-hidden="true">#</a> ◆ 11.1 友好的状态报告</h3>
<blockquote>
<blockquote>
<p>在MySQL命令行中，你可以通过以下的指令来获取当前数据库的实时状态：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214081244506.png" alt="image-20211214081244506"></p>
<p>mysqlreport为我们带来了更好的体验，它是一个第三方的MySQL状态报告工具。事实上，mysqlreport是站在巨人肩膀上的，它所做的事情是把MySQL中show status和show innodb status的结果进行一系列的后期处理，将我们最关心的内容以可读性更好的方式呈现出来。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214081544167.png" alt="image-20211214081544167"></p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214081559140.png" alt="image-20211214081559140"></p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214081612154.png" alt="image-20211214081612154"></p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214081625913.png" alt="image-20211214081625913"></p>
</blockquote>
</blockquote>
<h3 id="◆-11-2-正确使用索引" tabindex="-1"><a class="header-anchor" href="#◆-11-2-正确使用索引" aria-hidden="true">#</a> ◆ 11.2 正确使用索引</h3>
<blockquote>
<blockquote>
<p>假如一本书没有目录，那么试想你要找到本书某章的内容将会多么困难，你必须花时间一页一页地去查找，如果不幸的话，你要找的内容可能在书的末尾。这种情况在数据库中称为全表扫描（Full Table Scan），而通过目录直接找到内容的方式，在数据库中称为索引扫描（Index Scan）。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在大多数情况下，索引扫描当然要比全表扫描获得更好的性能，但这并不是绝对的，如果要查找的记录占据了整个数据表的很大比例，那么使用索引扫描反而性能更差，这不难想象，对于一本书来说，如果你希望阅读80%以上的内容，那么与其每篇文章都通过目录查找，倒不如抛开目录，来一次顺序的浏览，难道不是吗？不要忘了查找目录也需要时间开销啊。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>数据库中的查询优化器会判断一次查询是否有必要使用现有的索引以及使用哪个索引。当然，有些时候优化器也会犯错误，我们需要为它指引道路，比如在组合索引存在的时候</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>数据表中的索引并不像书的目录那么简单，因为目录一般只对应到章节，所以目录项比较少，而数据表中的索引则需要对应到每行记录，这样一来，索引项的数量也将会非常多，我们同样面临着在索引列表中找到某一个索引项的效率问题。然而，索引本身的数据结构（MySQL使用BTree、Hash以及RTree）决定了它们拥有非常高效的查找算法，我们基本上不用担心这部分的开销。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>从explain的分析结果中，我们可以看到两个重要信息：● type为const，意味着这次查询通过索引直接找到一个匹配行，所以优化器认为它的时间复杂度为常量。● key为PRIMARY，意味着这次查询使用了主键索引。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>type的结果为ALL，它意味着这次查询进行全表扫描，这在我们的预料之中，因为name字段没有索引，从key为NULL也可以看出这一点</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>key为name，意味着这次查询使用了name字段的索引。另一方面，对于没有使用主键索引或者唯一索引的条件查询，查询结果可能会有多个匹配行，MySQL为这种情况定义的type为ref，它在联合查询中也很常见。</p>
</blockquote>
</blockquote>
<h5 id="使用组合索引" tabindex="-1"><a class="header-anchor" href="#使用组合索引" aria-hidden="true">#</a> 使用组合索引</h5>
<blockquote>
<blockquote>
<p>再说说书的目录，假如目录中只列出了每一章，而我们要找的是第6章中的第9节，怎么办？我们只能根据目录先翻到第6章所在的确切页码，然后，一页一页地寻找第9节，这的确有些不符合阅读习惯，也许会让读者的激情一落千丈。幸运的是，大多数书的目录中都会列出“章”和“节”，这便是“组合索引”的目的。</p>
<p>在实际的数据库访问中，大多数的查询都包含组合条件，比如：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214083308575.png" alt="image-20211214083308575"></p>
<p>即使字段a和字段b已经分别建立了索引，它们仍然不能同时发挥作用，因为一次查询对于一个数据表只能使用一个索引，它们是无法进行效用叠加的。这样一来，便会存在一定程度的局部行扫描（Range Scan），这在有些特定的场景中将严重影响查询性能，比如上述第一条查询，数据库会先利用字段a的索引快速匹配a=1的记录，然后在这些记录中筛选b=2的记录，而此时b字段的索引将爱莫能助，试想，如果a=1的匹配行非常多的话，查询时间将花在b字段的筛选操作上。，MySQL5.0之后是有索引合并这个概念的，所以第一个问题解决了，MySQL可以同时使用多个索引</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214083558495.png" alt="image-20211214083558495"></p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214083700706.png" alt="image-20211214083700706"></p>
<p>优化器这样做的确是很明智的，它减少了开发人员大脑的开销，没有多少人能记得住数据库中字段的顺序。</p>
<p>组合索引对于包含order by和group by的查询也发挥着重要的作用，它们同样也遵循最左前缀原则。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214083733452.png" alt="image-20211214083733452"></p>
<p>其中type为index，表示这个查询只需要在索引中扫描即可，这里的索引即normal_key。也就是说，查询语句中order by指定的排序规则正好是索引本身的顺序，所以可以直接拿来派上用场，不需要重新排序。值得一提的是，有些非顺序的索引类型（如Hash），对order by是无效的。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214085242659.png" alt="image-20211214085242659"></p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214085326588.png" alt="image-20211214085326588"></p>
<p>虽然它用到了normal_key索引，但只是对where子句起作用，而后面的order by则需要排序计算，Using filesort已经证明了这一点。</p>
<p>对于包含group by的查询，数据库一般需要先将记录分组后放置在新的临时表中，然后分别对它们进行函数计算，比如count()、sum()或max()等。当有恰当的索引存在时，group by有时也可以使用索引来取代创建临时表，这当然是我们所希望的。以下这个SQL语句便利用了normal_key索引，避免了创建临时表。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214085624359.png" alt="image-20211214085624359"></p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214085742930.png" alt="image-20211214085742930"></p>
<p>小心组合索引的副作用</p>
</blockquote>
</blockquote>
<h5 id="使用慢查询分析工具" tabindex="-1"><a class="header-anchor" href="#使用慢查询分析工具" aria-hidden="true">#</a> 使用慢查询分析工具</h5>
<p>通过Web应用中的日志模块来记录所有SQL查询的执行时间是个不错的注意，你完全可以将它封装在数据访问层的查询方法中。另一方面，MySQL也考虑到了我们的需要，它提供了慢查询日志，可以将执行时间超过预设值的所有查询记录到日志中，以供后期分析。在MySQL中开启慢查询日志非常简单，在my．cnf中增加以下配置选项：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214090512454.png" alt="image-20211214090512454"></p>
<p>这意味着MySQL会自动将执行时间超过1秒的查询记录在指定路径的mysql_slow．log文件中。除此之外，你还可以将所有没有使用索引的查询也记录下来，只需增加以下选项即可：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214090551050.png" alt="image-20211214090551050"></p>
<p>随着MySQL的运行，我们开始不断地收集慢查询日志，在mysqlreport的报告中，可以看到有关慢查询的统计：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214090653744.png" alt="image-20211214090653744"></p>
<p>数据库平均每5秒有1个超过1秒的查询，它的次数占所有常规查询的0．08%，看起来问题不是很严重。</p>
<p>我们想看看究竟是哪些查询比较慢，尽管MySQL提供了mysqldumpslow，但是我更推荐使用mysqlsla，它同样是一个第三方的开源工具，有着更加清晰的统计风格。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214090804809.png" alt="image-20211214090804809"></p>
<p>完整的分析报告中包括了10个查询，以上只列出了第1个查询（001），报告中已经非常清楚地记录了这个查询的时间统计，我们看到它的平均执行时间为5秒，真是不看不知道，一看吓一跳。同时，下面还显示出了查询表达式和查询实例，接下来要做的你一定很清楚了，那就是拿着查询实例去分析，大多数的慢查询都是因为索引使用不当，其他的原因包括查询语句过于复杂（比如联合查询）或者数据表记录数过多，通过反范式化设计和数据分区可以有效改善这一状况。</p>
<h5 id="索引缓存" tabindex="-1"><a class="header-anchor" href="#索引缓存" aria-hidden="true">#</a> 索引缓存</h5>
<blockquote>
<blockquote>
<p>数据表的索引也可以缓存起来，当然它是缓存在内存中，最理想的情况是所有的索引都可以直接在内存中查找，而不需要访问磁盘。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214091206236.png" alt="image-20211214091206236"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>Write hit和Read hit分别代表了写缓存的命中率和读缓存的命中率，这里都在98%以上，比较正常。</p>
<p>这里报告中的索引缓存指的是MyISAM类型表的索引，缓存的大小由MySQL的key_buffer配置选项来指定，那么为它分配多大的空间合适呢？为此，我们得来看看，MyISAM类型表的存储方式，任何一个MyISAM类型的表包括以下三个文件：frm存储了表的结构信息。MYD存储了表中的数据。MYI存储了表中的索引。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>MyISAM类型的表只缓存索引，不缓存数据，也就是说只缓存MYI文件，不缓存MYD文件。那么，如果计算出所有MYI文件的大小，自然就是当前索引缓存区需要的空间大小了，但是显然这不具备可操作性，而且这个数值在不断增长。总之，如果你在大量使用MyISAM类型的表，给key_buffer预设一个较大的空间，并监控索引缓存的空间使用率是非常重要的。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>由于存在索引写缓存机制，MyISAM对于索引的写操作必然存在延迟，这是为了减少磁盘访问，但也给MyISAM带来了一些隐患，如果数据库崩溃的时候有些索引没有来得及写入磁盘，将会丢失索引，从而导致数据表损坏，尽管MySQL提供了修复工具来尽量弥补，但是假如数据表的数量有5位数，修复起来可不那么轻松。在这一点上，Innodb采用预写日志方式（Write-Ahead Logging，WAL）来实现事务，也就是只有当事务日志写入磁盘后才更新数据和索引，这样即使数据库崩溃，也可以通过事务日志来恢复数据和索引。</p>
</blockquote>
</blockquote>
<h5 id="索引的代价" tabindex="-1"><a class="header-anchor" href="#索引的代价" aria-hidden="true">#</a> 索引的代价</h5>
<blockquote>
<blockquote>
<p>索引会占据更多的磁盘空间，很多时候索引甚至比数据本身还要大，比如刚才我们将拥有100万行记录的表转为MyISAM类型后，可以看到索引文件（MYI文件）的大小几乎是数据文件（MYD文件）的两倍。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214093059183.png" alt="image-20211214093059183"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>现如今，存储空间比计算时间要廉价得多，TB级别的高速磁盘任你选择，而且大多数情况下在磁盘空间写满之前，计算能力的瓶颈早已迫使数据库进行扩展，所以你几乎不用担心索引空间的增长，从这一点上看，牺牲空间换取时间是值得的。</p>
<p>当建立索引的字段发生更新时，会引发索引本身的更新，这将产生不小的计算开销，我们做了一个简单的测试，同样对于刚才的key_t表，在不使用组合索引和使用组合索引两种情况下测试插入数据的性能：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214093140824.png" alt="image-20211214093140824"></p>
<p>对于update、delete等查询，一旦涉及索引字段的变更，也会引发索引计算，导致更多的时间开销。是否使用索引取决于站点的应用和你的权衡，mysqlreport中有一部分数据统计可以给你提供参考：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214093233012.png" alt="image-20211214093233012"></p>
<p>我们了解了站点中各种类型查询数量的比例，比如上面告诉我们在所有DMS查询中，select占据了85．56%，而update和insert加起来一共不到15%，那还犹豫什么呢？快使用索引吧，牺牲更新时间换取读取时间是值得的。</p>
<p>索引需要我们花费一些额外的时间来维护，比如前面我们提到的由于MyISAM表的索引写缓存而可能导致的索引损坏，就需要我们及时地去手动修复，但是，在可以接受的范围内，牺牲维护时间换取运行时间是值得的。</p>
</blockquote>
</blockquote>
<h3 id="◆-11-3-锁定与等待" tabindex="-1"><a class="header-anchor" href="#◆-11-3-锁定与等待" aria-hidden="true">#</a> ◆ 11.3 锁定与等待</h3>
<blockquote>
<blockquote>
<p>锁机制是影响查询性能的另一个重要因素。当有多个用户并发访问数据库中某一资源的时候，为了保证并发访问的一致性，数据库必须通过锁机制来协调这些访问。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>我们可以认为查询的时间开销主要包括两部分，即查询本身的计算时间和查询开始前的等待时间，所以说索引影响的是前者，而锁机制影响的是后者。显然，我们的目标很明确，那就是减少等待时间。</p>
</blockquote>
</blockquote>
<h5 id="减少表锁定等待" tabindex="-1"><a class="header-anchor" href="#减少表锁定等待" aria-hidden="true">#</a> 减少表锁定等待</h5>
<blockquote>
<blockquote>
<p>MySQL为MyISAM类型表提供了表级别的锁定，我们来看mysqlreport中关于表锁定的统计：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214101340654.png" alt="image-20211214101340654"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>MyISAM的表锁定可以允许多个线程同时读取数据，比如select查询，它们之间是不需要锁等待的。但是对于更新操作（如update操作），它会排斥对当前表的所有其他查询，包括select查询。除此之外，更新操作有着默认的高优先级，这意味着当表锁释放后，更新操作将先获得锁定，然后才轮到读取操作。也就是说，如果有很多update操作排着长队，那么对当前表的select查询必须等到所有的更新都完成之后才能开始。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214101718528.png" alt="image-20211214101718528"></p>
<p>它用时0．95 秒，的确是够慢的，但是跟前面慢查询日志中消耗了5 秒查询相比，这不算什么，所以，永远不要怀疑在运行环境中会存在超乎你想象的慢速查询。</p>
<p>接下来，我们编写PHP程序用来执行这个update操作，然后用ab模拟10个并发用户来请求这个PHP程序，这使得刚才的update操作语句会被执行10次，目的是让这个数据表忙于长时间的update操作。</p>
<p>在启动ab后不久，我们马上在MySQL命令行中发起一个select查询，如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214101759758.png" alt="image-20211214101759758"></p>
<p>这个查询并没有返回结果，而像是陷入了深思。这时我们在另一个MySQL命令行会话中监视所有线程的状态，因为我们想知道到底发生了什么。show processlist命令的结果如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214101821583.png" alt="image-20211214101821583"></p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214101833001.png" alt="image-20211214101833001"></p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214101844692.png" alt="image-20211214101844692"></p>
<p>我们看到除了show processlist本身的线程之外，还有5个update线程和1个select线程，其中只有1个update线程的状态为Updating，而其他的4个update线程和1个select线程都处于Locked状态，这意味着它们正在等待表锁的释放。从这里也可以看出，已经有5个update操作执行完毕，正在执行第6个update操作，而select查询将会等到所有的update执行完毕后才会进行。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>表级锁定如果大部分查询为读取操作，同时混合一小部分快速的更新操作，那么将不会存在太多的锁等待。</p>
</blockquote>
</blockquote>
<h5 id="行锁定带来了什么" tabindex="-1"><a class="header-anchor" href="#行锁定带来了什么" aria-hidden="true">#</a> 行锁定带来了什么</h5>
<blockquote>
<blockquote>
<p>如果你的站点主要依靠用户创造内容，那么频繁的数据更新在所难免，它将会给select等读取操作带来很大的影响，前面我们已经见证了7．6秒的select查询。这时候你希望使用行锁来解决问题，没错，MySQL在Innodb类型表中提供了行锁的支持，我们将刚才的count_t表转为Innodb类型，看看有何不同。</p>
<p>我们同样用ab模拟10个并发用户来执行update操作，同时我们监视MySQL线程，如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214102148937.png" alt="image-20211214102148937"></p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214102201277.png" alt="image-20211214102201277"></p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214102210723.png" alt="image-20211214102210723"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>可以看到还有4个update线程在执行，它们的状态都为Updating，这意味着已经有6个update操作执行完成。关键时刻到了，这时候我们在MySQL命令行中执行一个select查询，如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214102234295.png" alt="image-20211214102234295"></p>
<p>查询瞬间返回结果，用时0．00秒，这可不是什么魔术，即便还有4个update操作在执行，select查询还是迅速地获得结果。没错，刚才的update和select来自不同的线程，并且针对不同的记录行，所以它们可以轻松地并发进行，这就是行锁定真正带给我们的惊喜！</p>
<p>我们再来模拟一个更加接近实际情况的场景，为此，我们用PHP编写了以下程序：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214102511470.png" alt="image-20211214102511470"></p>
<p>不难理解，这段程序做了两件事情，首先它执行一次慢速的update操作，然后执行10000次快速的select查询。我们用ab模拟20个并发用户，每个用户请求一次程序，通过使用MyISAM和Innodb两种不同类型的锁定机制，我们来分别记录总执行时间。</p>
<p>首先是MyISAM类型表，测试结果的关键部分如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214102544083.png" alt="image-20211214102544083"></p>
<p>接下来是Innodb类型表，结果如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214102558904.png" alt="image-20211214102558904"></p>
<p>可以看到，使用了Innodb类型表的行锁机制后，时间缩短了接近20%，不难分析，这部分缩短的时间，正是由于在update操作的同时进行select查询所弥补的。另一方面，在Innodb类型表的测试中，select查询会得到很快的响应，而在MyISAM类型表的测试中，所有的select查询可能都要等到20秒以后才能开始执行，这是无法容忍的。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>行锁定的好处不是绝对的，很多人认为行锁定就比表锁定要快，这么说是缺乏上下文的，其实，理论上就锁定本身而言，行锁定的开销并不比表锁定小。通过刚才的例子我们知道，在select和update混合的情况下，行锁定可以很巧妙地解决读和写互斥的问题，在这种场景下使用行锁定是非常适合的。可见，行锁定的价值并不在于它本身的绝对速度，而是存在于特定的应用场景中。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>我们需要清楚的是，锁定只是一种逻辑层面的约束，即便是同时拥有Updating的状态，也不能加速这些update操作的总时间，因为磁盘的物理写操作最终还是依次进行的。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>另一种情况是涉及全表扫描的查询，比如以下这个select查询，where条件中的count字段并没有索引，所以这里只能进行全表扫描。对于Innodb的行锁定，扫描需要更多额外的开销</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214102825794.png" alt="image-20211214102825794"></p>
<p>可见，它从第一行记录开始扫描，经历了77777 行记录，终于找到了目标，一共花费了0．06秒。对于MyISAM的表锁定，我们也执行同样的查询，结果如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214102903824.png" alt="image-20211214102903824"></p>
<p>同样是经历了77777行记录找到目标，但是花费的时间是0．03秒，只有刚才的一半。</p>
</blockquote>
</blockquote>
<h3 id="◆-11-4-事务性表的性能" tabindex="-1"><a class="header-anchor" href="#◆-11-4-事务性表的性能" aria-hidden="true">#</a> ◆ 11.4 事务性表的性能</h3>
<blockquote>
<blockquote>
<p>MySQL中的Innodb除了支持行锁定外，还支持事务</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>Innodb的其他特性，比如行锁定、外键以及易于修复</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对于一个事务性表，我们需要注意哪些呢？前面我们提到过预写日志方式（WAL），这也是Innodb实现事务的方法。当有事务提交时，Innodb首先将它写到内存中的事务日志缓冲区，随后当事务日志写入磁盘时，Innodb才更新实际数据和索引。这里有一个关键点，那就是事务日志何时写入磁盘。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>为此，MySQL提供了一个配置选项，它有三个可选的值：</p>
<p>innodb_flush_log_at_trx_commit = 1表示事务提交时立即将事务日志写入磁盘，同时数据和索引也立即更新。这符合事务的持久性原则。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>innodb_flush_log_at_trx_commit = 0表示事务提交时不立即将事务日志写入磁盘，而是每隔1 秒写入磁盘文件一次，并且刷新到磁盘，同时更新数据和索引。这样一来，如果mysqld崩溃，那么在内存中事务日志缓冲区最近1秒的数据将会丢失，这些更新将永远无法恢复。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>innodb_flush_log_at_trx_commit = 2表示事务提交时立即写入磁盘文件，但是不立即刷新到磁盘，而是每隔1 秒刷新到磁盘一次，同时更新数据和索引。在这种情况下，即使mysqld崩溃后，位于内核缓冲区的事务日志仍然不会丢失，只有当操作系统崩溃的时候才会丢失最后1 秒的数据。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>“写入磁盘文件”和“刷新到磁盘”，我们在介绍磁盘I/O基础的时候曾经有过介绍，它们的区别在于前者只是将数据写入位于物理内存中的内核缓冲区，而后者是将内核缓冲区中的数据真正写入磁盘。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>将innodb_flush_log_at_trx_commit设置为0可以获得最佳性能，同时它的数据丢失可能性也最大</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>如果你希望尽量避免数据丢失，可以将其设置为2，它的性能相对于设置为0时略有下降，但是会带来更好的数据持久性保障，值得考虑。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>另一个重要的配置选项是Innodb数据和索引的内存缓冲池大小，MySQL提供了innodb_buffer_pool_size选项来设置这个数值，如果你在MySQL中大量使用Innodb类型表，则可以将缓冲池大小设置为物理内存的80%，并持续关注它的使用率。</p>
</blockquote>
</blockquote>
<h3 id="◆-11-5-使用查询缓存" tabindex="-1"><a class="header-anchor" href="#◆-11-5-使用查询缓存" aria-hidden="true">#</a> ◆ 11.5 使用查询缓存</h3>
<blockquote>
<blockquote>
<p>查询缓存的目的很简单，将select查询的结果缓存在内存中，以供下次直接获取。在默认情况下，MySQL是没有开启查询缓存的</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对于以select查询为主的应用，查询缓存理所当然地起到性能提升的作用，不论是Innodb还是MyISAM，查询缓存都可以很好地工作，因为它在逻辑中位于比较高的层次。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>查询缓存有一个需要注意的问题，那就是缓存过期策略，MySQL采用的机制是，当一个数据表有更新操作（比如update或者insert）后，那么涉及这个表的所有查询缓存都会失效。这的确令人比较沮丧，但是MySQL这样做是不希望引入新的开销而自找麻烦，所以“宁可错杀一千，不可放过一个”。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对于select和update混合的应用来说，查询缓存反而可能会添乱</p>
</blockquote>
</blockquote>
<h3 id="◆-11-6-临时表" tabindex="-1"><a class="header-anchor" href="#◆-11-6-临时表" aria-hidden="true">#</a> ◆ 11.6 临时表</h3>
<blockquote>
<blockquote>
<p>我们曾经看到一些查询在分析时出现Using temporary的状态，这意味着查询过程中需要创建临时表来存储中间数据，我们需要通过合理的索引来避免它。另一方面，当临时表在所难免时，我们也要尽量减少临时表本身的开销</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>MySQL可以将临时表创建在磁盘（Disk table）、内存（Table）以及临时文件（File）中，显然，在磁盘上创建临时表的开销最大，所以我们希望MySQL尽量不要在磁盘上创建临时表。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>如果你在show processlist中看到某些查询的状态为Copying to tmp table ondisk，这也意味着MySQL将临时表从内存转移到磁盘中，以节省内存空间。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在MySQL的配置中，我们可以通过tmp_table_size选项来设置用于存储临时表的内存空间大小，一旦这个空间不够用，MySQL将会启用磁盘来保存临时表，你可以根据mysqlreport的统计尽量给临时表设置较大的内存空间。</p>
</blockquote>
</blockquote>
<h3 id="◆-11-7-线程池" tabindex="-1"><a class="header-anchor" href="#◆-11-7-线程池" aria-hidden="true">#</a> ◆ 11.7 线程池</h3>
<blockquote>
<blockquote>
<p>MySQL采用多线程来处理并发的连接，通过mysqlreport中的Threads部分，我们可以看到线程创建的统计结果：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214104619807.png" alt="image-20211214104619807"></p>
<p>每秒创建43．6 个线程，虽然创建线程的开销不值一提，但是当系统比较繁忙的时候，我们当然不希望再给它添麻烦。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在应用中尽量使用持久连接，这将在一定程度上减少线程的重复创建。另一方面，从上面的Cached=0可以看出，这些线程并没有被复用，我们可以在my．cnf中设置以下选项：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214104655320.png" alt="image-20211214104655320"></p>
<p>这使得MySQL可以缓存100个线程。随后我们获得新的mysqlreport报告，Threads部分如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214104711184.png" alt="image-20211214104711184"></p>
<p>可以看到，MySQL在长达5天多的时间里，平均每秒处理45．1个连接，但是一共只创建了186个线程，线程池的命中率为100%。</p>
</blockquote>
</blockquote>
<h3 id="◆-11-9-放弃关系型数据库" tabindex="-1"><a class="header-anchor" href="#◆-11-9-放弃关系型数据库" aria-hidden="true">#</a> ◆ 11.9 放弃关系型数据库</h3>
<blockquote>
<blockquote>
<p>key-value数据库可以实现相对于关系型数据库更加出色的并发性能。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>性能是我们这里最为关注的方面，在这点上，MemcacheDB有着非常不错的表现，它是一个分布式key-value数据库，基于Memcache传输协议，这意味着所有使用Memcache的Web应用不需要进行任何修改，就可以直接迁移到MemcacheDB。同时，MemcacheDB将Berkeley DB作为持久化引擎，其出色的性能表现也正是得益于此。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>Berkeley DB本身是一个高性能的开源数据库引擎，它不能通过网络直接访问，而是需要作为函数库链接到应用程序。Berkeley DB同样支持事务，同时它通过预写日志方式（WAL）大大提高了写性能，这类似于前面提到的Innodb中配置innodb_flush_log_at_trx_commit=0的情况。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>MySQL曾经也采用Berkeley DB作为表引擎，但后来因为Berkeley DB被Oracle收购，并且改变了其版权许可协议性质，所以MySQL后来采用了Innodb取而代之。有意思的是，MySQL后来被Sun收购，而就在最近，Sun又被Oracle收购，时至今日，MySQL和Berkeley DB成了同门师兄弟，但今非昔比，MySQL已经将精力投入到了非常成熟的Innodb引擎。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>MemcacheDB同时很好地封装了Berkeley DB的复制功能，你可以通过主从复制来扩展MemcacheDB的规模，并且提升可用性。</p>
</blockquote>
</blockquote>
<h2 id="◆-第12章-web负载均衡" tabindex="-1"><a class="header-anchor" href="#◆-第12章-web负载均衡" aria-hidden="true">#</a> ◆ 第12章 Web负载均衡</h2>
<h3 id="◆-12-1-一些思考" tabindex="-1"><a class="header-anchor" href="#◆-12-1-一些思考" aria-hidden="true">#</a> ◆ 12.1 一些思考</h3>
<blockquote>
<blockquote>
<p>对于Web站点的水平扩展，负载均衡是一种常见的手段</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>这位外包接口人非常关键，他的缺席将会影响整个外包工作的正常进行，这也许是致命的，我们将这种情况称为单点故障（Single Point of Failure），如果公司只依赖一个因素、系统、设备或人，就会暴露出单点故障的隐患，所以，我们应该尽量避免单点故障。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>为此，公司为这位外包接口人配备了一名助理，全力协助他的工作，并且当他偶尔不在公司的时候，助理可以很好地充当他的角色继续工作。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>外包接口人需要跟多个外包公司进行沟通，并且将工作任务持续不断地分配给他们，那么，在任务分配过程中，可能发生这样一些情况：</p>
<p>● 给有些外包公司分配了太多的任务，其中一些任务没能按时完成；</p>
<p>● 有些外包公司比较闲，可是却没能及时给他们分配任务；</p>
<p>● 有些外包公司业务能力差，却给他们分配了高难度的任务，花费了大量的时间，最后还可能无法完成；</p>
<p>● 有些外包公司业务能力强，却给他们分配了非常简单的任务，支出了不必要的高额外包费用。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>从避免单点故障到准备备用方案，都是降低外包工作风险的一系列措施，同时也保障了外包工作的不间断运转，或者称为高可用性（High Availability，HA）。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>当更多的工作任务需要外包时，这位接口人的工作有点吃不消了，因为：● 一个接口人负责大量的任务，与多家外包公司分别进行沟通，这几乎花费了他全部的工作时间；● 一个接口人管理多家外包公司，已经超出了他的管理能力。显然，这些原因也是制约接口人处理更多外包工作的因素，这些因素限制了外包工作的无限扩展。这时，公司决定设立更多的外包接口人，成立一个新的团队，其中每位外包接口人分别负责管理一部分外包公司，并跟进相关的外包工作</p>
</blockquote>
</blockquote>
<h3 id="◆-12-2-http重定向" tabindex="-1"><a class="header-anchor" href="#◆-12-2-http重定向" aria-hidden="true">#</a> ◆ 12.2 HTTP重定向</h3>
<blockquote>
<blockquote>
<p>正是因为HTTP重定向具备了请求转移和自动跳转的本领，所以除了满足应用程序需要的各种自动跳转之外，它还可以用于实现负载均衡，以达到Web扩展的目的。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>你也许有过从php．net下载PHP源代码的经历，那么你是否注意过它是如何实现镜像下载的呢？没错，那就是HTTP重定向，而镜像下载的目的便是实现负载均衡，值得一提的是，这里我们暂且认为所有镜像服务器上的内容都是一致的。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>通过不同的地域来源来转移请求只是负载均衡的一种策略，在网络I/O成为主要瓶颈（比如下载服务）的时候，这种策略的优势表现得尤为突出。但是在其他一些时候，通过地域来源来划分请求并不那么合理，比如来自各个地域的请求到达Web服务器的响应时间差异不大，并且各地域的请求比例存在较大差异时，这种策略显然不太适合，它会造成各镜像服务器的负载分配不均衡，从而造成资源的浪费。</p>
<p>大多数情况下通过重定向来实现整个站点的负载均衡，并不那么让人满意，随后我们会探讨其他的扩展方法来实现站点负载均衡。</p>
<p>相比之下，对于文件下载、广告展示等一次性的请求，主站点调度程序可以牢牢地把握控制权，实际服务器的URL甚至可以含蓄地隐藏起来，与此同时，这种一次性的请求，也比较容易让多台实际服务器保持均衡的负载，但是也必须考虑一些现实的问题，比如分配给不同实际服务器下载的文件可能尺寸差异较大，我们需要在次数分配均衡的情况尽量保证文件尺寸分配均衡，也就是带宽使用分配均衡，这也许需要借助于应用程序，你可以记录下给每个实际服务器派发的下载文件的尺寸，从而在每次下载转移前挑选你认为比较轻松的实际服务器，但这样做存在风险，你可能是在毫无根据地指手画脚，因为某个用户可能请求下载一个1GB的文件，却在下载了1%后突然终止，而你却毫不知情，仍然以为某台实际服务器在卖力工作，随后一段时间你对这台服务器实施保护，而它却无所事事。</p>
<p>为了使提供下载服务的多台实际服务器比较均衡地使用带宽，另一个方法值得考虑，事实上它也是负载均衡系统中比较重要的一部分，那就是负载反馈。在这里，我们可以让主站点的定时任务不断获取每个实际服务器的实时流量，这可以通过SNMP获得原始数据并计算得出，这些数据将作为下载转移的权重参考。也许你觉得在请求转移逻辑中加入各实际服务器流量权重分析会带来额外的开销，但是，相比于下载的时间开销而言，这些额外的开销实在是九牛一毛。</p>
</blockquote>
</blockquote>
<h3 id="◆-12-3-dns负载均衡" tabindex="-1"><a class="header-anchor" href="#◆-12-3-dns负载均衡" aria-hidden="true">#</a> ◆ 12.3 DNS负载均衡</h3>
<blockquote>
<blockquote>
<p>DNS负责提供域名解析服务，当我们访问某个站点时，实际上首先需要通过该站点域名的DNS服务器来获取域名指向的IP地址，在这一过程中，DNS服务器完成了域名到IP地址的映射，同样，这种映射也可以是一对多的，这时候，DNS服务器便充当了负载均衡调度器（也称均衡器），它就像前面提到的重定向转移策略一样，将用户的请求分散到多台服务器上，但是它的实现机制完全不同</p>
</blockquote>
</blockquote>
<h5 id="多个a记录" tabindex="-1"><a class="header-anchor" href="#多个a记录" aria-hidden="true">#</a> 多个A记录</h5>
<blockquote>
<blockquote>
<p>在DNS的各种记录类型中，A记录你一定不陌生，它负责实现DNS的基本功能，用来指定域名对应的IP地址，常见的比较成熟的DNS系统如Linux的bind，以及Windows的DNS服务等，都支持为一个域名指定多个IP地址，并且可以选择使用各种调度策略，常见的便是RR方式。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>我们使用dig命令来查看www.baidu.com的A记录设置.</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214110608429.png" alt="image-20211214110608429"></p>
<p>这里我们首先看粗体部分，可以看到拥有三个A记录，分别指向三个不同的IP地址，这意味着什么呢？接下来我们通过ping命令来测试www.baidu.com域名的A记录解析，连续三次的测试结果如下所示：随机的两个服务器</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214110818317.png" alt="image-20211214110818317"></p>
<p>DNS服务器使用三个不同的IP地址来轮流解析baidu.com域名，实现了RR调度策略。</p>
<p>具体详细再看</p>
</blockquote>
</blockquote>
<h3 id="◆-12-4-反向代理负载均衡" tabindex="-1"><a class="header-anchor" href="#◆-12-4-反向代理负载均衡" aria-hidden="true">#</a> ◆ 12.4 反向代理负载均衡</h3>
<blockquote>
<blockquote>
<p>反向代理服务器的核心工作便是转发HTTP请求，因此它工作在HTTP层面，也就是TCP七层结构中的应用层（第七层），所以基于反向代理的负载均衡也称为七层负载均衡。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>目前几乎所有主流的Web服务器都热衷于支持基于反向代理的负载均衡。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>相比于前面介绍的HTTP重定向和DNS解析，作为负载均衡调度器的反向代理，对于HTTP请求的调度体现在“转发”上，而前者则是“转移”。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>不论是转移还是转发，它们的根本目的都是相同的，那就是希望扩展系统规模，来提高承载能力</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>这种机制的改变，使得调度器完全扮演用户和实际服务器的中间人，这意味着：</p>
<p>● 任何对于实际服务器的HTTP请求都必须经过调度器；</p>
<p>● 调度器必须等待实际服务器的HTTP响应，并将它反馈给用户。</p>
</blockquote>
</blockquote>
<h5 id="按照权重分配任务" tabindex="-1"><a class="header-anchor" href="#按照权重分配任务" aria-hidden="true">#</a> 按照权重分配任务</h5>
<blockquote>
<blockquote>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214111658334.png" alt="image-20211214111658334"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>也就是说，对于以上两台后端服务器，当它们的分配权重为3:1的时候，整个系统可以获得最佳的性能表现。你也许已经想到了，之所以是3:1，原因就在于两台后端服务器的独立成绩刚好近似等于3:1，所以按照它们的能力来分配权重比例，显然可以最大程度地物尽其用。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>支持带有权重分配机制的RR调度策略，不只出现在Nginx中，很多其他的反向代理服务器软件也都将此视为一个必备的内置功能，但是需要强调的是，不论是哪个具体的实现，它们的权重分配机制本身都是相同的，由于这种机制和工作方式的局限，不同实现所表现出的性能几乎不会有太大差异。</p>
</blockquote>
</blockquote>
<p>使用HAProxy来代替Nginx，完成同样的工作，并且依次进行各种权重比例下的压力测试。HAProxy也是一款主流的反向代理服务器，可以作为负载均衡调度器，我们对HAProxy进行后端配置，其中关键部分示例如下：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214111753894.png" alt="image-20211214111753894"></p>
<p>可以看到，我们将HAProxy监听在8003端口，然后为它设置了前面用到的两台后端服务器，HAProxy也同样支持权重分配机制，接下来我们进行同样的压力测试，这里只列出各种权重分配比例下的整体吞吐率，如表12-2所示。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214111844373.png" alt="image-20211214111844373"></p>
<p>在以上四种权重分配比例下，测试结果几乎与使用Nginx时不相上下。</p>
</blockquote>
</blockquote>
<h5 id="调度器的并发处理能力" tabindex="-1"><a class="header-anchor" href="#调度器的并发处理能力" aria-hidden="true">#</a> 调度器的并发处理能力</h5>
<blockquote>
<blockquote>
<p>反向代理服务器本身的并发处理能力显得尤为重要</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>主流的Web服务器都争先恐后地支持反向代理机制，比如Apache、Lighttpd、Nginx等，因为它们作为Web服务器所取得的辉煌成就让它们不费吹灰之力就可以转型成为一台马力强劲的负载均衡调度器。</p>
</blockquote>
</blockquote>
<h5 id="扩展的制约" tabindex="-1"><a class="header-anchor" href="#扩展的制约" aria-hidden="true">#</a> 扩展的制约</h5>
<blockquote>
<blockquote>
<p>我们知道反向代理服务器工作在HTTP层面，对于所有HTTP请求都要亲自转发，可谓是大事小事亲历亲为，这也让我们为它捏了一把冷汗，你也许在怀疑它究竟有多大能耐，能支撑多少后端服务器，的确，这直接关系到整个系统的扩展能力。</p>
<p>我们选择了两台承载能力基本相当的后端服务器，仍然通过反向代理服务器实现负载均衡，它们的网络结构如图12-8所示，其中各服务器的用途和IP地址如表12-3所示。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214112342689.png" alt="image-20211214112342689"></p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214112421794.png" alt="image-20211214112421794"></p>
<p>我们对后端服务器和调度器分别进行了一系列的压力测试，值得一提的是，这一次我们并不是在被测试的服务器上执行ab，而是在与被测试服务器同一网段的其他服务器上执行ab进行压力测试，这样可以减少ab本身的开销对测试结果的影响，同时也是为了和随后的IP负载均衡测试结果保持可比性。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214112509533.png" alt="image-20211214112509533"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214112608737.png" alt="image-20211214112608737"></p>
<p>从图上可以清晰地看出，当num=100000时，内容处理的开销最大，负载均衡系统的整体吞吐率几乎等于两台后端服务器的吞吐率之和，随后当num逐渐减少，整体吞吐率开始渐渐地落后于两台后端服务器的吞吐率之和，当num=1000的时候，整体吞吐率甚至还不如任意一个后端服务器的吞吐率高，如图12-10所示的对比图更加直观地反映了整体吞吐率的变化趋势。</p>
<p>这里我们虽然只用了两台后端服务器，但是已经充分暴露了调度器的“接口人瓶颈”，这种瓶颈效应随着后端服务器内容处理时间的减少而逐渐明显，这不难解释，反向代理服务器进行转发操作本身是需要一定开销的，比如创建线程、与后端服务器建立TCP连接、接收后端服务器返回的处理结果、分析HTTP头信息、用户空间和内核空间的频繁切换等，通常这部分时间并不长，但是当后端服务器处理请求的时间非常短时，转发的开销就显得尤为突出。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>调度器本身的并发处理能力，这也使得当反向代理服务器的吞吐率逐渐接近极限时，无论添加多少后端服务器都将无济于事，因为调度器已经忙不过来了。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>工作在HTTP层面的反向代理服务器扩展能力的制约，不仅来自于自身的对外服务能力，也归咎于其转发开销是否上升为主要时间。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>通常来说，我们希望能够监控后端服务器的很多方面，比如系统负载、响应时间、是否可用、TCP连接数、流量等，它们都是负载均衡调度策略需要考虑的因素。在这里，我们使用Varnish作为调度器，来监控后端服务器的可用性。</p>
<p>Varnish不仅是反向代理缓存，也是支持负载均衡，我们来修改Varnish的配置文件，增加后端服务器和负载均衡策略等配置，如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214113235257.png" alt="image-20211214113235257"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在实际应用中，为了提高整个负载均衡系统的可用性而不影响性能，我们可以部署一定数量的备用后端服务器，这样即便是一些后端服务器出现故障后被调度器放弃，备用后端服务器也可以接替它们的工作，保证整体的性能。</p>
</blockquote>
</blockquote>
<h5 id="粘滞会话" tabindex="-1"><a class="header-anchor" href="#粘滞会话" aria-hidden="true">#</a> 粘滞会话</h5>
<blockquote>
<blockquote>
<p>负载均衡调度器最大程度地让用户不必关心后端服务器，我们知道，当采用RR调度策略时，即便是同一用户对同一内容的多次请求，也可能被转发到了不同的后端服务器，这听起来似乎没什么大碍，但有时候，或许会带来一些问题。</p>
<p>● 当某台后端服务器启用了Session来本地化保存用户的一些数据后，下次用户的请求如果转发给了其他后端服务器，将导致之前的Session数据无法访问；</p>
<p>● 后端服务器实现了一定的动态内容缓存，而毫无规律的转发使得这些缓存的利用率下降。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>如何解决这些问题呢？从表面上看，我们需要做的就是调整调度策略，让用户在一次会话周期内的所有请求始终转发到一台特定的后端服务器上，这种机制也称为粘滞会话（Sticky Sessions），要实现它的关键在于如何设计持续性调度算法。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>既然要让调度器可以识别用户，那么将用户的IP地址作为识别标志最为合适，一些反向代理服务器对此都有支持，比如Nginx和HAProxy，它们可以将用户的IP地址进行Hash计算并散列到不同的后端服务器上。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对于Nginx，只需要在upstream中声明ip_hash即可：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214113617540.png" alt="image-20211214113617540"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对于HAProxy的配置也非常简单，你需要在balance关键字后面添加source策略名称，同时要使用TCP模式：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214113753849.png" alt="image-20211214113753849"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>还可以利用Cookies机制来设计持久性算法，比如调度器将某个后端服务器的编号追加到写给用户的Cookies中，这样调度器便可以在该用户随后的请求中知道应该转发给哪台后端服务器。这样做可以更加细粒度地追踪到每一个用户，试想一下，当有很多用户隐藏在一个公开IP地址后面时，利用Cookies的持久性算法将显得更加有效。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>我们希望将对同一个URL的请求始终转发到同一台特定的后端服务器，以充分利用后端服务器针对该URL进行的本地化缓存，要实现这一点，HAProxy也提供了支持：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214113944398.png" alt="image-20211214113944398"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>粘滞会话可能或多或少地破坏了均衡策略，至少像权重分配这样的动态策略已经无法工作</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>问题的关键在于，我们究竟是否要通过实现粘滞会话来迁就系统的特殊需要呢？在权衡代价之后你认为是否值得呢？最为关键的问题是前面提到的两个问题是否能从根本上避免呢？如果可以，这很值得去考虑。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>事实上，在后端服务器上保存Session数据和本地化缓存，的确是一件不明智的事情，它使得后端服务器显得过于个性化，以至于和整个系统格格不入，如果允许的话，我们应该尽量避免这样的设计，比如采用分布式Session或者分布式缓存等，让后端服务器的应用尽量与本地无关，也可更好地适应环境。</p>
</blockquote>
</blockquote>
<h3 id="◆-12-5-ip负载均衡" tabindex="-1"><a class="header-anchor" href="#◆-12-5-ip负载均衡" aria-hidden="true">#</a> ◆ 12.5 IP负载均衡</h3>
<blockquote>
<blockquote>
<p>反向代理服务器作为负载均衡调度器的工作机制，其本身的开销已经严重制约了这种框架的可扩展性，从而也限制了它的性能极限。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>能否在HTTP层面以下实现负载均衡呢？答案是肯定的</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在数据链路层（第二层）、网络层（第三层）以及传输层（四层）都可以实现不同机制的负载均衡，但有所不同的是，这些负载均衡调度器的工作必须由Linux内核来完成，因为我们希望网络数据包在从内核缓冲区进入进程用户地址空间之前，尽早地被转发到其他实际服务器上，没错，Linux内核当然可以办得到，随后我们会介绍位于内核的Netfilter和IPVS，而用户空间的应用程序对此却束手无策。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>因为可以将调度器工作在应用层以下，这些负载均衡系统可以支持更多的网络服务协议，比如FTP、SMTP、DNS，以及流媒体和VoIP等应用。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>介绍基于NAT技术的负载均衡，因为它可以工作在传输层，对数据包中的IP地址和端口信息进行修改，所以也称为四层负载均衡。</p>
</blockquote>
</blockquote>
<h5 id="dnat" tabindex="-1"><a class="header-anchor" href="#dnat" aria-hidden="true">#</a> DNAT</h5>
<blockquote>
<blockquote>
<p>网络地址转换（Network Address Translation，NAT），它可以让用户身处内部网络却与互联网建立通信</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>反向NAT”，有点类似于反向代理的命名，是的，我们将实际服务器放置在内部网络，而作为网关的NAT服务器将来自用户端的数据包转发给内部网络的实际服务器</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>“反向NAT”，其实就是DNAT，不同于SNAT的是，它需要修改的是数据包的目标地址和端口，这种技术在很多普通的家用宽带路由器上都有支持，如果你曾经配置过任意一款宽带路由器，或许会看到NAT设置，或者也称为端口映射设置，因为我们知道通过NAT可以修改数据包的目的地端口，很好地隐藏内网服务器的实际端口，提高安全性。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>我们利用家用宽带路由器进行NAT设置，再加上前面提到的动态DNS，完全可以在家里搭建一个公开的小型Web站点。</p>
</blockquote>
</blockquote>
<h5 id="nat服务器做了什么" tabindex="-1"><a class="header-anchor" href="#nat服务器做了什么" aria-hidden="true">#</a> NAT服务器做了什么</h5>
<blockquote>
<blockquote>
<p>NAT服务器拥有两块网卡，分别连接外部网络和内部网络，IP地址分别为125．12．12．12和10．0．1．50。与NAT服务器同在一个内部网络的是两台实际服务器，IP地址分别为10．0．1．210和10．0．1．211，它们的默认网关都是10．0．1．50，并且都在8000端口上运行着Web服务。另外，我们假想用户端的IP地址为202．20．20．20。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214144252641.png" alt="image-20211214144252641"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>现在我们以一个数据包的真实旅程为例，来跟踪它是如何从用户端到实际服务器，又如何从实际服务器返回用户端，在这一系列过程中，数据包的命运发生了多次改变。</p>
<p>首先，用户端通过DNS服务器得知站点的IP地址为125．12．12．12，当然，如果站点不使用域名的话，这一步也可能省略。</p>
<p>接下来，用户端向125．12．12．12发送了IP数据包，我们将目光放在其中的一个数据包上，它的来源地址和目标地址如下所示：</p>
<p>来源地址： 202.20.20.20:6584</p>
<p>目标地址： 125.12.12.12:80</p>
<p>当数据包到达125．12．12．12的内核缓冲区后，NAT服务器并没有把它交给用户空间的进程去处理，而是挑选了一台实际服务器，这里恰好为10．0．1．210，接下来NAT服务器将刚刚收到的数据包进行了修改，修改后的来源地址和目标地址如下所示：</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>来源地址： 202.20.20.20:6584</p>
<p>目标地址： 10.0.1.210:8000</p>
<p>可以看到，NAT服务器修改了数据包的目标地址和端口号，紧接着，NAT服务器指定内部网卡将这个数据包原封不动地投递到内部网络中，根据IP层的寻址机制，数据包自然会流入10．0．1．210这台实际服务器。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>接下来，运行在10．0．1．210上的Web服务处理了这个数据包，然后将要包含结果数据的响应数据包投递到内部网络，它的来源地址和目标地址如下所示：</p>
<p>来源地址： 10.0.1.210:8000</p>
<p>目标地址： 202.20.20.20:6584</p>
<p>显然，这个数据包必须先到达默认网关，于是它再次回到了NAT服务器，这时候NAT服务器再次修改数据包，修改后的来源地址和目标地址如下所示：</p>
<p>来源地址： 125.12.12.12:80</p>
<p>目标地址： 202.20.20.20:6584</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>最后，数据包终于回到了用户端。</p>
<p>在整个过程中，NAT服务器的动作可谓天衣无缝，它欺骗了实际服务器，而在实际服务器上运行的进程总是天真地以为数据包是用户端直接发给自己的，不过，这也许是善意的欺骗。同时，NAT服务器也扮演了负载均衡调度器的角色，那么，在讨论调度策略之前，你也许更关心的问题是如何实现NAT服务器。</p>
</blockquote>
</blockquote>
<h5 id="netfilter-iptables" tabindex="-1"><a class="header-anchor" href="#netfilter-iptables" aria-hidden="true">#</a> Netfilter/iptables</h5>
<blockquote>
<blockquote>
<p>我们必须得知道Linux如何修改IP数据包，从Linux 2.4内核开始，其内置的Netfilter模块便肩负起这样的使命，它在内核中维护着一些数据包过滤表，这些表包含了用于控制数据包过滤的规则。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>当网络数据包到达服务器的网卡并且进入某个进程的地址空间之前，先要通过内核缓冲区，这时候内核中的Netfilter便对数据包有着绝对控制权，它可以修改数据包，改变路由规则。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>Linux提供了iptables，它是工作在用户空间的一个命令行工具，我们可以通过它来对Netfilter的过滤表进行插入、修改或删除等操作，也就是建立了与Netfilter沟通的桥梁，告诉它我们的意图。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>我们要做的就是让Linux服务器成为连接外部网络和私有网络的路由器，但这不是普通的路由器，我们知道路由器的工作是存储转发，除了修改数据包的MAC地址以外，通常它不会对数据包做其他手脚，而我们要实现的路由器恰恰是要对数据包进行必要的修改，包括来源地址和端口，或者目标地址和端口。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>总之，Linux内核改变数据包命运的惊人能力，决定了我们可以构建强大的负载均衡调度器，将请求分散到其他实际服务器上。</p>
</blockquote>
</blockquote>
<h5 id="用iptables来实现调度器" tabindex="-1"><a class="header-anchor" href="#用iptables来实现调度器" aria-hidden="true">#</a> 用iptables来实现调度器</h5>
<blockquote>
<blockquote>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214145424780.png" alt="image-20211214145424780"></p>
<p>我们将用iptables来实现NAT，在此之前，我们需要执行以下的命令行操作：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214145458693.png" alt="image-20211214145458693"></p>
<p>这意味着该服务器将被允许转发数据包，这也为它成为NAT服务器提供了可能。接下来，我们在作为调度器的服务器上执行以下的iptables规则：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214145513919.png" alt="image-20211214145513919"></p>
<p>这条规则几乎完成了所有DNAT的实现，它将调度器外网网卡上8001端口接收的所有请求转发给10．0．1．210 这台服务器的8000 端口</p>
<p>我们在调度器上执行以下的iptables规则：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214145549446.png" alt="image-20211214145549446"></p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214145626298.png" alt="image-20211214145626298"></p>
<p>查看路由表，刚才的默认网关已经出现了：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214145642272.png" alt="image-20211214145642272"></p>
<p>现在，我们终于可以通过调度器的8001和8002端口分别将请求转发到两个实际服务器上，可是，回想前面的问题，用iptables来实现负载均衡调度器，看起来有点困难，iptables似乎只能按照我们的规则来干活，没有调度器应该具备的调度能力和调度策略。</p>
<p>接下来，IPVS上场的时刻到了。</p>
</blockquote>
</blockquote>
<h5 id="ipvs-ipvsadm" tabindex="-1"><a class="header-anchor" href="#ipvs-ipvsadm" aria-hidden="true">#</a> IPVS/ipvsadm</h5>
<blockquote>
<blockquote>
<p>IPVS不仅可以实现基于NAT的负载均衡，同时还包括后面要介绍的直接路由和IP隧道等负载均衡。令人振奋的是，IPVS模块已经内置到Linux 2．6．x内核中，这意味着使用Linux 2．6．x内核的服务器将无须重新编译内核就可以直接使用它。</p>
<p>要想知道内核中是否已经安装IPVS模块，可以进行以下查看：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214145804251.png" alt="image-20211214145804251"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>IPVS也需要管理工具，那就是ipvsadm，它为我们提供了基于命令行的配置界面，你可以通过它快速实现负载均衡系统，这里也称为LVS（Linux VirtualServer，Linux虚拟服务器）或者集群。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>我们将使用ipvsadm来实现基于NAT的负载均衡系统，也就是LVS-NAT</p>
</blockquote>
</blockquote>
<h5 id="lvs-nat" tabindex="-1"><a class="header-anchor" href="#lvs-nat" aria-hidden="true">#</a> LVS-NAT</h5>
<blockquote>
<blockquote>
<p>考验NAT服务器转发能力的时刻到了，由于转发数据包工作在内核中，我们几乎可以不考虑额外的开销，所以，转发能力主要取决于NAT服务器的网络带宽，包括内部网络和外部网络。举个例子，假如NAT服务器通过100Mbps的交换机与多台实际服务器组成内部网络，通过前面介绍带宽的章节，我们知道这些实际服务器到NAT服务器的带宽为共享100Mbps，这样一来，尽管实际服务器本身可以很容易达到100Mbps的响应流量，比如提供下载服务等，但是NAT服务器的100Mbps出口带宽成了制约条件，使得无论添加多少台实际服务器，整个集群最多只能提供100Mbps的服务。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>要解决网关带宽的瓶颈也并不困难，我们可以为NAT服务器使用千兆网卡，并且为内部网络使用千兆交换机</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>如果我们希望继续扩展带宽，比如实际服务器使用千兆网卡，那么NAT服务器将使用万兆（10Gb）网卡以及万兆交换机，可是，这些设备的昂贵价格足可以让你再购买几台服务器，至少目前是这样。即便是为NAT服务器使用万兆网卡，充分满足网络带宽，但是服务器本身能够以这样的速度转发数据包吗？我们得看服务器的总线带宽，这取决于总线频率和总线位宽，比如对于支持400MHz前端总线频率的32位处理器，理论上它的总线带宽为：[插图]这意味着CPU和内存直接交换数据的速度，当网络数据包通过10Gbps的带宽进入内核缓冲区后，理论上可以快速地执行转发，也就是将缓冲区中的数据包进行简单修改后复制到另一块负责发送数据的缓冲区。当然，实际的转发速度肯定是达不到12．8Gbps的，还要考虑处理时间等其他因素。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>假如你不想花钱去购买千兆交换机或万兆交换机，甚至负载均衡硬件设备，又不想让NAT服务器成为制约扩展的瓶颈，怎么办呢？一个简单有效的办法是，将基于NAT的集群和前面的DNS-RR混合使用，你可以组建多个条件允许的NAT集群，比如5个100Mbps出口带宽的集群，然后通过DNS-RR方式来将用户请求均衡地指向这些集群，同时，你还可以利用DNS智能解析实现地域就近访问。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对于大多数中等规模的站点，拥有5个100Mbps的NAT集群，再加上DNS-RR，通常足够应付全部的业务。</p>
</blockquote>
</blockquote>
<h3 id="◆-12-6-直接路由" tabindex="-1"><a class="header-anchor" href="#◆-12-6-直接路由" aria-hidden="true">#</a> ◆ 12.6 直接路由</h3>
<h3 id="◆-12-7-ip隧道" tabindex="-1"><a class="header-anchor" href="#◆-12-7-ip隧道" aria-hidden="true">#</a> ◆ 12.7 ip隧道</h3>
<h3 id="◆-12-8-考虑可用性" tabindex="-1"><a class="header-anchor" href="#◆-12-8-考虑可用性" aria-hidden="true">#</a> ◆ 12.8 考虑可用性</h3>
<blockquote>
<blockquote>
<p>为了实现高可用性的系统，我们不能容忍任何的单点故障，即便只是偶然。所谓的单点故障，是指系统中一旦某个组件发生故障，便会导致整个系统的失败，所以这种故障是致命的。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在负载均衡系统中，多台实际服务器在分散开销的同时，本身也提高了实际服务器的可用性，一般来说，为了在个别实际服务器发生故障后整个系统能够继续承载同样的负载，我们必须将实际服务器的数量保证在略多于实际情况下的数目，这也有利于避免由于大量突发请求造成的雪崩效应。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对于调度器，转移请求的机制注定它存在单点故障，为此，我们必须通过其他办法来有效实现故障平滑转移，以保证调度器的高可用性。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>Heartbeat可以很好地解决这个问题，它的诞生正是为了实现高可用性。简单地说，我们可以准备一台备用调度器，通过运行Heartbeat对主调度器进行心跳检测，一旦发现主调度器停止心跳，便立即启动故障转移，接管主调度器，这个接管过程包括IP别名变更、相关服务的启动等。随后，一旦主调度器恢复后，备用调度器便自动将相关资源转交回主调度器。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>为了避免主调度器和备用调度器之间线路的单点故障，大量的事实证明，最好采用多条独立线路进行连接，这样将不依赖需要电源的交换机。你也可以使用串行电缆，通过服务器的COM端口进行连接，虽然线路长度有限，但这样还可以带来一点点额外的安全性，即便是主调度器被恶意破坏者登录，它也无法通过串行电缆登录备用调度器。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>至于交换机，由于异常流量造成阻塞的情况时有发生，但大多数情况下发生在IDC级别的交换机上，所以不需要我们担心。一般来说，IDC级别的交换机在物理层发生故障的概率较小，即便是发生了，更换备用交换机也并不困难，灾难总是在所难免的。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>物理层设备的网线，也有可能发生故障</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>如果希望让线路不存在单点故障，可以使用Linux Bonding技术来将多条线路绑定在一台服务器的多个网卡上，对流量进行RR负载均衡，或者将一条线路设置成为备用模式。可以通过以下方法查看内核是否支持Bonding，否则便需要将它编译到内核。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214150550291.png" alt="image-20211214150550291"></p>
</blockquote>
</blockquote>
<h2 id="◆-第13章-共享文件系统" tabindex="-1"><a class="header-anchor" href="#◆-第13章-共享文件系统" aria-hidden="true">#</a> ◆ 第13章 共享文件系统</h2>
<blockquote>
<blockquote>
<p>在实际情况中，一旦存在多台实际服务器提供同样的Web服务时，一个潜在的问题便开始浮现，如何来保证多台实际服务器拥有一致的程序和文件呢？你也许觉得拥有一致的程序并不难，没错，因为它们的更新通常并不频繁，即便是采用持续构建，每天一次的程序更新已经很了不起了。</p>
<p>文件的一致性呢？这是我们不得不面对的现实问题，比如提供照片分享服务的站点，用户发起的照片下载请求被转移到多台实际服务器上，而这些照片正是由用户不断上传的文件，如何保证上传的照片同时存在于多台实际服务器上呢？</p>
</blockquote>
</blockquote>
<h3 id="◆-13-1-网络共享" tabindex="-1"><a class="header-anchor" href="#◆-13-1-网络共享" aria-hidden="true">#</a> ◆ 13.1 网络共享</h3>
<blockquote>
<blockquote>
<p>从某个角度来看，MySQL服务器正是扮演了可以让多台Web服务器通过网络来访问的共享存储，而这一形式也适用于文件</p>
<p>从使用的角度来看，共享文件系统几乎不需要你考虑网络传输和访问的细节，你完全可以像访问本地文件一样地访问网络上其他服务器文件系统上的文件。通过有效地使用共享文件系统，前面的问题将可以得到一定程度的解决，我们可以为集群中的多台实际服务器共享同一个物理存储设备，当然，用户上传的照片也将直接存储在这里。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214151102249.png" alt="image-20211214151102249"></p>
<p>共享文件系统本身并不是我们通常意义上讲的磁盘文件系统，它不能用于存储和管理磁盘数据，而只是定义了文件在网络传输过程中的组织格式和传输协议，所以，一个文件从网络的一端到达另一端的过程中，需要进行两次格式转换，分别发生在进入网络和离开网络的时候。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对于共享文件系统的实现，常用的有NFS（Network File System）和Samba，其中NFS主要用于UNIX/Linux平台下，而Samba的设计初衷是用于将UNIX/Linux的文件映射到Windows的网上邻居中，实现UNIX/Linux到Windows的文件共享，但同时它也支持UNIX/Linux平台之间的文件共享。</p>
</blockquote>
</blockquote>
<h3 id="◆-13-2-nfs" tabindex="-1"><a class="header-anchor" href="#◆-13-2-nfs" aria-hidden="true">#</a> ◆ 13.2 NFS</h3>
<blockquote>
<blockquote>
<p>NFS由Sun公司于1984年开发，直到现在，它一直都是主流异构平台实现文件共享的首选方案。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>共享文件系统主要包括文件格式和传输协议的定义</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>NFS并没有设计自己的传输协议，而是直接基于RPC（Remote Procedure Calls）协议，它和HTTP一样都工作在应用层，负责客户端和服务器端之间请求与响应数据的传输控制。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>NFS的服务器端和客户端软件包一般都在Linux中被默认安装，配置它们非常容易，比如我们希望将10．0．1．200这台服务器的/data目录共享给10．0．1．201这台服务器，那么首先需要在10．0．1．200上声明如下：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214151223542.png" alt="image-20211214151223542"></p>
<p>然后启动NFS服务器端：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214151231582.png" alt="image-20211214151231582"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>这意味着它允许IP地址为10．0．1．201的服务器对自己的/data目录进行读写操作，并且写操作采用同步方式。然后我们需要在10．0．1．201这台服务器上执行mount操作，将10．0．1．200共享的目录绑定到自己的文件系统中，如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214151311212.png" alt="image-20211214151311212"></p>
<p>当然，在执行这行命令之前，你必须首先创建/mnt/data目录。</p>
</blockquote>
</blockquote>
<h5 id="基于rpc传输" tabindex="-1"><a class="header-anchor" href="#基于rpc传输" aria-hidden="true">#</a> 基于RPC传输</h5>
<blockquote>
<blockquote>
<p>NFS并不包括自己的传输协议，而是利用了Linux内置的RPC服务。通过rpcinfo，我们可以很容易地查看所有基于RPC的服务，对于刚才打开NFS服务器端的服务器，执行rpcinfo命令后的结果如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214151403070.png" alt="image-20211214151403070"></p>
<p>虽然NFS采用RPC作为应用层协议，但是其性能更多地取决于传输层协议TCP/UDP，以及服务器端和客户端程序的实现。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对于传输层，RPC服务默认使用了UDP，但是当网络质量较差的时候，比如使用WAN，一旦数据包发生丢失，这些数据包的重发工作需要由应用层的RPC来进行，它将重新发送整个RPC请求。而如果采用TCP来进行底层数据传输控制，其自身的自动重传机制可以针对某个数据包进行重发，这无疑更加高效。另外，TCP还可以更好地根据网络环境进行流量控制。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>NFS的服务器端程序采用多进程（nfsd）模型，并且进程数是固定的，默认情况下为4。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>可以通过修改/etc/sysconfig/nfs将NFS服务器端进程数适当提高</p>
</blockquote>
</blockquote>
<h5 id="统计i-o" tabindex="-1"><a class="header-anchor" href="#统计i-o" aria-hidden="true">#</a> 统计I/O</h5>
<blockquote>
<blockquote>
<p>NFS为我们提供了I/O统计界面，通过nfsstat命令，你可以随时查看任何NFS客户端对远程的文件进行了哪些操作，这些数据可以帮助你更好地了解NFS客户端的工作状态，以及各种远程I/O操作的统计和比例。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214151638129.png" alt="image-20211214151638129"></p>
<p>具体可以详细再看。</p>
</blockquote>
</blockquote>
<h3 id="◆-13-3-局限性" tabindex="-1"><a class="header-anchor" href="#◆-13-3-局限性" aria-hidden="true">#</a> ◆ 13.3 局限性</h3>
<blockquote>
<blockquote>
<p>不同于本地磁盘I/O，当我们通过NFS来对远程文件进行读写操作时，影响性能的因素不仅包括NFS服务器本身的磁盘吞吐率上限，还有NFS服务器端的并发处理能力以及网络带宽等。</p>
</blockquote>
</blockquote>
<h2 id="◆-第14章-内容分发与同步" tabindex="-1"><a class="header-anchor" href="#◆-第14章-内容分发与同步" aria-hidden="true">#</a> ◆ 第14章 内容分发与同步</h2>
<h3 id="◆-14-1-复制" tabindex="-1"><a class="header-anchor" href="#◆-14-1-复制" aria-hidden="true">#</a> ◆ 14.1 复制</h3>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214152105328.png" alt="image-20211214152105328"></p>
<p>Web服务器将可以直接读取本地磁盘的图片来响应用户的HTTP请求，这意味着只要Web负载均衡调度器不出意外，那么文件访问本身将不再成为瓶颈，因为共享文件系统的单点性能问题已经彻底不存在了。</p>
<p>如何实现复制呢？更加严峻的问题是在面对大量的服务器进行复制时，我们需要考虑哪些策略呢？</p>
<p>总的来说，我们可以通过两种方式来实现复制，分别为主动分发和被动同步，主要区别在于复制的发起方和触发方式不同，所以，这里的“被动”实质上是相对于发送文件一端而言的。无论如何，它们都基于TCP/IP网络来传输数据。</p>
<h3 id="◆-14-2-ssh" tabindex="-1"><a class="header-anchor" href="#◆-14-2-ssh" aria-hidden="true">#</a> ◆ 14.2 SSH</h3>
<blockquote>
<blockquote>
<p>提到SSH（Secure Shell），大家并不陌生，它是建立在应用层和传输层基础上的安全协议，可以用于传输任何数据，我们希望用它来实现文件复制，当然，这属于主动分发的方式。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>SSH有很多功能，它既可以代替Telnet，又可以为FTP、POP等协议提供安全的传输通道。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>它对传输的数据进行了压缩，以加快传输速度</p>
<p>我们这里需要的并不是基于命令行的SSH，而是将SSH的能力集成到站点的应用程序中</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>很多用于Web开发的主流语言都拥有SSH客户端代码库或者相应的扩展。这里我们以PHP为例，它可以通过PECL扩展来实现SSH客户端操作，更重要的是，它还实现了基于SSH的SCP和SFTP，这正是我们进行文件复制所需要的。</p>
</blockquote>
</blockquote>
<h5 id="scp" tabindex="-1"><a class="header-anchor" href="#scp" aria-hidden="true">#</a> SCP</h5>
<blockquote>
<blockquote>
<p>在PHP程序中通过SCP来进行文件分发并不困难，按照函数手册的指引，不需要几行代码就可以完成服务器之间的文件复制。这里有一点需要注意，在SSH服务器端，也就是分发文件的目标服务器上，我们需要对SSH的服务器端配置选项进行一些修改，通常配置文件为/etc/ssh/sshd_config，我们修改以下内容：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214152447645.png" alt="image-20211214152447645"></p>
<p>这样一来可以减少SSH服务器进行DNS解析的时间，并且允许我们在Web应用程序中通过密码验证的方式来登录SSH服务器。</p>
<p>一个用PHP编写的用于分发文件的例子如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214152527227.png" alt="image-20211214152527227"></p>
<p>通过基于SSH通道的SCP进行文件分发，它的I/O延迟与前面的NFS相比，结果如何呢？</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214152542640.png" alt="image-20211214152542640"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>与前面NFS的测试结果相比，我们不难看出，对于22KB的文件，SCP花费了较多的时间，这或许归咎于额外的身份验证机制。对于23MB的大文件，远程SCP传输的时间和NFS的测试结果不相上下，这时候身份验证的开销已经成为次要因素，而在本地SCP传输中，身份验证的开销再次成为不可忽略的一部分。</p>
<p>显然，在绝对速度的对比中，SCP并没有什么明显的优势，但我们要知道，关键的问题是，在我们试图通过文件复制来创建更多副本的那一刻起，绝对速度已经不是那么重要了，相比之下，如何将文件复制到更多的服务器上成为延续生命力的关键所在。所以，这又回到应该选择共享文件系统还是文件复制的问题上，对此，我们的目的已经完全不同。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在共享文件系统中，文件都存储在NFS服务器上，即便网络带宽可以充分满足需要，但总有一天NFS服务器的磁盘会成为性能的杀手，从而制约更大规模的共享访问。而在文件分发机制中，文件被复制到多台服务器上，这本身便分散了磁盘I/O，相比于NFS的集中式访问，这相当于是针对磁盘I/O的负载均衡。</p>
</blockquote>
</blockquote>
<h5 id="sftp" tabindex="-1"><a class="header-anchor" href="#sftp" aria-hidden="true">#</a> SFTP</h5>
<blockquote>
<blockquote>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214152807430.png" alt="image-20211214152807430"></p>
<p>通过SFTP，我们可以对远程服务器的文件系统进行任何操作，就像操作本地的文件系统一样</p>
</blockquote>
</blockquote>
<h5 id="多级分发" tabindex="-1"><a class="header-anchor" href="#多级分发" aria-hidden="true">#</a> 多级分发</h5>
<blockquote>
<blockquote>
<p>在较大规模的站点中，我们可能需要将文件复制到更多的服务器上，比如我们拥有200台静态内容服务器，它们通过LVS和DNS等混合方式实现负载均衡，并且分布在不同地域的IDC中，对于任何静态内容的更新，我们都需要快速地更新到这200台服务器上。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>直接将文件从文件服务器分发到200台Web服务器很容易产生以下问题：</p>
<p>● 由于目标服务器较多，并且部署在不同地域，分发过程会持续较长时间，这会造成一部分目标服务器表现出较大的内容更新延迟。</p>
<p>● 大量消耗文件服务器的本地系统资源，当分发任务较多时，本地会产生大量进程阻塞，影响其他任务的正常运行。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>为此，我们可以考虑进行多级分发</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214152946887.png" alt="image-20211214152946887"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>这样一来，文件服务器将很大一部分工作量巧妙地转移到了更多的Web服务器上，并且可以充分利用地域就近分发策略，一些Web服务器可以更快地将文件转发给同地域的其他Web服务器。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>可以基于一系列的开源产品进行快速构建，比如：</p>
<p>● 通过HTTP请求来触发某台Web服务器启动某文件的分发；</p>
<p>● Web服务器使用异步的方式进行文件分发，分发采用前面提到的SSH扩展，分发结束后根据需要通知下一级进行分发。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对于以上的多级文件分发，我们应该将用于分发的网络进行最大程度的流量隔离，使它不要消耗Web服务器的WAN带宽。一般而言，对于跨地域或者跨IDC的文件分发，除非使用专线，否则必须通过WAN来进行，而对于同一IDC内的文件分发，我们完全可以根据需要来组建LAN，实现分发流量隔离。</p>
</blockquote>
</blockquote>
<h3 id="◆-14-3-webdav" tabindex="-1"><a class="header-anchor" href="#◆-14-3-webdav" aria-hidden="true">#</a> ◆ 14.3 WebDAV</h3>
<blockquote>
<blockquote>
<p>实现文件分发的简单方式是利用HTTP扩展协议WebDAV。RFC2518对WebDAV进行了详细的定义，它允许我们基于HTTP/1．1协议来对Web服务器进行远程文件操作，包括文件和目录的创建、修改和修改等，另外，WebDAV的设计目的还包括了对于版本控制的支持，还记得Subversion的HTTP工作方式吗？它便是使用WebDAV来实现的。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>主流的Web服务器软件几乎都支持WebDAV扩展，比如Apache和Lighttpd都有相应的WebDAV模块，安装并配置它们非常容易，同时，你可以使用litmus来测试它们对于WebDAV的支持程度，比如是否支持COPY、MOVE等指令，并且能否正常工作等。</p>
<p>WebDAV完全基于HTTP，它的定义方式与RESTful风格十分接近，或者说RESTful是对WebDAV风格的回归，无论如何，WebDAV很容易使用，你几乎可以自己构造客户端HTTP请求，比如：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214153243336.png" alt="image-20211214153243336"></p>
<p>这意味着我们请求Web服务器创建相应的目录，随后的HTTP响应如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214153256755.png" alt="image-20211214153256755"></p>
</blockquote>
</blockquote>
<h3 id="◆-14-4-rsync" tabindex="-1"><a class="header-anchor" href="#◆-14-4-rsync" aria-hidden="true">#</a> ◆ 14.4 rsync</h3>
<blockquote>
<blockquote>
<p>除了SCP或WebDAV等主动分发方式，我们还可以采用被动同步的方式来实现文件复制，在这种情况下，接收文件的一端将主动向文件服务器发起同步请求，并根据两端文件列表的差异，有选择性地进行更新，从而保证它和文件服务器的内容一致。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>Linux下的rsync工具便可以非常出色地完成这项任务，但值得一提的是，通常情况下rsync可以采用SSH作为传输通道，但这种方式在性能上受限于SSH的传输机制</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>我们尽量使用rsync的独立服务器端进程rsyncd来负责文件传输，它同时也将使用独立的服务器端口。</p>
</blockquote>
</blockquote>
<h3 id="◆-14-5-hash-tree" tabindex="-1"><a class="header-anchor" href="#◆-14-5-hash-tree" aria-hidden="true">#</a> ◆ 14.5 Hash tree</h3>
<blockquote>
<blockquote>
<p>rsync在同步文件时需要分析目录中每一个文件的更新标记，当有多级目录时，依然如此。如果能让它在减少扫描范围或者次数的同时，仍然可以实现目的，那当然是我们所希望的。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>Linux内核中的inotify模块可以帮助我们完成这一工作，简单地说，它通过在内核中监视文件系统的一举一动，可以在任何文件的修改时间发生变化后，发出事件通知。这样一来，我们可以通过inotify提供的API来编写守护进程，持续不断地监视文件修改事件，一旦发现某个文件被修改，便随之更新上级甚至更上一级目录的修改时间，直到被同步目录的根节点。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>inotify本身提供了基于C语言的API，除此之外，你也可以在PHP中使用相应的PECL扩展</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>这种将文件的修改时间不断向上传递的机制体现了Hash tree的动机，但有所不同的是，在Hash tree的存储结构中，我们通过特定的Hash算法，为树中的每个非叶节点生成一个可以描述它所有子节点唯一性的特征字符串，以便提供对节点信息完整性和正确性的验证，这样一来，一旦某个节点的信息发生变化，那么这种变化便会向上传递，体现在上级节点的特征字符串中。</p>
</blockquote>
</blockquote>
<h3 id="◆-14-6-分发还是同步" tabindex="-1"><a class="header-anchor" href="#◆-14-6-分发还是同步" aria-hidden="true">#</a> ◆ 14.6 分发还是同步</h3>
<blockquote>
<blockquote>
<p>对于分发和同步这两种文件复制方式</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 文件分发需要依赖一定的应用程序逻辑，比如通过SCP扩展来编写代码控制文件传输，或者自己开发专用的分发程序，而文件同步可以对应用程序完全透明，部署非常简单；</p>
<p>● 文件分发可以更好地控制触发条件，更加灵活，比如更容易实现多级复制，也就是前面介绍的多级分发；</p>
<p>● 文件同步（比如利用rsync）是一种天然的异步复制操作，不阻塞Web应用程序的运行，而在Web应用中要想对文件分发实现异步复制操作，必须得借助其他支持，比如后面要介绍的异步计算。</p>
</blockquote>
</blockquote>
<h2 id="◆-第15章-分布式文件系统" tabindex="-1"><a class="header-anchor" href="#◆-第15章-分布式文件系统" aria-hidden="true">#</a> ◆ 第15章 分布式文件系统</h2>
<blockquote>
<blockquote>
<p>当我们为多台Web服务器实现负载均衡后，用户的请求通过调度器被分散到不同的Web服务器上。但是，随之产生一个问题，多台Web服务器如何共享一些文件呢？比如用户上传的照片、定时更新的静态化页面等。这时候，我们通过NFS实现了文件共享，这使得所有Web服务器都可以像访问本地磁盘一样访问同一个远程目录，但是没过多久，它便不能胜任成为众人关注的焦点，大量的集中访问让它喘不过气来。于是，我们开始考虑为文件创建多份副本，将它们复制到多台服务器上。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>我们通过分发和同步等方式来实现复制</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>● 需要维护大量的rsync同步脚本，或者需要配置大量的分发参数，尤其对于大规模的集群，这些工作很容易让人产生挫败感，而且毫无乐趣；</p>
<p>● 缺乏整体的管理和监控，我们需要能够一览无余地了解所有复制工作的状态，以及更多细节报告，这有助于我们更好地了解它们在运行中的状态。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>这时候，分布式文件系统是一个值得考虑的选择，它既可以满足文件复制的需要，同时又带来了像文件系统一样的可管理性。</p>
</blockquote>
</blockquote>
<h3 id="◆-15-1-文件系统" tabindex="-1"><a class="header-anchor" href="#◆-15-1-文件系统" aria-hidden="true">#</a> ◆ 15.1 文件系统</h3>
<blockquote>
<blockquote>
<p>分布式文件系统并不是传统意义上的文件系统，它工作在操作系统的用户空间，由应用程序来实现，比如MogileFS或Hadoop等。所以，这使得分布式文件系统并不依赖于底层文件系统的具体实现，只要是POSIX兼容的文件系统即可。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>如何来访问分布式文件系统呢？通常，分布式文件系统会提供专用的访问接口，比如以Hadoop实现的分布式文件系统为例，我们来对它进行一系列的访问操作。首先，我们来查看目录列表：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214154105397.png" alt="image-20211214154105397"></p>
<p>可以看到，hadoop程序为我们提供了访问入口，通过不同的参数选项，我们可以非常轻松地操作分布式文件系统，接下来，我们创建html目录，如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214154126645.png" alt="image-20211214154126645"></p>
<p>然后我们就可以将本地文件复制到分布式文件系统中，如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214154144121.png" alt="image-20211214154144121"></p>
<p>我们将本地的index．htm文件复制到了刚刚创建的html目录中，有点像操作FTP的感觉。现在，我们可以查看html目录下的文件列表，如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214154210061.png" alt="image-20211214154210061"></p>
<p>返回的结果看起来非常亲切，跟本地文件系统没什么区别。现在我们来读取文件内容，如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214154230091.png" alt="image-20211214154230091"></p>
<p>从文件系统的传统思想来看，这种访问方式非常优雅。的确，就是这么简单。同样，我们再以MogileFS为例，看看它提供的访问方式。</p>
<p>有点不同的是，MogileFS没有目录的概念，它要求每个文件都要有唯一的Key，比如我们希望将index．htm文件复制到MogileFS分布式文件系统中，如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214154301847.png" alt="image-20211214154301847"></p>
<p>这里我们为index．htm创建了名为index的Key，同样，当我们读取这个文件时，也必须指定Key为index，如下所示：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214154315809.png" alt="image-20211214154315809"></p>
<p>看来，Hadoop和MogileFS的具体访问方式各不相同，这取决于它们的不同设计和实现，的确，每一个实现分布式文件系统的具体产品都有个性化的访问接口，但透过访问方式的差异，它们所扮演的角色往往都是相同的。</p>
</blockquote>
</blockquote>
<h3 id="◆-15-2-存储节点和追踪器" tabindex="-1"><a class="header-anchor" href="#◆-15-2-存储节点和追踪器" aria-hidden="true">#</a> ◆ 15.2 存储节点和追踪器</h3>
<blockquote>
<blockquote>
<p>在分布式文件系统中，所有的文件都存储在被称为存储节点（StroageNode）的地方，一个存储节点往往对应物理磁盘上的一个实际目录</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>这样一来，我们用多台服务器创建多个存储节点，文件便会在这些存储节点之间根据规则进行自动复制。另外，我们也可以在同一台物理服务器上创建多个存储节点，分别指向不同的磁盘，实现冗余备份，相当于某种意义上实现了RAID机制。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对于多个存储节点，客户端如何与它们打交道呢？必须得有一定的调度机制，而追踪器（Tracker）正是负责完成这项工作，它发挥着客户端和存储节点之间桥梁的作用，就像负载均衡系统中的调度器一样，但应该说更像基于DNS的调度器，因为大多数时候，当客户端通过追踪器找到合适的存储节点后，客户端将直接与存储节点服务器进行文件读写通信，这样设计也可以最大程度地减少追踪器的瓶颈效应。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214154719926.png" alt="image-20211214154719926"></p>
<p>踪器要做的事情非常多，通常包括：</p>
<p>● 维护存储节点的各种信息，知道哪些文件存储在哪些节点上，这些信息可以存储在文件或者数据库中，比如MogileFS使用MySQL来存储；</p>
<p>● 监听在特定的TCP端口，与客户端以及存储节点进行通信，为客户端提供文件访问接口；</p>
<p>● 控制文件复制策略；</p>
<p>● 实现存储节点的负载均衡；</p>
<p>● 实现存储节点的故障转移，提高存储系统的可用性。</p>
</blockquote>
</blockquote>
<h3 id="◆-15-3-mogilefs" tabindex="-1"><a class="header-anchor" href="#◆-15-3-mogilefs" aria-hidden="true">#</a> ◆ 15.3 MogileFS</h3>
<blockquote>
<blockquote>
<p>MogileFS是一个开源的分布式文件系统，它采用Perl编写，包括追踪器、存储节点，以及一些管理工具，除此之外，追踪器使用MySQL来存储分布式文件系统运行中的所有信息。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>MogileFS适合存储那些一次保存多次读取（write-once-read-many）的文件，比如用户上传的图片等，而对于需要频繁更新的静态网页并不那么适合。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>MogileFS提供了一个非常易用的管理工具mogadm，通过它我们可以轻松地管理追踪器，从而管理所有的存储节点。</p>
</blockquote>
</blockquote>
<h2 id="◆-第16章-数据库扩展" tabindex="-1"><a class="header-anchor" href="#◆-第16章-数据库扩展" aria-hidden="true">#</a> ◆ 第16章 数据库扩展</h2>
<h3 id="◆-16-1-复制和分离" tabindex="-1"><a class="header-anchor" href="#◆-16-1-复制和分离" aria-hidden="true">#</a> ◆ 16.1 复制和分离</h3>
<h5 id="主从复制" tabindex="-1"><a class="header-anchor" href="#主从复制" aria-hidden="true">#</a> 主从复制</h5>
<blockquote>
<blockquote>
<p>几乎所有的主流数据库产品都支持复制，这是它们进行简单扩展的基本手段，我们以MySQL为例，它支持主从复制，配置并不复杂，简单地说，你只需要做到以下两点：</p>
<p>● 开启主服务器上的二进制日志（log-bin）。</p>
<p>● 在主服务器和从服务器上分别进行简单的配置和授权。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>MySQL的主从复制是依据主服务器的二进制日志进行的，也就是说主服务器日志中记录的操作会在从服务器上进行重放，从而实现复制，所以主服务器必须开启二进制日志，它会自动记录所有对数据库产生更新的操作，也包括潜在的更新操作，比如没有删除任何实际记录的DELETE操作。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>这种复制是异步进行的，从服务器定时向主服务器请求最新日志，而主服务器只需要通过一个I/O线程来读取本地二进制日志，并输送给从服务器即可，所以，复制过程对于主服务器的影响非常有限。但是，当存在多个从服务器同时从一个主服务器进行复制的时候，主服务器的磁盘压力会有不同程度的增长，这也并不是无法解决的，你可以采用多级复制策略，就像前面提到的多级分发一样。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在主服务器上对于所有更新操作记录二进制日志，这部分额外开销对性能大概有1%的影响，与复制的意义相比，这几乎是微不足道的。</p>
</blockquote>
</blockquote>
<h5 id="读写分离" tabindex="-1"><a class="header-anchor" href="#读写分离" aria-hidden="true">#</a> 读写分离</h5>
<blockquote>
<blockquote>
<p>对于所有的更新操作，我们必须让它作用于主服务器上，这样才能保证所有数据库服务器上的数据一致。这也是任何使用单向复制机制的系统必须遵循的更新原则，比如在前面提到的文件分发和同步中，我们也只能对文件源进行更新。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>我们采用读写分离（R/W Splitting）的方法将应用程序中对数据库的写操作指向主服务器，而将读操作指向从服务器。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214155504025.png" alt="image-20211214155504025"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>这样做保证了多台服务器的数据一致性，一般而言，大多数站点的数据库读操作要比写操作更加密集，而且查询条件相对复杂，你可以通过mysql-report来查看SELECT操作的比例，通过前面对数据库性能优化的介绍，我们知道大部分的开销都消耗在这些查询上，而通过读写分离，我们可以将大量的读操作剥离出来，转移到更多的从服务器上实现水平扩展，这是一个非常简单而有效的策略。</p>
<p>对于以用户创造内容为主的站点来说，数据库写操作依然密集，随后我们会介绍通过分区来实现扩展的思路。</p>
<p>一旦决定采用读写分离，我们必须在应用程序中配置多个数据库连接选项，并且修改数据访问层代码，应用程序知道应该如何区分读写操作，对于将读操作分散到多台从服务器上，应用程序就不那么擅长了，简单的随机分配可能会造成多台从服务器的工作量不均衡，更重要的是，当某台从服务器发生故障后，应用程序并不知道。你也可以不修改应用程序，而将这部分工作交给数据库反向代理，我们来看看它的作用。</p>
</blockquote>
</blockquote>
<h5 id="使用数据库反向代理" tabindex="-1"><a class="header-anchor" href="#使用数据库反向代理" aria-hidden="true">#</a> 使用数据库反向代理</h5>
<blockquote>
<blockquote>
<p>如果你在使用MySQL，那么可以尝试MySQL Proxy，它工作在应用程序和MySQL服务器之间，负责所有请求和响应数据的转发。</p>
<p>就像Web反向代理服务器一样，MySQL Proxy同样可以在SQL语句转发到后端的MySQL服务器之前对它进行修改，比如将所有对数据库A更新的SQL语句修改为更新数据库B，这也许有些不合时宜，但充分体现了MySQL Proxy大权在握，那么对于读写分离，它同样可以实现。</p>
<p>MySQL Proxy通过LUA脚本来描述转发规则，以下的LUA代码用于实现读写分离：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214155931779.png" alt="image-20211214155931779"></p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214155942812.png" alt="image-20211214155942812"></p>
<p>MySQL Proxy默认监听在4040 端口，我们可以在启动时通过选项参数来指定后端的MySQL服务器：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214155957211.png" alt="image-20211214155957211"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>应用程序只需要跟MySQL Proxy通信即可，而读写分离的工作都由MySQL Proxy来完成，与此同时，MySQL Proxy还对多个从服务器实现负载均衡以及可用性检测，这些工作由它来做，的确非常适合。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214160053584.png" alt="image-20211214160053584"></p>
<p>当存在大量的从服务器时，MySQL Proxy必然会出现瓶颈效应，不过，即便是到那个时候，主服务器已经无法承受写操作的压力了，图16-2 中的结构将不可避免地再次调整，接下来怎么办呢？</p>
</blockquote>
</blockquote>
<h3 id="◆-16-2-垂直分区" tabindex="-1"><a class="header-anchor" href="#◆-16-2-垂直分区" aria-hidden="true">#</a> ◆ 16.2 垂直分区</h3>
<blockquote>
<blockquote>
<p>对于数据库写操作频繁（write-heavy）的站点来说，仅仅采用主从复制和读写分离可能效果并不明显，假如你的主服务器花费了80%的时间在写数据，那么所有的从服务器也将花费更多的时间来同步数据，可想而知，所有从服务器只能依靠剩余不到20%的时间来处理你的SELECT查询请求，这时候，增加从服务器所获得的回报将越来越少，呈现边际效益递减。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>站点在成长，用户活动越来越频繁，写操作不断增多，主服务器的压力也逐渐接近极限，这时候不论你增加多少台从服务器都无济于事，因为那只是对读操作的分散，并没有对写操作起到任何作用。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>最简单的分离方法当然是将不同的数据库分布到不同的服务器上，你会发现有很多数据库之间并不存在关系，或者不需要进行联合（JOIN）查询，那么为什么不把它们放在不同的服务器上呢？比如我们将用户的博客数据库和好友数据库分别转移到独立的数据库服务器上，这种方式称为垂直分区。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214160331522.png" alt="image-20211214160331522"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>我们将db_blog和db_friend两个数据库分别转移到了独立的服务器上，而db_main则代表仍然存放在主服务器上的其他一系列数据库，你仍然可以在这里进行必要的联合查询，但是，不论从扩展的角度，还是从查询性能的角度来看，在进行数据库模型设计以及编写应用程序的时候，都应该尽量减少使用联合查询。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>按照这样的思路，一旦我们需要为站点开发新的应用，便可以通过增加新的数据库服务器来实现扩展。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>同样，我们还可以再次通过主从复制来对db_blog和db_friend两个数据库进行读写分离</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214160439594.png" alt="image-20211214160439594"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>当db_blog数据库的主服务器再次无法承受写操作压力时，我们又该如何呢？对这个数据库再次进行垂直分区吗？也许你可以将不同的数据表转移到不同的服务器上，但是当数据表也达到写操作极限的时候呢？似乎这种扩展方式要山穷水尽了，那么，我们换一种思路，来看看水平分区。</p>
</blockquote>
</blockquote>
<h3 id="◆-16-3-水平分区" tabindex="-1"><a class="header-anchor" href="#◆-16-3-水平分区" aria-hidden="true">#</a> ◆ 16.3 水平分区</h3>
<blockquote>
<blockquote>
<p>水平分区（Sharding）意味着我们可以将同一数据表中的记录通过特定的算法进行分离，分别保存在不同的数据表中，从而可以部署在不同的数据库服务器上。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>很多大规模的站点基本上都经历了从简单主从复制到垂直分区，再到水平分区的步骤，这是一个必然的成长过程。</p>
</blockquote>
</blockquote>
<h5 id="把数据放在不同分区中" tabindex="-1"><a class="header-anchor" href="#把数据放在不同分区中" aria-hidden="true">#</a> 把数据放在不同分区中</h5>
<blockquote>
<blockquote>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214161240318.png" alt="image-20211214161240318"></p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214161355184.png" alt="image-20211214161355184"></p>
</blockquote>
</blockquote>
<h5 id="分区和分表" tabindex="-1"><a class="header-anchor" href="#分区和分表" aria-hidden="true">#</a> 分区和分表</h5>
<blockquote>
<blockquote>
<p>在考虑分区之前，我们一般会对数据库进行分表，它们的思路都是相同的。比如对于刚才的tbl_posts表，我们仍然按照user_id%10将它分为10个数据表：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214161607334.png" alt="image-20211214161607334"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>这种分表的策略甚至会在数据库建立时便采用，因为我们希望数据表的记录数保持在相对较少的数量，这有利于减少查询时间，从而为数据库减少不必要的开销。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>分表只是单台数据库的优化策略</strong>，一旦到了必须考虑扩展的时候，分区便派上用场，不过，已经实现的分表将使得分区更加容易，因为数据已经是分离的，只需要迁移到其他服务器即可。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>分表算法和分区算法可能不一致，比如我们希望将这10 个表分布在两台服务器上，那么我们要在应用程序中维护一份映射关系表，比如将前5个数据表分配到一台数据库服务器，后5个数据表分配到另一台数据库服务器</strong></p>
</blockquote>
</blockquote>
<h5 id="如何分区" tabindex="-1"><a class="header-anchor" href="#如何分区" aria-hidden="true">#</a> 如何分区</h5>
<blockquote>
<blockquote>
<p>首先，你得考虑，应该对哪些数据进行分区，哪些数据可以放到分区中，比如用户信息、好友关系，它们是否适合进行分区呢？其实，这个问题很难回答，但在大多数时候，对哪些数据进行分区并不是我们可以选择的，对于那些频繁访问而导致站点接近崩溃的热点数据，我们没有理由不对它们考虑分区，甚至在一开始就要考虑。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在我们不得不对一些数据进行分区的时候，也意味着我们将失去一些操作它们的能力，比如原本你可以通过一条联合查询语句就能轻松搞定的任务，在分区后，你必须先通过用户ID找到正确的分区，然后查到对应的好友ID，再通过好友ID找到对应的分区，从而找到好友信息。但是，你也不必感到沮丧，曾经的联合查询将会随着站点规则的增大变得越来越昂贵，而分区后的查询方式将会更加容易保持相对稳定的开销。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><strong>一旦我们知道要对哪些数据实施分区后，接下来就得找到一个用于分区的字段，我们称为分区索引字段，比如前面的user_id，它必须和所有的记录都存在关系，一般我们会用被分区数据的主键或者外键。当使用主键时，你得保证它不能使用auto_increment自增类型。</strong></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>基于这个字段，你得考虑采用什么分区算法，我们希望它可以带来良好的可扩展性，并且让各个分区的工作量相对均衡，通常有以下几种常用的分区算法：</p>
</blockquote>
</blockquote>
<h5 id="分区算法" tabindex="-1"><a class="header-anchor" href="#分区算法" aria-hidden="true">#</a> 分区算法：</h5>
<h6 id="_1-哈希算法" tabindex="-1"><a class="header-anchor" href="#_1-哈希算法" aria-hidden="true">#</a> （1）哈希算法</h6>
<blockquote>
<blockquote>
<p>这种算法对于扩展并不友好，一旦我们需要从10个分区扩展到20个分区，这便涉及所有数据的重新分区，你不得不暂停站点，等待漫长的计算。</p>
</blockquote>
</blockquote>
<h6 id="_2-范围" tabindex="-1"><a class="header-anchor" href="#_2-范围" aria-hidden="true">#</a> （2）范围</h6>
<blockquote>
<blockquote>
<p>这种算法是指按照分区索引字段的范围进行分区，比如我们可以将user_id为1～10000的记录存储在一个分区中，而将10001～20000的用户存储在另一个分区中，以此类推。这使得应用程序需要维护一个简单的范围映射表，比如根据user_id来计算所属分区。</p>
<p>它可以带来很好的扩展性，随着用户数量的不断增长，我们可以创建更多的分区。但是，各个分区的工作量会存在较大的差异，比如老用户所在的分区压力相对较大，或者一部分ID比较接近的热点用户导致所在分区压力过大。</p>
</blockquote>
</blockquote>
<h6 id="_3-映射关系" tabindex="-1"><a class="header-anchor" href="#_3-映射关系" aria-hidden="true">#</a> （3）映射关系</h6>
<blockquote>
<blockquote>
<p>这种算法将对分区索引字段的每个可能的结果创建一个分区映射关系，这个映射关系将会非常庞大，应用程序已经无法通过简单的逻辑或者配置文件来维护它，而需要将它也写入数据库，比如当应用程序需要知道user_id为10的用户的博客内容在哪个分区时，它必须查询数据库获得答案，当然，我们会使用缓存来提高性能。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>由于这种方式详细保存了每一个记录的分区对应关系，所以各个分区具有较强的可伸缩性，我们可以灵活地控制它们的规模，并且轻松地将数据从一个分区迁移到另一个分区，这也使得各个分区可以通过灵活的动态调节来保持平衡。</p>
</blockquote>
</blockquote>
<h5 id="分区扩展" tabindex="-1"><a class="header-anchor" href="#分区扩展" aria-hidden="true">#</a> 分区扩展</h5>
<blockquote>
<blockquote>
<p>分区的可扩展性更多体现在能否快速平滑地实现扩展，并且进行最少单位的数据移动。</p>
</blockquote>
</blockquote>
<h5 id="分区反向代理" tabindex="-1"><a class="header-anchor" href="#分区反向代理" aria-hidden="true">#</a> 分区反向代理</h5>
<blockquote>
<blockquote>
<p>MySQL Proxy帮助应用程序实现了读写分离，而在这里，另一个开源产品Spock Proxy也起到了类似的作用，它可以帮助应用程序实现水平分区的访问调度，这意味着我们不需要在应用程序中维护那些分区对应关系了。Spock Proxy本身的大部分代码正是基于MySQL Proxy，同时它也进行了一些改进。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214163014252.png" alt="image-20211214163014252"></p>
</blockquote>
</blockquote>
<h2 id="◆-第17章-分布式计算" tabindex="-1"><a class="header-anchor" href="#◆-第17章-分布式计算" aria-hidden="true">#</a> ◆ 第17章 分布式计算</h2>
<blockquote>
<blockquote>
<p>通过Web负载均衡，我们已经可以将用户的请求分散到多台Web服务器上进行处理，但是，有些请求涉及比较耗时的计算，比如：</p>
<p>● 需要连接邮件服务器，发送一封超长的HTML邮件。</p>
<p>● 需要对用户上传的照片进行裁剪，并生成多份缩略图。</p>
<p>● 需要将用户上传的文件分发到多台服务器上。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>这些都是我们在站点开发中经常遇到的问题，如果它们都在用户请求的处理过程中完成计算，即便是我们可以通过负载均衡来减轻服务器的压力，但是关键在于，用户无法接受漫长的等待，可能从此不再愿意去体验诸如上传照片和邮件分享等功能。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>我们能否将这些计算再次进行拆分呢？比如将一部分计算转移到后台异步进行，或者将一个耗时的计算拆分到多台服务器上同时进行，答案是肯定的，通过分布式计算，我们可以非常灵活地控制分布在多台服务器上的计算能力</p>
</blockquote>
</blockquote>
<h3 id="◆-17-1-异步计算" tabindex="-1"><a class="header-anchor" href="#◆-17-1-异步计算" aria-hidden="true">#</a> ◆ 17.1 异步计算</h3>
<h5 id="分布式消息队列" tabindex="-1"><a class="header-anchor" href="#分布式消息队列" aria-hidden="true">#</a> 分布式消息队列</h5>
<blockquote>
<blockquote>
<p>我们所说的分布式消息队列，实际上是指监听在服务器某个端口上的服务，也可以说是分布式消息队列服务，它可以维护并管理很多消息队列，应用程序可以通过网络快速地访问它，为某个队列追加消息或者从某个队列领取消息。除此之外，你几乎不用关心它的内部构造，而我们更加关心的是，如何将它融入我们的Web计算中，最大程度地发挥分布式消息队列的作用。</p>
</blockquote>
</blockquote>
<h5 id="gearman" tabindex="-1"><a class="header-anchor" href="#gearman" aria-hidden="true">#</a> Gearman</h5>
<blockquote>
<blockquote>
<p>Gearman是一个开源产品，它的初衷是用来实现远程函数调用，这样一来，它便可以将计算转移到其他服务器上，而这一切都巧妙地隐藏在它提供的API中。同时，它的这种机制是跨语言的，这意味着你可以在PHP程序中调用另一台服务器上用C++编写的程序中的某个函数。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>Gearman在远程调用中发挥作用，其中的任务服务器（JobServer）运行着gearmand，它负责处理Web应用的远程调用请求，并且维护计算任务，而工作服务器（Worker Server）则负责从Job Server那里领取任务，并且执行实际的计算，然后将结果返回给Job Server。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214163346821.png" alt="image-20211214163346821"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>运行gearmand的Job Server实际上便是某种意义上的分布式消息队列服务，当然，与随后要介绍的MemcacheQ相比，gearmand做了更多的事情，它所维护的消息，实际上已经具备了某种更适合计算的任务形态，可以说，它几乎定义了整个异步计算的开发框架。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214163541259.png" alt="image-20211214163541259"></p>
<p>Worker Server便完全具备了发送邮件的能力，现在，我们可以将Web应用程序中发送邮件的部分代码修改为以下形式：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214163659118.png" alt="image-20211214163659118"></p>
<p>程序首先创建了一个连接到Job Server的Worker实例，然后注册了SendMail函数，这意味着Worker告诉Job Server自己拥有处理SendMail函数的能力，这将在Worker随后领取任务的时候起到作用。随后的beginWork()使得这个程序进入循环等待，它将一直领取任务并进行处理，而不会退出。</p>
<p>由于采用了异步计算，应用程序也并不知道邮件是否发送成功，这怎么办呢？千万不要为了解决这个问题而放弃异步计算，这将让你前功尽弃，在异步计算的世界里，我们的思维方式必须改变，既然计算是异步的，那么反馈也应该是异步的，你完全可以让SendMail函数将发送结果写入数据库，并生成报表，然后让应用程序定期对报告中发送失败的邮件执行再次发送。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>你也可以运行多个Job Server，从而提高可用性，每个Job Server都注册了所有的Worker Server，不论你将任务投递给谁，它们都可以出色地完成工作。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214163907898.png" alt="image-20211214163907898"></p>
</blockquote>
</blockquote>
<h5 id="memcacheq" tabindex="-1"><a class="header-anchor" href="#memcacheq" aria-hidden="true">#</a> MemcacheQ</h5>
<blockquote>
<blockquote>
<p>MemcacheQ是一个单纯的分布式消息队列服务，它同样也是开源产品，和前面介绍的MemcacheDB师出同门，当然，它同样基于Memcache访问协议。</p>
</blockquote>
</blockquote>
<h3 id="◆-17-2-并行计算" tabindex="-1"><a class="header-anchor" href="#◆-17-2-并行计算" aria-hidden="true">#</a> ◆ 17.2 并行计算</h3>
<blockquote>
<blockquote>
<p>对于分布式计算来说，异步计算才只是刚刚开始，一个现实的问题在于，即便我们将一个耗时的计算从Web服务进程中剥离并转移到其他的服务器上，但这仅仅是转移，并不能减少它的计算时间，而这个任务可能恰恰是我们需要快速得到结果的，这该怎么办呢？</p>
</blockquote>
</blockquote>
<h5 id="分而治之" tabindex="-1"><a class="header-anchor" href="#分而治之" aria-hidden="true">#</a> 分而治之</h5>
<blockquote>
<blockquote>
<p>必须要想办法对计算任务进行再次拆分，并将它们分散到更多的服务器上同时进行，这便是真正意义上的并行计算。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>如何拆分任务呢？多个任务分别计算后又怎么合并呢？</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>对于各种不同的计算任务，如何拆分、拆分后如何计算、最后如何汇总都是应用程序需要考虑的事情，几乎很难设计出一套通用的、具体的并行计算方法，但是存在一定的并行计算框架，我们来看随后介绍的Map/Reduce。里我们谈的是基于分布式的并行计算，而不是多核处理器环境下的并行计算，对于后者，这是操作系统和编程语言需要考虑的，本书将它们视为垂直扩展的范畴。</p>
</blockquote>
</blockquote>
<h5 id="map-reduce" tabindex="-1"><a class="header-anchor" href="#map-reduce" aria-hidden="true">#</a> Map/Reduce</h5>
<blockquote>
<blockquote>
<p>它认为任何的计算任务都可以经历从拆分到汇总的两个过程，反过来，只需要用这两个过程就可以描述所有的计算任务，这两个过程分别为Map和Reduce。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>Map/Reduce是一种分布式并行计算的开发框架，Google用它来实现很多产品中海量数，它提供了一个简单的可扩展模型，同时隐藏了很多底层的技术细节，比如传输、监控、容错、可用性、负载均衡等，这使得我们只需要考虑如何对计算进行拆分和汇总，以及编写具体的计算逻辑即可。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>Map/Reduce只是Google内部的实现，整个系统我们是无法拿来使用的，但是它的思想是开放的，我们完全可以通过其他的方法或者开源产品来实现，比如Hadoop支持在分布式文件系统中实现Map/Reduce计算，另外，我们也完全可以通过前面介绍的Gearman来实现Map/Reduce，其中Job Server仍然负责调度任务，而Worker Server主要负责两部分，一部分用于执行Map操作，而另一部分用于执行Reduce操作，当然，由Client Server负责对计算进行分解。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214164524507.png" alt="image-20211214164524507"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>有一个问题在于，假如某个Worker Server上的Map操作领取了计算任务后，突然发生了故障，比如I/O缓慢或者CPU不可用等，这将导致它无法完成计算，必然会拖累整个计算过程，大家不能浪费时间来等待它，这属于掉队者问题，解决办法是，将同样的计算任务分配给多个Map操作，实现冗余计算，一旦获得结果后，终止其他计算。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>另一方面，在并行计算的过程中，必然会涉及大量的数据移动，在刚才的例子中，主要的数据便是学生的试卷内容，往往在实际应用中，我们会对数据进行压缩传输，在一定程度上减少网络I/O开销，从Google的一份报告中可以看到，经过数据压缩后，1800台Worker Server完成了31Gbytes/s原始数据的计算，而压缩之前，只能完成10Gbytes/s的计算。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>当然，你可以将分布式文件系统很好地与Map/Reduce结合使用，比如在刚才的例子中，我们通过分布式文件系统将所有试卷内容都事先复制到Worker Server上，这时候，Worker Server也扮演了分布式文件系统的存储节点，它可以快速地直接从本地提取原始数据。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>在分布式计算环境下，Map/Reduce是一种解决特定问题的策略，通过它，你甚至可以驾驭成百上千个CPU和几乎无限的扩展计算能力</p>
</blockquote>
</blockquote>
<h2 id="◆-第18章-性能监控" tabindex="-1"><a class="header-anchor" href="#◆-第18章-性能监控" aria-hidden="true">#</a> ◆ 第18章 性能监控</h2>
<h3 id="◆-18-1-实时监控" tabindex="-1"><a class="header-anchor" href="#◆-18-1-实时监控" aria-hidden="true">#</a> ◆ 18.1 实时监控</h3>
<blockquote>
<blockquote>
<p>Nmon是一款工作在服务器本地的实时监控软件，它可以提供时间间隔为秒的系统监控</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214164806013.png" alt="image-20211214164806013"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>可以监控内核状态、系统负载、虚拟内存、NFS等，只要有足够大的显示器，你可以把它们都添加到主界面中，服务器的一切活动尽收眼底，你可以在最短的时间内知道服务器现在在忙什么。</p>
<p>可以用Nmon来录制数据，并通过另一个分析工具Nmon Analyser生成监控统计报告，我们生成了间隔为1秒的监控报告，其中包括CPU使用率和磁盘I/O。</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214164943545.png" alt="image-20211214164943545"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>通常，我们只是在一些必要的时候进行实时监控，主要包括：</p>
<p>● 快速查看系统某组件的状态，比如进行网络结构调整后需要快速知道当前时刻的网络流量变化，而通过MRTG等方法你可能需要等待几分钟后的图表更新。</p>
<p>● 观察一些底层的系统状态，比如内核切换、进程队列、中断次数等，这些状态通常无法通过其他监控系统获得。</p>
<p>● 根据最小时间间隔的状态变化来进行一些诊断，比如通常的监控系统会将5分钟内的状态进行平均计算，那么5分钟内的变化情况我们并不知道，也许对于一些系统组件来说，这5分钟内的变化曲线隐藏着重要的线索。</p>
<p>除此之外，你可能并不需要经常进行实时监控，毕竟这需要你通过SSH登录到服务器上进行操作，而如果你要兼顾很多台服务器的话，这样做显然很不现实，其实，大多数时候，我们更希望通过远程来对服务器实施监控，并且建立统一入口的监控中心，同时，也可以放慢监控的节奏，因为过于实时的监控对服务器本身来说也存在一定的开销。</p>
</blockquote>
</blockquote>
<h3 id="◆-18-2-监控代理" tabindex="-1"><a class="header-anchor" href="#◆-18-2-监控代理" aria-hidden="true">#</a> ◆ 18.2 监控代理</h3>
<blockquote>
<blockquote>
<p>一旦我们需要通过远程来监控服务器，那么监控代理程序便必不可少，它负责将服务器本地的各种状态定期上报给监控中心，或者响应来自监控中心的请求，这使得远程监控成为可能。</p>
<p>你可以为站点服务器开发专用的监控代理程序，并从本地文件中获取状态数据，在Linux中，一切都是文件，当然也包括各种系统状态，比如我们可以在这里看到详细的内存使用情况：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214165308084.png" alt="image-20211214165308084"></p>
<p>也可以看到系统负载和进程队列状态：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214165321431.png" alt="image-20211214165321431"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>你只需要将这些数据打包，按照你定义的格式上报给监控中心即可。</p>
<p>SNMP服务器端本身便是一个出色的监控代理程序，它已经逐渐成为标准，并且支持很多异构平台。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>通过SNMP来获取另一台服务器的所有设备状态:</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214165523241.png" alt="image-20211214165523241"></p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214165552030.png" alt="image-20211214165552030"></p>
<p>包括MRTG、Cacti以及Nagios在内的很多监控工具都利用SNMP来监控远程服务器，而你只需要在被监控的服务器上开启SNMP服务，同时对监控来源进行授权配置即可。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>有一些我们希望监控的服务并没有提供相应的SNMP支持，比如我们要监控Nginx服务器当前的HTTP并发连接数，该怎么做呢？是否需要自己开发监控代理程序呢？幸运的是，Nginx提供了必要的HTTP监控接口，你可以直接在监控中心请求它即可，比如我们通过请求以下URL：</p>
<p><img src="@source/docs/theme-reco/img/9.构建高性能Web站点/image-20211214165651327.png" alt="image-20211214165651327"></p>
</blockquote>
</blockquote>
<h3 id="◆-18-3-系统监控" tabindex="-1"><a class="header-anchor" href="#◆-18-3-系统监控" aria-hidden="true">#</a> ◆ 18.3 系统监控</h3>
<blockquote>
<blockquote>
<p>通过SNMP便可以很容易地对服务器进行一些常规的系统监控，这包括CPU使用率、系统负载、内存使用率、网络I/O、磁盘I/O、磁盘使用率等</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>我们还需要建立监控中心，对这些状态数据进行统计和呈现。幸运的是，有很多开源产品可以帮助我们，这里我们主要以Cacti为例，它完全可以支持刚刚提到的这些系统监控，并且绘制出相应的图表，便于我们浏览。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>Cacti采用RRDtool作为监控数据的存储引擎，它是一种专门针对绘制坐标图而设计的存储格式，相对于其他存储结构来说要节省很多存储空间，这为我们长期监控大量服务器提供了可能。</p>
</blockquote>
</blockquote>
<h3 id="◆-18-4-服务监控" tabindex="-1"><a class="header-anchor" href="#◆-18-4-服务监控" aria-hidden="true">#</a> ◆ 18.4 服务监控</h3>
<blockquote>
<blockquote>
<p>Cacti中支持插件机制，我们可以通过一些第三方模板来监控这些服务，同时，我们还需要相应的监控代理，幸运的是，一些常用的服务已经考虑到这一点，它们提供了一定的监控访问接口，比如Apache中基于HTTP的mod_status模块</p>
</blockquote>
</blockquote>
<h3 id="◆-18-5-响应时间监控" tabindex="-1"><a class="header-anchor" href="#◆-18-5-响应时间监控" aria-hidden="true">#</a> ◆ 18.5 响应时间监控</h3>
<blockquote>
<blockquote>
<p>借助一些第三方工具，这里推荐监控宝（www．jiankongbao．com），它像Cacti一样提供基于Web的服务界面，但你不用花时间去安装和部署，只需要注册和添加监控任务便可以快速使用。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>监控宝拥有独立的监控点，所有它可以定时模拟用户来请求你指定的多个URL，并且记录详细的响应时间</p>
</blockquote>
</blockquote>
</div></template>


